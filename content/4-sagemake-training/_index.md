---
title: "SageMaker Training & Model Registry"
weight: 4
chapter: false
pre: "<b>4. </b>"
---

## ğŸ¯ Má»¥c tiÃªu Task 4

Huáº¥n luyá»‡n mÃ´ hÃ¬nh dá»± bÃ¡o **BASKET_PRICE_SENSITIVITY** (Low/Medium/High) báº±ng Amazon SageMaker, tá»± Ä‘á»™ng hÃ³a toÃ n bá»™ luá»“ng **ETL â†’ Training â†’ Evaluation â†’ Model Registry**.

â†’ **ÄÃ¢y lÃ  trÃ¡i tim cá»§a pipeline MLOps** â€” nÆ¡i thá»±c hiá»‡n quy trÃ¬nh xá»­ lÃ½ dá»¯ liá»‡u tá»± Ä‘á»™ng (ETL) vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh há»c mÃ¡y.

ğŸ“¥ **Input**
- AWS Account vá»›i quyá»n SageMaker/S3/CloudWatch
- Retail transaction data trong S3 `raw/`
- Project naming: `mlops-retail-prediction-dev`

âœ… **Káº¿t quáº£ mong Ä‘á»£i**
- Luá»“ng ETL â†’ Train â†’ Evaluate â†’ Save â†’ (Register) cháº¡y tá»± Ä‘á»™ng end-to-end
- Model Ä‘áº¡t accuracy â‰¥ 80%, F1 â‰¥ 0.7
- Artifact vÃ  káº¿t quáº£ huáº¥n luyá»‡n Ä‘Æ°á»£c lÆ°u Ä‘áº§y Ä‘á»§ trong S3

ğŸ’° **Chi phÃ­ Æ°á»›c tÃ­nh**: ~**$0.3-0.5/job** (instance ml.m5.large, thá»i gian ~10-15 phÃºt). Náº¿u báº­t Spot â†’ giáº£m 70-80%.


ğŸ“Œ **CÃ¡c bÆ°á»›c**
1. **ETL Pipeline** - Automated data processing  
2. **Multi-Model Training** - LR, RF, XGBoost comparison
3. **Model Evaluation** - Comprehensive metrics + confusion matrix
4. **Model Registry** - Version control vÃ  approval workflow
5. **CI/CD Integration** - Export model info for deployment

{{% notice info %}}
**ğŸ’¡ Task 4 Focus - MLOps Core Pipeline:**
- âœ… **Automated ETL** - Raw data â†’ Gold features pipeline
- âœ… **Multi-Model Training** - LR/RF/XGBoost comparison  
- âœ… **Comprehensive Evaluation** - Accuracy, F1, Confusion Matrix
- âœ… **Model Registry** - Version control vÃ  metadata tracking
- âœ… **Cost Optimization** - Spot instances + auto cleanup

**TrÃ¡i tim cá»§a MLOps** - tá»± Ä‘á»™ng hÃ³a hoÃ n toÃ n tá»« data Ä‘áº¿n model
{{% /notice %}}

ğŸ“Š **Success Criteria**
- âœ… **ETL Success** - Raw data â†’ clean features pipeline hoáº¡t Ä‘á»™ng
- âœ… **Model Performance** - Accuracy â‰¥ 80%, F1 â‰¥ 0.7
- âœ… **Automation** - End-to-end pipeline cháº¡y tá»± Ä‘á»™ng
- âœ… **Cost Efficiency** - Spot instances, auto cleanup

âš ï¸ **LÆ°u Ã½**
- **ETL Pipeline** cáº§n memory Ä‘á»§ lá»›n Ä‘á»ƒ xá»­ lÃ½ transaction data
- **Model Training** sá»­ dá»¥ng Spot Ä‘á»ƒ giáº£m chi phÃ­ 
- **Evaluation Metrics** pháº£i consistent vá»›i business requirements

## Thá»±c hiá»‡n báº±ng AWS Console - HÆ°á»›ng dáº«n chi tiáº¿t tá»«ng bÆ°á»›c

### BÆ°á»›c 1: Kiá»ƒm tra dá»¯ liá»‡u trong S3 (tá»« Task 3)

#### 1.1. XÃ¡c minh bucket vÃ  dá»¯ liá»‡u
1. **AWS Console** â†’ **S3** 
2. **TÃ¬m bucket**: `mlops-retail-prediction-dev-[account-id]` (Ä‘Ã£ táº¡o á»Ÿ Task 3)
3. **Kiá»ƒm tra cáº¥u trÃºc**:
   ```
   âœ… raw/        # cÃ³ file dunnhumby_The-Complete-Journey.csv
   âœ… silver/     # sáº½ chá»©a dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
   âœ… gold/       # sáº½ chá»©a features training
   âœ… artifacts/  # sáº½ chá»©a model outputs
   ```

#### 1.2. XÃ¡c minh IAM Role (tá»« Task 2)
1. **AWS Console** â†’ **IAM** â†’ **Roles**
2. **TÃ¬m role**: `mlops-retail-prediction-dev-sagemaker-execution` (Ä‘Ã£ táº¡o á»Ÿ Task 2)
3. **Kiá»ƒm tra permissions**:
   - âœ… `AmazonSageMakerFullAccess`
   - âœ… `AmazonS3FullAccess`
   - âœ… `CloudWatchLogsFullAccess`

### BÆ°á»›c 2: Training Model vá»›i SageMaker Studio

#### 2.1. Má»Ÿ SageMaker Studio
1. **AWS Console** â†’ TÃ¬m "SageMaker" â†’ Click **Amazon SageMaker**
2. **Sidebar trÃ¡i** â†’ Click **"Studio"**
3. **Click "Create a SageMaker domain"** (náº¿u chÆ°a cÃ³)
4. **Domain name**: `mlops-retail-domain`
5. **Default execution role**: Chá»n `mlops-retail-prediction-dev-sagemaker-execution` (tá»« Task 2)
6. Click **"Submit"** â†’ Äá»£i 5-10 phÃºt
7. **Domain created** â†’ Click **"Launch"** â†’ **"Studio"**

#### 2.2. Táº¡o Training Job
1. **SageMaker Studio má»Ÿ** â†’ **Sidebar trÃ¡i** â†’ Click **"Training"** â†’ **"Training jobs"**
2. Click **"Create training job"**

#### 2.3. Cáº¥u hÃ¬nh Job Details
1. **Job name**: `retail-prediction-training-20241011`
2. **IAM role**: Chá»n `mlops-retail-prediction-dev-sagemaker-execution` (tá»« Task 2)
3. Click **"Next"**

#### 2.4. Algorithm Options
1. **Algorithm source**: Chá»n **"Your own algorithm container in ECR"**
2. **Container path**: 
   ```
   382416733822.dkr.ecr.ap-southeast-1.amazonaws.com/scikit-learn:1.0-1-cpu-py3
   ```
3. **Input mode**: `File`
4. Click **"Next"**

#### 2.5. Configure Resource
1. **Instance type**: `ml.m5.large`
2. **Instance count**: `1`
3. **Additional storage per instance (GB)**: `30`
4. **Use Spot training**: âœ… **Check** (tiáº¿t kiá»‡m 70% chi phÃ­)
5. **Max wait time**: `7200` seconds (2 hours)
6. **Max training time**: `3600` seconds (1 hour)
7. Click **"Next"**

#### 2.6. Configure Input Data
1. **Channel name**: `training`
2. **Input mode**: `File`
3. **Data source**: `S3`
4. **S3 location**: `s3://mlops-retail-prediction-dev-[account-id]/raw/`
5. **Content type**: `text/csv`
6. **Compression**: `None`
7. **Record wrapper**: `None`
8. Click **"Add channel"**
9. Click **"Next"**

#### 2.7. Configure Output Data
1. **S3 output path**: `s3://mlops-retail-prediction-dev-[account-id]/artifacts/` (bucket tá»« Task 3)
2. **Encryption**: `None`
3. Click **"Next"**

#### 2.8. Configure Hyperparameters
1. Click **"Add hyperparameter"** cho tá»«ng tham sá»‘:

| Key | Value |
|-----|-------|
| `model_type` | `all` |
| `random_state` | `42` |
| `cv_folds` | `5` |
| `test_size` | `0.2` |

2. Click **"Next"**

#### 2.9. Review vÃ  Create
1. **Review** táº¥t cáº£ cÃ i Ä‘áº·t
2. Click **"Create training job"**

### BÆ°á»›c 3: Upload Training Script

#### 3.1. Táº¡o Training Script
1. **SageMaker Studio** â†’ Click **"+"** â†’ **"Python 3"** notebook
2. **Táº¡o cell má»›i** vÃ  paste code sau:

```python
%%writefile train.py

import pandas as pd
import numpy as np
import joblib
import os
import json
import argparse
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, f1_score, classification_report
import xgboost as xgb

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))
    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))
    parser.add_argument('--model_type', type=str, default='all')
    args = parser.parse_args()
    
    # Load data
    input_files = [os.path.join(args.train, file) for file in os.listdir(args.train)]
    raw_data = pd.concat([pd.read_csv(file) for file in input_files])
    
    # Basic preprocessing
    clean_data = raw_data.dropna(subset=['BASKET_ID', 'SPEND', 'BASKET_PRICE_SENSITIVITY'])
    
    # Create features
    features = clean_data.groupby('BASKET_ID').agg({
        'SPEND': ['sum', 'mean', 'std', 'count'],
        'QUANTITY': ['sum', 'mean'],
        'PROD_CODE': 'nunique',
        'BASKET_PRICE_SENSITIVITY': lambda x: x.iloc[0]
    }).reset_index()
    
    # Flatten columns
    features.columns = ['basket_id', 'total_spend', 'avg_spend', 'spend_std', 
                       'basket_size', 'total_qty', 'avg_qty', 'unique_products', 'target']
    features['spend_std'] = features['spend_std'].fillna(0)
    
    # Prepare for ML
    X = features[['total_spend', 'avg_spend', 'spend_std', 'basket_size', 
                  'total_qty', 'avg_qty', 'unique_products']]
    y = features['target']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Train models
    models = {
        'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'logistic': LogisticRegression(random_state=42, max_iter=1000),
        'decision_tree': DecisionTreeClassifier(random_state=42, max_depth=10)
    }
    
    best_model = None
    best_score = 0
    results = {}
    
    for name, model in models.items():
        model.fit(X_train, y_train)
        pred = model.predict(X_test)
        f1 = f1_score(y_test, pred, average='macro')
        acc = accuracy_score(y_test, pred)
        
        results[name] = {'accuracy': acc, 'f1_score': f1}
        
        if f1 > best_score:
            best_score = f1
            best_model = model
            best_name = name
    
    # Save best model
    joblib.dump(best_model, os.path.join(args.model_dir, 'model.joblib'))
    
    # Save results
    with open(os.path.join(args.model_dir, 'results.json'), 'w') as f:
        json.dump({
            'best_model': best_name,
            'best_f1_score': best_score,
            'all_results': results
        }, f)
    
    print(f"Best model: {best_name} with F1-score: {best_score:.4f}")

if __name__ == '__main__':
    main()
```

3. **Run cell** â†’ File `train.py` Ä‘Æ°á»£c táº¡o

#### 3.2. Upload script lÃªn S3 (sá»­ dá»¥ng bucket tá»« Task 3)
1. **Táº¡o cell má»›i**:

```python
import boto3

s3 = boto3.client('s3')
bucket_name = 'mlops-retail-prediction-dev-[account-id]'  # Thay [account-id]

# Upload training script
s3.upload_file('train.py', bucket_name, 'code/train.py')
print("âœ… Training script uploaded to S3")
```

2. **Run cell**

### BÆ°á»›c 4: Cháº¡y Training Job

#### 4.1. Theo dÃµi Training Job
1. **SageMaker Console** â†’ **Training jobs**
2. **TÃ¬m job**: `retail-prediction-training-20241011`
3. **Status**: `InProgress` â†’ `Completed` (5-10 phÃºt)
4. **Click vÃ o job name** Ä‘á»ƒ xem details

#### 4.2. Xem káº¿t quáº£
1. **Scroll xuá»‘ng** â†’ **Monitor** section
2. **CloudWatch logs** â†’ Click **"View logs"**
3. **TÃ¬m dÃ²ng**: `Best model: random_forest with F1-score: 0.8234`

### BÆ°á»›c 5: Model Registry

#### 5.1. Táº¡o Model Package Group
1. **SageMaker Console** â†’ **Sidebar** â†’ **"Inference"** â†’ **"Model registry"**
2. Click **"Create model package group"**
3. **Name**: `retail-price-sensitivity-models`
4. **Description**: `Models for retail customer price sensitivity prediction`
5. Click **"Create model package group"**

#### 5.2. Register Model
1. **Training jobs** â†’ Click job `retail-prediction-training-20241011`
2. **Scroll xuá»‘ng** â†’ **Model artifacts** section
3. Click **"Create model package"**
4. **Model package group**: `retail-price-sensitivity-models`
5. **Model package version description**: `First retail prediction model v1.0`
6. **Approval status**: `PendingManualApproval`
7. Click **"Create model package"**

#### 5.3. Approve Model (náº¿u F1-score â‰¥ 0.7)
1. **Model registry** â†’ **Model package groups** â†’ Click `retail-price-sensitivity-models`
2. **Versions** tab â†’ Click **version 1**
3. **Update status** â†’ **"Approved"**
4. **Description**: `Auto-approved: F1-score â‰¥ 0.7`
5. Click **"Update status"**

### BÆ°á»›c 6: Kiá»ƒm tra káº¿t quáº£

#### 6.1. Download Model Artifacts
1. **Training job details** â†’ **Output** section
2. **S3 model artifacts**: Click **S3 URI**
3. **S3 Console má»Ÿ** â†’ Click **"Download"** file `model.tar.gz`
4. **Extract** â†’ Kiá»ƒm tra file `model.joblib` vÃ  `results.json`

#### 6.2. Verification
```json
{
  "best_model": "random_forest",
  "best_f1_score": 0.8234,
  "all_results": {
    "random_forest": {"accuracy": 0.8456, "f1_score": 0.8234},
    "logistic": {"accuracy": 0.8123, "f1_score": 0.7891},
    "decision_tree": {"accuracy": 0.7765, "f1_score": 0.7456}
  }
}
```

### âœ… HoÃ n thÃ nh!

**Báº¡n Ä‘Ã£ thÃ nh cÃ´ng:**
- âœ… **Táº¡o S3 bucket** vÃ  upload dá»¯ liá»‡u
- âœ… **Cáº¥u hÃ¬nh IAM role** cho SageMaker  
- âœ… **Train model** vá»›i Random Forest, Logistic Regression, Decision Tree
- âœ… **Chá»n best model** dá»±a trÃªn F1-score
- âœ… **Register model** trong Model Registry
- âœ… **Approve model** cho production

**Káº¿t quáº£:**
- ğŸ¯ **Best Model**: Random Forest
- ğŸ“Š **F1-Score**: 0.8234 (> 0.7 âœ…)
- ğŸ“Š **Accuracy**: 0.8456 (> 0.8 âœ…)
- ğŸ’° **Chi phÃ­**: ~$0.3 (vá»›i Spot instances)

**Next Steps:** Model Ä‘Ã£ sáºµn sÃ ng cho deployment trong Task 10!

## Thá»±c hiá»‡n báº±ng Code

### 1. ETL Pipeline - Automated Data Processing

### 1.1. ETL Processing Script

**Táº¡o file `etl_processing.py`:**
```python
import pandas as pd
import numpy as np
import boto3
import os
import argparse
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import json
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def parse_args():
    """Parse command line arguments for ETL"""
    parser = argparse.ArgumentParser()
    parser.add_argument('--input-path', type=str, required=True)
    parser.add_argument('--output-path', type=str, required=True)
    parser.add_argument('--bucket-name', type=str, required=True)
    return parser.parse_args()

def load_raw_data(bucket_name, raw_prefix='raw/'):
    """Load transaction data from S3 raw folder"""
    logger.info(f"Loading raw data from s3://{bucket_name}/{raw_prefix}")
    
    s3 = boto3.client('s3')
    
    # List all CSV files in raw folder
    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=raw_prefix)
    csv_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.csv')]
    
    # Load and concatenate all transaction files
    dataframes = []
    for file_key in csv_files:
        logger.info(f"Loading {file_key}")
        df = pd.read_csv(f's3://{bucket_name}/{file_key}')
        dataframes.append(df)
    
    combined_df = pd.concat(dataframes, ignore_index=True)
    logger.info(f"Combined dataset shape: {combined_df.shape}")
    
    return combined_df

def clean_data(df):
    """Clean and preprocess transaction data"""
    logger.info("Starting data cleaning...")
    
    # Remove missing values in critical columns
    critical_cols = ['BASKET_ID', 'SPEND', 'QUANTITY', 'BASKET_PRICE_SENSITIVITY']
    df = df.dropna(subset=critical_cols)
    
    # Convert date columns
    if 'SHOP_DATE' in df.columns:
        df['SHOP_DATE'] = pd.to_datetime(df['SHOP_DATE'], errors='coerce')
    
    # Remove outliers (spend > 99th percentile)
    spend_99 = df['SPEND'].quantile(0.99)
    df = df[df['SPEND'] <= spend_99]
    
    # Filter valid price sensitivity values
    valid_sensitivity = ['Low', 'Medium', 'High']
    df = df[df['BASKET_PRICE_SENSITIVITY'].isin(valid_sensitivity)]
    
    logger.info(f"After cleaning: {df.shape}")
    return df

def create_features(df):
    """Create ML features from transaction data"""
    logger.info("Creating features...")
    
    # Aggregate by BASKET_ID to create basket-level features
    basket_features = df.groupby('BASKET_ID').agg({
        'SPEND': ['sum', 'mean', 'std', 'count'],
        'QUANTITY': ['sum', 'mean'],
        'PROD_CODE': 'nunique',
        'STORE_FORMAT': lambda x: x.iloc[0],
        'STORE_REGION': lambda x: x.iloc[0],
        'SHOP_WEEK': lambda x: x.iloc[0],
        'SHOP_WEEKDAY': lambda x: x.iloc[0] if 'SHOP_WEEKDAY' in df.columns else 1,
        'SHOP_HOUR': lambda x: x.iloc[0] if 'SHOP_HOUR' in df.columns else 12,
        'BASKET_PRICE_SENSITIVITY': lambda x: x.iloc[0]  # Target
    }).reset_index()
    
    # Flatten column names
    basket_features.columns = [
        'basket_id', 'total_spend', 'avg_spend', 'spend_std', 'basket_size',
        'total_quantity', 'avg_quantity', 'unique_products',
        'store_format', 'store_region', 'shop_week', 'shop_weekday', 'shop_hour',
        'price_sensitivity'
    ]
    
    # Fill NaN values
    basket_features['spend_std'] = basket_features['spend_std'].fillna(0)
    
    # Create additional features
    basket_features['spend_per_item'] = basket_features['total_spend'] / basket_features['basket_size']
    basket_features['quantity_per_product'] = basket_features['total_quantity'] / basket_features['unique_products']
    
    # Encode categorical variables
    le_store_format = LabelEncoder()
    le_store_region = LabelEncoder()
    
    basket_features['store_format_encoded'] = le_store_format.fit_transform(basket_features['store_format'])
    basket_features['store_region_encoded'] = le_store_region.fit_transform(basket_features['store_region'])
    
    # Save encoders for later use
    encoders = {
        'store_format': le_store_format.classes_.tolist(),
        'store_region': le_store_region.classes_.tolist()
    }
    
    logger.info(f"Features created: {basket_features.shape}")
    return basket_features, encoders

def split_and_save_data(features_df, output_path, bucket_name, test_size=0.2):
    """Split data and save to S3 gold folder"""
    logger.info("Splitting and saving data...")
    
    # Prepare features and target
    feature_cols = [col for col in features_df.columns if col not in ['basket_id', 'price_sensitivity', 'store_format', 'store_region']]
    X = features_df[feature_cols]
    y = features_df['price_sensitivity']
    
    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=y
    )
    
    # Create train and test datasets
    train_df = X_train.copy()
    train_df['price_sensitivity'] = y_train
    
    test_df = X_test.copy()
    test_df['price_sensitivity'] = y_test
    
    # Save to S3 as Parquet
    train_path = f's3://{bucket_name}/gold/train_features.parquet'
    test_path = f's3://{bucket_name}/gold/test_features.parquet'
    
    train_df.to_parquet(train_path, compression='snappy', index=False)
    test_df.to_parquet(test_path, compression='snappy', index=False)
    
    logger.info(f"Train data saved: {train_path} ({train_df.shape})")
    logger.info(f"Test data saved: {test_path} ({test_df.shape})")
    
    return train_path, test_path, feature_cols

def main():
    """Main ETL function"""
    args = parse_args()
    
    try:
        # Load raw data
        raw_df = load_raw_data(args.bucket_name)
        
        # Clean data
        clean_df = clean_data(raw_df)
        
        # Create features
        features_df, encoders = create_features(clean_df)
        
        # Split and save
        train_path, test_path, feature_cols = split_and_save_data(
            features_df, args.output_path, args.bucket_name
        )
        
        # Save metadata
        metadata = {
            'feature_columns': feature_cols,
            'encoders': encoders,
            'train_samples': len(features_df) * 0.8,
            'test_samples': len(features_df) * 0.2,
            'target_classes': ['Low', 'Medium', 'High'],
            'etl_timestamp': pd.Timestamp.now().isoformat()
        }
        
        metadata_path = f's3://{args.bucket_name}/gold/metadata.json'
        with open('/tmp/metadata.json', 'w') as f:
            json.dump(metadata, f, indent=2)
        
        s3 = boto3.client('s3')
        s3.upload_file('/tmp/metadata.json', args.bucket_name, 'gold/metadata.json')
        
        logger.info("âœ… ETL Pipeline completed successfully")
        
    except Exception as e:
        logger.error(f"âŒ ETL Pipeline failed: {str(e)}")
        raise

if __name__ == '__main__':
    main()
```

### 1.2. SageMaker Processing Job Setup

**Cháº¡y ETL Pipeline trÃªn SageMaker:**
```python
import boto3
import sagemaker
from sagemaker.sklearn.processing import SKLearnProcessor
from sagemaker.processing import ProcessingInput, ProcessingOutput

# Initialize SageMaker session
sagemaker_session = sagemaker.Session()
role = sagemaker.get_execution_role()
bucket_name = 'mlops-retail-prediction-dev-123456789012'

# Create SKLearn processor
sklearn_processor = SKLearnProcessor(
    framework_version='1.0-1',
    role=role,
    instance_type='ml.m5.large',
    instance_count=1,
    base_job_name='retail-etl-processing',
    sagemaker_session=sagemaker_session
)

# Run ETL processing job
sklearn_processor.run(
    code='etl_processing.py',
    inputs=[
        ProcessingInput(
            source=f's3://{bucket_name}/raw/',
            destination='/opt/ml/processing/input'
        )
    ],
    outputs=[
        ProcessingOutput(
            output_name='features',
            source='/opt/ml/processing/output',
            destination=f's3://{bucket_name}/gold/'
        )
    ],
    arguments=[
        '--input-path', '/opt/ml/processing/input',
        '--output-path', '/opt/ml/processing/output', 
        '--bucket-name', bucket_name
    ]
)

print("âœ… ETL Processing job completed")
```

## 2. Multi-Model Training Pipeline

### 2.1. Multi-Model Training Script

**Táº¡o file `train_multi_model.py`:**
```python
import argparse
import os
import pandas as pd
import numpy as np
import joblib
import json
import logging
from datetime import datetime

# ML models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
import xgboost as xgb

# Metrics
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix
)
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import seaborn as sns

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser()
    
    # SageMaker paths
    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))
    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))
    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))
    
    # Model selection
    parser.add_argument('--model-type', type=str, default='all', 
                       choices=['logistic', 'decision_tree', 'random_forest', 'xgboost', 'all'])
    
    # Hyperparameters
    parser.add_argument('--random-state', type=int, default=42)
    parser.add_argument('--cv-folds', type=int, default=5)
    
    return parser.parse_args()

def load_data(train_path, test_path):
    """Load training and test data"""
    logger.info(f"Loading data from {train_path} and {test_path}")
    
    train_df = pd.read_parquet(f'{train_path}/train_features.parquet')
    test_df = pd.read_parquet(f'{test_path}/test_features.parquet')
    
    # Separate features and target
    feature_cols = [col for col in train_df.columns if col != 'price_sensitivity']
    
    X_train = train_df[feature_cols]
    y_train = train_df['price_sensitivity']
    X_test = test_df[feature_cols]
    y_test = test_df['price_sensitivity']
    
    logger.info(f"Train shape: {X_train.shape}, Test shape: {X_test.shape}")
    logger.info(f"Target distribution: {y_train.value_counts().to_dict()}")
    
    return X_train, X_test, y_train, y_test, feature_cols

def train_logistic_regression(X_train, y_train, random_state=42):
    """Train Logistic Regression model"""
    logger.info("Training Logistic Regression...")
    
    model = LogisticRegression(
        random_state=random_state,
        max_iter=1000,
        multi_class='multinomial',
        solver='lbfgs'
    )
    model.fit(X_train, y_train)
    
    return model

def train_decision_tree(X_train, y_train, random_state=42):
    """Train Decision Tree model"""
    logger.info("Training Decision Tree...")
    
    model = DecisionTreeClassifier(
        random_state=random_state,
        max_depth=10,
        min_samples_split=20,
        min_samples_leaf=10
    )
    model.fit(X_train, y_train)
    
    return model

def train_random_forest(X_train, y_train, random_state=42):
    """Train Random Forest model"""
    logger.info("Training Random Forest...")
    
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=15,
        min_samples_split=20,
        min_samples_leaf=5,
        random_state=random_state,
        n_jobs=-1
    )
    model.fit(X_train, y_train)
    
    return model

def train_xgboost(X_train, y_train, random_state=42):
    """Train XGBoost model"""
    logger.info("Training XGBoost...")
    
    # Encode target labels for XGBoost
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y_encoded = le.fit_transform(y_train)
    
    model = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=random_state,
        eval_metric='mlogloss'
    )
    model.fit(X_train, y_encoded)
    
    # Store label encoder
    model.label_encoder = le
    
    return model

def evaluate_model(model, X_train, X_test, y_train, y_test, model_name, cv_folds=5):
    """Comprehensive model evaluation"""
    logger.info(f"Evaluating {model_name}...")
    
    # Handle XGBoost predictions
    if model_name == 'xgboost':
        y_train_pred = model.label_encoder.inverse_transform(model.predict(X_train))
        y_test_pred = model.label_encoder.inverse_transform(model.predict(X_test))
        
        # Cross-validation with encoded labels
        y_train_encoded = model.label_encoder.transform(y_train)
        cv_scores = cross_val_score(model, X_train, y_train_encoded, cv=cv_folds, scoring='accuracy')
    else:
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)
        cv_scores = cross_val_score(model, X_train, y_train, cv=cv_folds, scoring='accuracy')
    
    # Calculate metrics
    metrics = {
        'model_name': model_name,
        'train_accuracy': accuracy_score(y_train, y_train_pred),
        'test_accuracy': accuracy_score(y_test, y_test_pred),
        'test_precision': precision_score(y_test, y_test_pred, average='macro'),
        'test_recall': recall_score(y_test, y_test_pred, average='macro'),
        'test_f1': f1_score(y_test, y_test_pred, average='macro'),
        'cv_accuracy_mean': cv_scores.mean(),
        'cv_accuracy_std': cv_scores.std()
    }
    
    # Classification report
    class_report = classification_report(y_test, y_test_pred, output_dict=True)
    
    # Confusion matrix
    cm = confusion_matrix(y_test, y_test_pred)
    
    logger.info(f"{model_name} - Test Accuracy: {metrics['test_accuracy']:.4f}, F1: {metrics['test_f1']:.4f}")
    
    return metrics, class_report, cm, y_test_pred

def save_confusion_matrix(cm, model_name, output_dir, class_names=['High', 'Low', 'Medium']):
    """Save confusion matrix plot"""
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    plt.title(f'Confusion Matrix - {model_name}')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    
    cm_path = os.path.join(output_dir, f'confusion_matrix_{model_name.lower()}.png')
    plt.savefig(cm_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    return cm_path

def compare_models(all_metrics):
    """Compare all models and select best one"""
    logger.info("Comparing models...")
    
    comparison_df = pd.DataFrame(all_metrics)
    comparison_df = comparison_df.sort_values('test_f1', ascending=False)
    
    best_model_name = comparison_df.iloc[0]['model_name']
    logger.info(f"Best model: {best_model_name} (F1: {comparison_df.iloc[0]['test_f1']:.4f})")
    
    return comparison_df, best_model_name

def main():
    """Main training function"""
    args = parse_args()
    
    try:
        # Load data
        X_train, X_test, y_train, y_test, feature_cols = load_data(args.train, args.test)
        
        # Initialize results storage
        all_models = {}
        all_metrics = []
        all_reports = {}
        all_predictions = {}
        
        # Define models to train
        models_to_train = ['logistic', 'decision_tree', 'random_forest', 'xgboost'] if args.model_type == 'all' else [args.model_type]
        
        # Train models
        for model_name in models_to_train:
            logger.info(f"\n{'='*50}")
            logger.info(f"Training {model_name.upper()}")
            logger.info(f"{'='*50}")
            
            # Train model
            if model_name == 'logistic':
                model = train_logistic_regression(X_train, y_train, args.random_state)
            elif model_name == 'decision_tree':
                model = train_decision_tree(X_train, y_train, args.random_state)
            elif model_name == 'random_forest':
                model = train_random_forest(X_train, y_train, args.random_state)
            elif model_name == 'xgboost':
                model = train_xgboost(X_train, y_train, args.random_state)
            
            # Evaluate model
            metrics, class_report, cm, y_pred = evaluate_model(
                model, X_train, X_test, y_train, y_test, model_name, args.cv_folds
            )
            
            # Store results
            all_models[model_name] = model
            all_metrics.append(metrics)
            all_reports[model_name] = class_report
            all_predictions[model_name] = y_pred
            
            # Save confusion matrix
            save_confusion_matrix(cm, model_name, args.model_dir)
        
        # Compare models
        comparison_df, best_model_name = compare_models(all_metrics)
        
        # Save best model
        best_model = all_models[best_model_name]
        joblib.dump(best_model, os.path.join(args.model_dir, 'model.joblib'))
        
        # Save all evaluation results
        evaluation_results = {
            'model_comparison': comparison_df.to_dict('records'),
            'best_model': best_model_name,
            'classification_reports': all_reports,
            'feature_columns': feature_cols,
            'target_classes': ['High', 'Low', 'Medium'],
            'training_timestamp': datetime.now().isoformat()
        }
        
        with open(os.path.join(args.model_dir, 'evaluation_results.json'), 'w') as f:
            json.dump(evaluation_results, f, indent=2)
        
        # Save model metadata
        metadata = {
            'model_type': best_model_name,
            'accuracy': float(comparison_df.iloc[0]['test_accuracy']),
            'f1_score': float(comparison_df.iloc[0]['test_f1']),
            'precision': float(comparison_df.iloc[0]['test_precision']),
            'recall': float(comparison_df.iloc[0]['test_recall']),
            'cv_accuracy': float(comparison_df.iloc[0]['cv_accuracy_mean']),
            'feature_count': len(feature_cols),
            'train_samples': len(X_train),
            'test_samples': len(X_test)
        }
        
        with open(os.path.join(args.model_dir, 'model_metadata.json'), 'w') as f:
            json.dump(metadata, f, indent=2)
        
        logger.info(f"\nâœ… Training completed successfully!")
        logger.info(f"Best model: {best_model_name}")
        logger.info(f"Test accuracy: {metadata['accuracy']:.4f}")
        logger.info(f"Test F1-score: {metadata['f1_score']:.4f}")
        
    except Exception as e:
        logger.error(f"âŒ Training failed: {str(e)}")
        raise

if __name__ == '__main__':
    main()
```

### 2.2. SageMaker Training Job Setup

**Khá»Ÿi cháº¡y multi-model training:**
```python
import boto3
import sagemaker
from sagemaker.sklearn import SKLearn
from sagemaker.inputs import TrainingInput
from datetime import datetime

# Initialize SageMaker
sagemaker_session = sagemaker.Session()
role = sagemaker.get_execution_role()
bucket_name = 'mlops-retail-prediction-dev-123456789012'

# Define hyperparameters
hyperparameters = {
    'model-type': 'all',  # Train all models
    'random-state': 42,
    'cv-folds': 5
}

# Create SKLearn estimator with Spot instances
sklearn_estimator = SKLearn(
    entry_point='train_multi_model.py',
    source_dir='.',
    role=role,
    instance_type='ml.m5.large',
    instance_count=1,
    framework_version='1.0-1',
    py_version='py3',
    output_path=f's3://{bucket_name}/artifacts/',
    hyperparameters=hyperparameters,
    max_run=3600,  # 1 hour timeout
    use_spot_instances=True,  # Cost optimization
    max_wait=7200,  # 2 hours max wait
    checkpoint_s3_uri=f's3://{bucket_name}/checkpoints',
    base_job_name='retail-prediction-training'
)

# Define input data channels
train_input = TrainingInput(
    s3_data=f's3://{bucket_name}/gold/',
    content_type='application/x-parquet',
    s3_data_type='S3Prefix',
    input_mode='File'
)

# Start training job
job_name = f"retail-prediction-{datetime.now().strftime('%Y%m%d-%H%M%S')}"

sklearn_estimator.fit(
    {
        'train': train_input,
        'test': train_input
    },
    job_name=job_name,
    wait=True
)

print(f"âœ… Multi-model training completed: {job_name}")
print(f"Model artifacts: {sklearn_estimator.model_data}")
```

## 3. Model Evaluation & Analysis
### 3.1. Model Performance Analysis

**Analyze training results:**
```python
import boto3
import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def download_and_analyze_results(bucket_name, job_name):
    """Download and analyze training results"""
    s3 = boto3.client('s3')
    
    # Download evaluation results
    eval_key = f'artifacts/{job_name}/output/model.tar.gz'
    
    # Extract and load results
    import tarfile
    import tempfile
    
    with tempfile.TemporaryDirectory() as temp_dir:
        # Download model artifacts
        s3.download_file(bucket_name, eval_key, f'{temp_dir}/model.tar.gz')
        
        # Extract
        with tarfile.open(f'{temp_dir}/model.tar.gz', 'r:gz') as tar:
            tar.extractall(temp_dir)
        
        # Load evaluation results
        with open(f'{temp_dir}/evaluation_results.json', 'r') as f:
            eval_results = json.load(f)
        
        return eval_results

def create_model_comparison_chart(eval_results):
    """Create model comparison visualization"""
    comparison_df = pd.DataFrame(eval_results['model_comparison'])
    
    # Metrics to compare
    metrics = ['test_accuracy', 'test_f1', 'test_precision', 'test_recall']
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.ravel()
    
    for i, metric in enumerate(metrics):
        ax = axes[i]
        bars = ax.bar(comparison_df['model_name'], comparison_df[metric])
        ax.set_title(f'{metric.replace("_", " ").title()}')
        ax.set_ylabel('Score')
        ax.set_ylim(0, 1)
        
        # Add value labels on bars
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{height:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return comparison_df

def analyze_confusion_matrices(eval_results):
    """Analyze confusion matrices for each model"""
    models = eval_results['model_comparison']
    
    for model_info in models:
        model_name = model_info['model_name']
        print(f"\n{'='*50}")
        print(f"CONFUSION MATRIX ANALYSIS - {model_name.upper()}")
        print(f"{'='*50}")
        
        # Get classification report
        if model_name in eval_results['classification_reports']:
            report = eval_results['classification_reports'][model_name]
            
            print(f"Per-class Performance:")
            for class_name in ['High', 'Low', 'Medium']:
                if class_name in report:
                    metrics = report[class_name]
                    print(f"  {class_name:8}: Precision={metrics['precision']:.3f}, "
                         f"Recall={metrics['recall']:.3f}, F1={metrics['f1-score']:.3f}")
            
            # Overall metrics
            macro_avg = report['macro avg']
            print(f"\n  Overall: Precision={macro_avg['precision']:.3f}, "
                 f"Recall={macro_avg['recall']:.3f}, F1={macro_avg['f1-score']:.3f}")

# Usage
eval_results = download_and_analyze_results(bucket_name, job_name)
comparison_df = create_model_comparison_chart(eval_results)
analyze_confusion_matrices(eval_results)
```

### 3.2. Business Impact Analysis

**Analyze business metrics:**
```python
def calculate_business_impact(eval_results, revenue_per_prediction=10):
    """Calculate business impact of model predictions"""
    
    best_model = eval_results['best_model']
    best_model_metrics = None
    
    for model_info in eval_results['model_comparison']:
        if model_info['model_name'] == best_model:
            best_model_metrics = model_info
            break
    
    if not best_model_metrics:
        return
    
    # Calculate business metrics
    accuracy = best_model_metrics['test_accuracy']
    f1_score = best_model_metrics['test_f1']
    
    # Estimate business impact
    daily_predictions = 1000  # Estimate
    monthly_predictions = daily_predictions * 30
    
    # Revenue impact
    accurate_predictions = monthly_predictions * accuracy
    revenue_from_accurate_predictions = accurate_predictions * revenue_per_prediction
    
    # Cost savings from automated segmentation
    manual_cost_per_prediction = 2  # USD
    automation_savings = monthly_predictions * manual_cost_per_prediction
    
    business_impact = {
        'model_name': best_model,
        'accuracy': accuracy,
        'f1_score': f1_score,
        'monthly_predictions': monthly_predictions,
        'accurate_predictions': int(accurate_predictions),
        'monthly_revenue_impact': revenue_from_accurate_predictions,
        'monthly_cost_savings': automation_savings,
        'total_monthly_value': revenue_from_accurate_predictions + automation_savings
    }
    
    print(f"\n{'='*60}")
    print(f"BUSINESS IMPACT ANALYSIS - {best_model.upper()}")
    print(f"{'='*60}")
    print(f"Model Accuracy: {accuracy:.1%}")
    print(f"Model F1-Score: {f1_score:.3f}")
    print(f"Monthly Predictions: {monthly_predictions:,}")
    print(f"Accurate Predictions: {int(accurate_predictions):,}")
    print(f"Revenue Impact: ${revenue_from_accurate_predictions:,.2f}/month")
    print(f"Cost Savings: ${automation_savings:,.2f}/month")
    print(f"Total Value: ${business_impact['total_monthly_value']:,.2f}/month")
    
    return business_impact

# Calculate business impact
business_impact = calculate_business_impact(eval_results)
```

## 4. Model Registry & Versioning

### 4.1. Model Package Group Setup

**Create Model Package Group:**
```python
import boto3
from datetime import datetime

def create_model_package_group():
    """Create SageMaker Model Package Group"""
    sm_client = boto3.client('sagemaker')
    
    group_name = "retail-price-sensitivity-models"
    
    try:
        response = sm_client.create_model_package_group(
            ModelPackageGroupName=group_name,
            ModelPackageGroupDescription="Retail BASKET_PRICE_SENSITIVITY prediction models (Low/Medium/High classification)",
            Tags=[
                {'Key': 'Project', 'Value': 'RetailPredictionMLOps'},
                {'Key': 'Environment', 'Value': 'dev'},
                {'Key': 'ModelType', 'Value': 'Classification'},
                {'Key': 'Target', 'Value': 'BASKET_PRICE_SENSITIVITY'}
            ]
        )
        print(f"âœ… Created model package group: {group_name}")
        return group_name
        
    except sm_client.exceptions.ResourceInUse:
        print(f"â„¹ï¸ Model package group {group_name} already exists")
        return group_name

model_package_group = create_model_package_group()
```

### 4.2. Register Best Model

**Register model in Model Registry:**
```python
def register_best_model(sklearn_estimator, eval_results, model_package_group):
    """Register best performing model in SageMaker Model Registry"""
    
    # Get best model info
    best_model_name = eval_results['best_model']
    best_model_metrics = None
    
    for model_info in eval_results['model_comparison']:
        if model_info['model_name'] == best_model_name:
            best_model_metrics = model_info
            break
    
    # Create model metrics for registry
    model_metrics = {
        "accuracy": best_model_metrics['test_accuracy'],
        "f1_score": best_model_metrics['test_f1'],
        "precision": best_model_metrics['test_precision'],
        "recall": best_model_metrics['test_recall'],
        "cv_accuracy": best_model_metrics['cv_accuracy_mean']
    }
    
    # Register model package
    model_package_arn = sklearn_estimator.register(
        content_types=["application/json", "text/csv"],
        response_types=["application/json"],
        inference_instances=["ml.t2.medium", "ml.m5.large"],
        transform_instances=["ml.m5.large"],
        model_package_group_name=model_package_group,
        approval_status="PendingManualApproval",
        description=f"Retail price sensitivity model - {best_model_name} (Accuracy: {best_model_metrics['test_accuracy']:.3f})",
        model_metrics={
            "accuracy": best_model_metrics['test_accuracy'],
            "f1_score": best_model_metrics['test_f1']
        },
        metadata_properties={
            "training_job_name": sklearn_estimator._current_job_name,
            "model_algorithm": best_model_name,
            "target_variable": "BASKET_PRICE_SENSITIVITY",
            "dataset_size": eval_results.get('train_samples', 'unknown'),
            "feature_count": len(eval_results.get('feature_columns', [])),
            "training_timestamp": datetime.now().isoformat()
        },
        customer_metadata_properties={
            "business_kpi": "customer_segmentation",
            "use_case": "retail_price_sensitivity_prediction",
            "model_version": "v1.0",
            "data_source": "dunnhumby_retail_transactions"
        }
    )
    
    print(f"âœ… Model registered: {model_package_arn}")
    print(f"Model algorithm: {best_model_name}")
    print(f"Accuracy: {best_model_metrics['test_accuracy']:.3f}")
    print(f"F1-score: {best_model_metrics['test_f1']:.3f}")
    
    return model_package_arn

# Register the best model
model_package_arn = register_best_model(sklearn_estimator, eval_results, model_package_group)
```

### 4.3. Model Approval Workflow

**Automated model approval based on performance:**
```python
def approve_model_if_meets_criteria(model_package_arn, eval_results, 
                                   min_accuracy=0.80, min_f1=0.70):
    """Automatically approve model if it meets performance criteria"""
    
    sm_client = boto3.client('sagemaker')
    
    # Get best model metrics
    best_model_metrics = eval_results['model_comparison'][0]  # Already sorted by F1
    
    accuracy = best_model_metrics['test_accuracy']
    f1_score = best_model_metrics['test_f1']
    
    # Check if model meets criteria
    meets_criteria = accuracy >= min_accuracy and f1_score >= min_f1
    
    if meets_criteria:
        # Approve model
        sm_client.update_model_package(
            ModelPackageArn=model_package_arn,
            ModelApprovalStatus="Approved",
            ApprovalDescription=f"Auto-approved: Accuracy={accuracy:.3f} (â‰¥{min_accuracy}), F1={f1_score:.3f} (â‰¥{min_f1})"
        )
        
        print(f"âœ… Model AUTO-APPROVED for production")
        print(f"   Accuracy: {accuracy:.3f} (required: â‰¥{min_accuracy})")
        print(f"   F1-score: {f1_score:.3f} (required: â‰¥{min_f1})")
        
        return "Approved"
    else:
        print(f"âš ï¸ Model requires MANUAL REVIEW")
        print(f"   Accuracy: {accuracy:.3f} (required: â‰¥{min_accuracy}) {'âœ…' if accuracy >= min_accuracy else 'âŒ'}")
        print(f"   F1-score: {f1_score:.3f} (required: â‰¥{min_f1}) {'âœ…' if f1_score >= min_f1 else 'âŒ'}")
        
        return "PendingManualApproval"

# Auto-approve if meets criteria
approval_status = approve_model_if_meets_criteria(model_package_arn, eval_results)
```

## 5. CI/CD Integration & Deployment Preparation

### 5.1. Export Model Information

**Prepare model info for CI/CD pipeline:**
```python
def export_model_for_deployment(model_package_arn, eval_results, output_bucket):
    """Export model information for CI/CD deployment"""
    
    sm_client = boto3.client('sagemaker')
    s3_client = boto3.client('s3')
    
    # Get model package details
    model_package = sm_client.describe_model_package(ModelPackageName=model_package_arn)
    
    # Best model info
    best_model = eval_results['model_comparison'][0]
    
    # Create deployment configuration
    deployment_config = {
        "model_info": {
            "model_package_arn": model_package_arn,
            "model_data_url": model_package['InferenceSpecification']['Containers'][0]['ModelDataUrl'],
            "image_uri": model_package['InferenceSpecification']['Containers'][0]['Image'],
            "model_name": best_model['model_name'],
            "approval_status": model_package['ModelApprovalStatus']
        },
        "performance_metrics": {
            "accuracy": best_model['test_accuracy'],
            "f1_score": best_model['test_f1'],
            "precision": best_model['test_precision'],
            "recall": best_model['test_recall'],
            "cv_accuracy": best_model['cv_accuracy_mean']
        },
        "deployment_config": {
            "endpoint_config": {
                "instance_type": "ml.t2.medium",
                "initial_instance_count": 1,
                "max_instance_count": 10
            },
            "auto_scaling": {
                "min_capacity": 1,
                "max_capacity": 10,
                "target_cpu_utilization": 70
            }
        },
        "feature_info": {
            "feature_columns": eval_results['feature_columns'],
            "target_classes": eval_results['target_classes']
        },
        "metadata": {
            "training_timestamp": eval_results['training_timestamp'],
            "model_version": "v1.0",
            "export_timestamp": datetime.now().isoformat()
        }
    }
    
    # Save deployment config locally
    with open('model_deployment_config.json', 'w') as f:
        json.dump(deployment_config, f, indent=2)
    
    # Upload to S3 for CI/CD access
    config_key = f'deployment-configs/model_deployment_config_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    s3_client.upload_file('model_deployment_config.json', output_bucket, config_key)
    
    # Also create "latest" version for CI/CD
    s3_client.upload_file('model_deployment_config.json', output_bucket, 'deployment-configs/latest_model_config.json')
    
    print(f"âœ… Deployment config exported:")
    print(f"   S3 location: s3://{output_bucket}/{config_key}")
    print(f"   Latest config: s3://{output_bucket}/deployment-configs/latest_model_config.json")
    print(f"   Model ready for deployment: {deployment_config['model_info']['approval_status']}")
    
    return deployment_config

# Export for deployment
deployment_config = export_model_for_deployment(model_package_arn, eval_results, bucket_name)
```

### 5.2. Training Pipeline Summary

**Complete pipeline execution summary:**
```python
def print_pipeline_summary(eval_results, deployment_config, business_impact):
    """Print comprehensive pipeline execution summary"""
    
    print(f"\n{'='*80}")
    print(f"ğŸ¯ MLOPS PIPELINE EXECUTION SUMMARY")
    print(f"{'='*80}")
    
    # ETL Summary
    print(f"\nğŸ“Š ETL PIPELINE:")
    print(f"   âœ… Raw data processed from S3 raw/ â†’ gold/")
    print(f"   âœ… Features engineered and saved as Parquet")
    print(f"   âœ… Train/test split completed")
    
    # Training Summary
    print(f"\nğŸ¤– MODEL TRAINING:")
    best_model = eval_results['model_comparison'][0]
    print(f"   âœ… Multi-model training completed (LR, DT, RF, XGBoost)")
    print(f"   ğŸ† Best model: {best_model['model_name']}")
    print(f"   ğŸ“ˆ Test accuracy: {best_model['test_accuracy']:.3f}")
    print(f"   ğŸ“ˆ Test F1-score: {best_model['test_f1']:.3f}")
    print(f"   ğŸ“ˆ Cross-validation: {best_model['cv_accuracy_mean']:.3f} Â± {best_model['cv_accuracy_std']:.3f}")
    
    # Model Registry
    print(f"\nğŸ“‹ MODEL REGISTRY:")
    print(f"   âœ… Model registered in SageMaker Model Registry")
    print(f"   ğŸ“¦ Package ARN: {deployment_config['model_info']['model_package_arn'].split('/')[-1]}")
    print(f"   ğŸ”– Approval status: {deployment_config['model_info']['approval_status']}")
    
    # Business Impact
    if business_impact:
        print(f"\nğŸ’¼ BUSINESS IMPACT:")
        print(f"   ğŸ’° Monthly revenue impact: ${business_impact['monthly_revenue_impact']:,.2f}")
        print(f"   ğŸ’° Monthly cost savings: ${business_impact['monthly_cost_savings']:,.2f}")
        print(f"   ğŸ’° Total monthly value: ${business_impact['total_monthly_value']:,.2f}")
    
    # Deployment Readiness
    print(f"\nğŸš€ DEPLOYMENT READINESS:")
    print(f"   âœ… Model artifacts ready in S3")
    print(f"   âœ… Deployment config exported")
    print(f"   âœ… CI/CD pipeline can proceed to Task 10 (EKS Deployment)")
    
    print(f"\n{'='*80}")

# Print complete summary
print_pipeline_summary(eval_results, deployment_config, business_impact)
```

## ğŸ‘‰ Káº¿t quáº£ Task 4

âœ… **ETL Pipeline** - Automated raw â†’ gold data processing hoÃ n thÃ nh  
âœ… **Multi-Model Training** - LR, Decision Tree, Random Forest, XGBoost comparison  
âœ… **Model Evaluation** - Comprehensive metrics + confusion matrix analysis  
âœ… **Best Model Selection** - Tá»± Ä‘á»™ng chá»n model tá»‘t nháº¥t dá»±a trÃªn F1-score  
âœ… **Model Registry** - Version control vÃ  approval workflow  
âœ… **Business Impact** - Revenue vÃ  cost savings analysis  
âœ… **CI/CD Ready** - Deployment config exported cho EKS deployment  

**ğŸ¯ Model Performance**: Accuracy â‰¥ **80%**, F1-score â‰¥ **0.7**  
**ğŸ’° Cost Optimization**: Spot instances, auto-cleanup  
**ğŸ”„ Automation**: End-to-end ETL â†’ Train â†’ Evaluate â†’ Register â†’ Export  

{{% notice success %}}
**ğŸ¯ Task 4 Complete - MLOps Core Pipeline!**

**ETL Automation**: Raw transaction data â†’ ML-ready features  
**Multi-Model Training**: Comprehensive algorithm comparison  
**Performance Validation**: Accuracy + F1 + business impact analysis  
**Model Registry**: Version control vá»›i automated approval  
**CI/CD Integration**: Deployment config ready cho Task 10 (EKS)  
**Next**: Task 5 - VPC Networking setup
{{% /notice %}}

{{% notice tip %}}
**ğŸš€ Next Steps:**
- **Task 5**: VPC networking cho secure model serving
- **Task 6**: ECR container registry cho prediction API  
- **Task 7**: EKS cluster setup
- **Task 10**: Deploy prediction API vá»›i best model
{{% /notice %}}

{{% notice info %}}
**ğŸ“Š Achieved Performance Benchmarks:**
- **ETL Processing**: Automated feature engineering pipeline
- **Model Training**: Multi-algorithm comparison (LR/DT/RF/XGBoost)
- **Accuracy Target**: â‰¥80% classification accuracy achieved
- **F1-Score Target**: â‰¥0.70 macro F1-score achieved  
- **Business Value**: Revenue impact + cost savings quantified
- **Automation**: End-to-end pipeline vá»›i minimal manual intervention
{{% /notice %}}