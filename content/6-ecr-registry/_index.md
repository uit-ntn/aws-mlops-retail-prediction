---
title: "ECR Container Registry"
date: 2024-01-01T00:00:00+07:00
weight: 6
chapter: false
pre: "<b>6. </b>"
---

{{% notice info %}}
**üéØ M·ª•c ti√™u Task 6:**

Thi·∫øt l·∫≠p Amazon Elastic Container Registry (ECR) cho retail prediction MLOps pipeline:

1. **API Container**: FastAPI prediction service v·ªõi model loader t·ª´ S3
2. **Training Container**: Custom model training image (optional)
3. **Automated CI/CD**: Build ‚Üí Push ‚Üí Deploy workflow
4. **Security**: Image scanning, encryption, access control, lifecycle policies

‚Üí ƒê·∫£m b·∫£o quy tr√¨nh build‚Äìpush‚Äìdeploy image di·ªÖn ra t·ª± ƒë·ªông, an to√†n, v√† d·ªÖ m·ªü r·ªông.
{{% /notice %}}

## T·ªïng quan

**Amazon ECR (Elastic Container Registry)** l√† d·ªãch v·ª• Docker container registry ƒë∆∞·ª£c qu·∫£n l√Ω ho√†n to√†n b·ªüi AWS, t√≠ch h·ª£p s√¢u v·ªõi EKS v√† CI/CD pipeline. ECR cung c·∫•p kh·∫£ nƒÉng l∆∞u tr·ªØ, qu·∫£n l√Ω v√† tri·ªÉn khai container images m·ªôt c√°ch an to√†n cho MLOps workflow.

### Ki·∫øn tr√∫c ECR trong MLOps Pipeline

```
MLOps Container Strategy:
‚îú‚îÄ‚îÄ mlops/retail-api (FastAPI Prediction Service)
‚îÇ   ‚îú‚îÄ‚îÄ FastAPI app (main.py)
‚îÇ   ‚îú‚îÄ‚îÄ Model loader (t·ª´ S3 artifacts)
‚îÇ   ‚îú‚îÄ‚îÄ Dependencies (pandas, scikit-learn, xgboost, boto3)
‚îÇ   ‚îî‚îÄ‚îÄ Health checks & monitoring
‚îú‚îÄ‚îÄ mlops/train-model (Optional Training Container)
‚îÇ   ‚îú‚îÄ‚îÄ Training scripts (train.py, evaluate.py)
‚îÇ   ‚îú‚îÄ‚îÄ Data processing (feature engineering)
‚îÇ   ‚îú‚îÄ‚îÄ Model export to S3
‚îÇ   ‚îî‚îÄ‚îÄ MLflow integration
‚îî‚îÄ‚îÄ CI/CD Integration
    ‚îú‚îÄ‚îÄ GitHub Actions / Jenkins
    ‚îú‚îÄ‚îÄ Automated build on code changes
    ‚îú‚îÄ‚îÄ Image scanning & security
    ‚îî‚îÄ‚îÄ Auto-deploy to EKS

Security & Lifecycle:
‚îú‚îÄ‚îÄ Image scanning on push (vulnerability detection)
‚îú‚îÄ‚îÄ Tag immutability (reproducibility)
‚îú‚îÄ‚îÄ Encryption (AES-256)
‚îú‚îÄ‚îÄ IAM-based access control
‚îî‚îÄ‚îÄ Lifecycle policies (automatic cleanup)
```

### Th√†nh ph·∫ßn ch√≠nh

1. **ECR Repositories**: Separate repos cho API v√† training images
2. **Image Scanning**: Automatic vulnerability detection on push
3. **Lifecycle Policies**: Auto-cleanup old images
4. **IAM Integration**: EKS pull access, CI/CD push permissions
5. **CI/CD Automation**: Build ‚Üí Push ‚Üí Deploy workflow

## 1. ECR Repositories Setup

### 1.1. Create ECR Repositories

1. **Navigate to ECR Console:**
   - ƒêƒÉng nh·∫≠p AWS Console
   - Navigate to ECR service
   - Region: ap-southeast-1
   - Ch·ªçn "Create repository"

{{< imgborder src="/images/06-ecr-registry/01-create-repository.png" title="Navigate to ECR Console" >}}

2. **API Repository Configuration:**
   ```
   Visibility settings: Private
   Repository name: mlops/retail-api
   Tag immutability: ‚úÖ Enabled (Reproducibility)
   Image scan settings: ‚úÖ Scan on push
   Encryption settings: AES-256 (Default)
   ```

{{< imgborder src="/images/06-ecr-registry/02-api-repository-config.png" title="Configure API repository" >}}

3. **Training Repository Configuration (Optional):**
   ```
   Visibility settings: Private
   Repository name: mlops/train-model
   Tag immutability: ‚úÖ Enabled
   Image scan settings: ‚úÖ Scan on push
   Encryption settings: AES-256
   ```

{{< imgborder src="/images/06-ecr-registry/03-training-repository-config.png" title="Configure training repository" >}}

4. **Repository Creation Complete:**
   ```
   API Repository URI: 123456789012.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api
   Training Repository URI: 123456789012.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/train-model
   ```

{{< imgborder src="/images/06-ecr-registry/04-repositories-created.png" title="ECR repositories created successfully" >}}

### 1.2. Repository Policies Configuration

1. **API Repository Access Policy:**
   - Ch·ªçn repository `mlops/retail-api`
   - Click "Permissions" tab ‚Üí "Edit policy JSON"

{{< imgborder src="/images/06-ecr-registry/05-api-repository-permissions.png" title="Configure API repository permissions" >}}

2. **Configure API Repository Policy:**
   ```json
   {
     "Version": "2008-10-17",
     "Statement": [
       {
         "Sid": "AllowEKSNodeGroupPull",
         "Effect": "Allow",
         "Principal": {
           "AWS": "arn:aws:iam::123456789012:role/mlops-hybrid-eks-nodes-role"
         },
         "Action": [
           "ecr:BatchCheckLayerAvailability",
           "ecr:GetDownloadUrlForLayer",
           "ecr:BatchGetImage"
         ]
       },
       {
         "Sid": "AllowCICDPushAccess", 
         "Effect": "Allow",
         "Principal": {
           "AWS": "arn:aws:iam::123456789012:role/mlops-cicd-role"
         },
         "Action": [
           "ecr:BatchCheckLayerAvailability",
           "ecr:GetDownloadUrlForLayer", 
           "ecr:BatchGetImage",
           "ecr:PutImage",
           "ecr:InitiateLayerUpload",
           "ecr:UploadLayerPart",
           "ecr:CompleteLayerUpload"
         ]
       }
     ]
   }
   ```

{{< imgborder src="/images/06-ecr-registry/06-api-repository-policy.png" title="API repository access policy" >}}

3. **Training Repository Access Policy:**
   ```json
   {
     "Version": "2008-10-17",
     "Statement": [
       {
         "Sid": "AllowSageMakerAccess",
         "Effect": "Allow",
         "Principal": {
           "AWS": "arn:aws:iam::123456789012:role/mlops-sagemaker-execution-role"
         },
         "Action": [
           "ecr:BatchCheckLayerAvailability",
           "ecr:GetDownloadUrlForLayer",
           "ecr:BatchGetImage"
         ]
       },
       {
         "Sid": "AllowCICDPushAccess", 
         "Effect": "Allow",
         "Principal": {
           "AWS": "arn:aws:iam::123456789012:role/mlops-cicd-role"
         },
         "Action": [
           "ecr:BatchCheckLayerAvailability",
           "ecr:GetDownloadUrlForLayer", 
           "ecr:BatchGetImage",
           "ecr:PutImage",
           "ecr:InitiateLayerUpload",
           "ecr:UploadLayerPart",
           "ecr:CompleteLayerUpload"
         ]
       }
     ]
   }
   ```

### 1.3. Lifecycle Policy Setup

1. **API Repository Lifecycle Policy:**
   - Repository ‚Üí "Lifecycle policy" tab ‚Üí "Create rule"

{{< imgborder src="/images/06-ecr-registry/07-api-lifecycle-policy.png" title="Create API lifecycle policy" >}}

2. **Configure API Lifecycle Rules:**
   
   **Rule 1 - Keep Latest Production Images:**
   ```
   Rule priority: 1
   Description: Keep latest 10 production images
   Image status: Tagged
   Tag status: Starts with "v" (production versions)
   
   Match criteria:
   - Count type: imageCountMoreThan
   - Count number: 10
   
   Action: expire
   ```

   **Rule 2 - Keep Latest Development Images:**
   ```
   Rule priority: 2
   Description: Keep latest 5 development images
   Image status: Tagged
   Tag status: Starts with "dev", "feature", "main"
   
   Match criteria:
   - Count type: imageCountMoreThan
   - Count number: 5
   
   Action: expire
   ```

   **Rule 3 - Remove Old Untagged Images:**
   ```
   Rule priority: 3  
   Description: Delete untagged images after 1 day
   Image status: Untagged
   
   Match criteria:
   - Count type: sinceImagePushed
   - Count number: 1
   - Count unit: days
   
   Action: expire
   ```

{{< imgborder src="/images/06-ecr-registry/08-api-lifecycle-rules.png" title="API lifecycle policy rules" >}}

3. **Training Repository Lifecycle Policy:**
   ```
   Rule 1: Keep latest 5 training images
   Rule 2: Delete untagged images after 1 day
   Rule 3: Keep experiment images (tagged with "exp-") for 30 days max
   ```

### 1.4. Image Scanning Verification

1. **Check Scan Settings:**
   - Repository ‚Üí "Image scan settings" tab
   - Verify "Scan on push" is enabled
   - Review enhanced scanning options

{{< imgborder src="/images/06-ecr-registry/09-scan-settings.png" title="Verify image scanning configuration" >}}

2. **Enhanced Scanning (Optional):**
   ```
   ‚úÖ Basic scanning: Included with ECR (FREE)
   ‚öôÔ∏è Enhanced scanning: $0.09 per image scan (more detailed CVE detection)
   ```

{{% notice success %}}
**üéØ ECR Repositories Setup Complete!**

**Created Repositories:**
- ‚úÖ `mlops/retail-api`: FastAPI prediction service container
- ‚úÖ `mlops/train-model`: Custom training container (optional)
- ‚úÖ Image scanning enabled on push
- ‚úÖ Tag immutability enabled for reproducibility
- ‚úÖ Lifecycle policies configured for cost optimization
- ‚úÖ IAM access policies for EKS and CI/CD
{{% /notice %}}

## 2. Container Images Development

### 2.1. FastAPI Prediction Service Container

**Create directory structure:**
```
server/
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ model_loader.py
‚îú‚îÄ‚îÄ prediction_service.py
‚îî‚îÄ‚îÄ health_check.py
```

**File: `server/Dockerfile`**
```dockerfile
# Multi-stage build for FastAPI retail prediction API
FROM python:3.9-slim as builder

# Set working directory
WORKDIR /app

# Install system dependencies for ML libraries
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    libc6-dev \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --user -r requirements.txt

# Production stage
FROM python:3.9-slim

# Create non-root user for security
RUN groupadd -r apiuser && useradd -r -g apiuser apiuser

# Set working directory
WORKDIR /app

# Copy Python packages from builder stage
COPY --from=builder /root/.local /home/apiuser/.local

# Copy application code
COPY --chown=apiuser:apiuser . .

# Create directories for model and logs
RUN mkdir -p /app/models /app/logs && \
    chown -R apiuser:apiuser /app

# Switch to non-root user
USER apiuser

# Set Python path
ENV PATH=/home/apiuser/.local/bin:$PATH

# Environment variables
ENV PYTHONPATH=/app
ENV AWS_DEFAULT_REGION=ap-southeast-1
ENV MODEL_BUCKET=mlops-retail-forecast-models
ENV MODEL_PREFIX=models/retail-price-sensitivity

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=30s --retries=3 \
    CMD python health_check.py || exit 1

# Run application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]
```

**File: `server/requirements.txt`**
```txt
# FastAPI and web framework
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0

# ML libraries
scikit-learn==1.3.2
xgboost==2.0.2
pandas==2.1.3
numpy==1.25.2

# AWS SDK
boto3==1.34.0
botocore==1.34.0

# Utilities
python-json-logger==2.0.7
python-multipart==0.0.6
httpx==0.25.2

# Monitoring
prometheus-client==0.19.0
```

**File: `server/main.py`**
```python
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import logging
import time
import os
from typing import Dict, List, Optional
import json

from model_loader import ModelLoader
from prediction_service import PredictionService

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI(
    title="Retail Price Sensitivity Prediction API",
    description="MLOps retail prediction service for BASKET_PRICE_SENSITIVITY classification",
    version="1.0.0"
)

# Global model loader and prediction service
model_loader = None
prediction_service = None

@app.on_event("startup")
async def startup_event():
    """Initialize model loader and prediction service on startup"""
    global model_loader, prediction_service
    
    logger.info("Starting FastAPI application...")
    
    # Initialize model loader
    model_loader = ModelLoader(
        bucket_name=os.getenv("MODEL_BUCKET", "mlops-retail-forecast-models"),
        model_prefix=os.getenv("MODEL_PREFIX", "models/retail-price-sensitivity")
    )
    
    # Load latest model
    try:
        model_artifacts = await model_loader.load_latest_model()
        prediction_service = PredictionService(model_artifacts)
        logger.info("Model loaded successfully")
    except Exception as e:
        logger.error(f"Failed to load model: {e}")
        # Continue startup without model for health checks

# Request/Response models
class PredictionRequest(BaseModel):
    features: Dict[str, float]
    customer_id: Optional[str] = None

class PredictionResponse(BaseModel):
    prediction: str  # Low, Medium, High
    probability: Dict[str, float]
    customer_id: Optional[str] = None
    model_version: str
    prediction_time: float

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "Retail Price Sensitivity Prediction API",
        "version": "1.0.0",
        "status": "healthy" if prediction_service else "model_not_loaded"
    }

@app.get("/health")
async def health_check():
    """Health check endpoint for ALB"""
    if prediction_service is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    return {
        "status": "healthy",
        "model_loaded": True,
        "model_version": prediction_service.get_model_version(),
        "timestamp": time.time()
    }

@app.get("/ready")
async def readiness_check():
    """Readiness check endpoint for Kubernetes"""
    if prediction_service is None:
        raise HTTPException(status_code=503, detail="Service not ready")
    
    return {
        "status": "ready",
        "model_version": prediction_service.get_model_version()
    }

@app.post("/predict", response_model=PredictionResponse)
async def predict_price_sensitivity(request: PredictionRequest):
    """Predict customer price sensitivity"""
    if prediction_service is None:
        raise HTTPException(status_code=503, detail="Model not available")
    
    start_time = time.time()
    
    try:
        # Make prediction
        prediction, probabilities = prediction_service.predict(request.features)
        
        prediction_time = time.time() - start_time
        
        return PredictionResponse(
            prediction=prediction,
            probability=probabilities,
            customer_id=request.customer_id,
            model_version=prediction_service.get_model_version(),
            prediction_time=prediction_time
        )
        
    except Exception as e:
        logger.error(f"Prediction error: {e}")
        raise HTTPException(status_code=500, detail=f"Prediction failed: {str(e)}")

@app.get("/model/info")
async def model_info():
    """Get current model information"""
    if prediction_service is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    return prediction_service.get_model_info()

@app.post("/model/reload")
async def reload_model():
    """Reload model from S3 (admin endpoint)"""
    global prediction_service
    
    try:
        model_artifacts = await model_loader.load_latest_model()
        prediction_service = PredictionService(model_artifacts)
        
        return {
            "status": "success",
            "message": "Model reloaded successfully",
            "model_version": prediction_service.get_model_version()
        }
    except Exception as e:
        logger.error(f"Model reload failed: {e}")
        raise HTTPException(status_code=500, detail=f"Model reload failed: {str(e)}")

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """Global exception handler"""
    logger.error(f"Unhandled exception: {exc}")
    return JSONResponse(
        status_code=500,
        content={"detail": f"Internal server error: {str(exc)}"}
    )
```

**File: `server/model_loader.py`**
```python
import boto3
import pickle
import joblib
import json
import os
import logging
from typing import Dict, Any, Optional
import asyncio
from concurrent.futures import ThreadPoolExecutor

logger = logging.getLogger(__name__)

class ModelLoader:
    def __init__(self, bucket_name: str, model_prefix: str):
        self.bucket_name = bucket_name
        self.model_prefix = model_prefix
        self.s3_client = boto3.client('s3')
        self.executor = ThreadPoolExecutor(max_workers=2)
    
    async def load_latest_model(self) -> Dict[str, Any]:
        """Load the latest model artifacts from S3"""
        logger.info(f"Loading latest model from s3://{self.bucket_name}/{self.model_prefix}")
        
        try:
            # Get latest model artifacts
            loop = asyncio.get_event_loop()
            model_artifacts = await loop.run_in_executor(
                self.executor, self._download_model_artifacts
            )
            
            return model_artifacts
            
        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            raise
    
    def _download_model_artifacts(self) -> Dict[str, Any]:
        """Download model artifacts from S3"""
        
        # Download model file
        model_path = "/tmp/model.pkl"
        self.s3_client.download_file(
            self.bucket_name,
            f"{self.model_prefix}/model.pkl",
            model_path
        )
        
        # Load model
        with open(model_path, 'rb') as f:
            model = pickle.load(f)
        
        # Download metadata
        try:
            metadata_path = "/tmp/model_metadata.json"
            self.s3_client.download_file(
                self.bucket_name,
                f"{self.model_prefix}/model_metadata.json",
                metadata_path
            )
            
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
        except:
            logger.warning("Model metadata not found, using defaults")
            metadata = {
                "model_version": "unknown",
                "training_timestamp": "unknown",
                "model_type": "unknown"
            }
        
        # Download feature columns
        try:
            features_path = "/tmp/feature_columns.json"
            self.s3_client.download_file(
                self.bucket_name,
                f"{self.model_prefix}/feature_columns.json",
                features_path
            )
            
            with open(features_path, 'r') as f:
                feature_columns = json.load(f)
        except:
            logger.warning("Feature columns not found")
            feature_columns = []
        
        # Clean up temp files
        for temp_file in [model_path, "/tmp/model_metadata.json", "/tmp/feature_columns.json"]:
            if os.path.exists(temp_file):
                os.remove(temp_file)
        
        return {
            "model": model,
            "metadata": metadata,
            "feature_columns": feature_columns
        }
```

**File: `server/prediction_service.py`**
```python
import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Tuple, Any

logger = logging.getLogger(__name__)

class PredictionService:
    def __init__(self, model_artifacts: Dict[str, Any]):
        self.model = model_artifacts["model"]
        self.metadata = model_artifacts["metadata"]
        self.feature_columns = model_artifacts["feature_columns"]
        
        # Expected classes for price sensitivity
        self.target_classes = ["Low", "Medium", "High"]
        
        logger.info(f"Initialized prediction service with model version: {self.get_model_version()}")
    
    def predict(self, features: Dict[str, float]) -> Tuple[str, Dict[str, float]]:
        """Make prediction for customer price sensitivity"""
        
        # Convert features to DataFrame
        df = pd.DataFrame([features])
        
        # Ensure all required features are present
        missing_features = set(self.feature_columns) - set(df.columns)
        if missing_features:
            # Fill missing features with default values (0 or median)
            for feature in missing_features:
                df[feature] = 0.0
                logger.warning(f"Missing feature {feature}, using default value 0.0")
        
        # Reorder columns to match training
        df = df[self.feature_columns]
        
        # Make prediction
        prediction_proba = self.model.predict_proba(df)[0]
        predicted_class_idx = np.argmax(prediction_proba)
        predicted_class = self.target_classes[predicted_class_idx]
        
        # Create probability dictionary
        probabilities = {
            class_name: float(prob) 
            for class_name, prob in zip(self.target_classes, prediction_proba)
        }
        
        logger.info(f"Prediction: {predicted_class}, Probabilities: {probabilities}")
        
        return predicted_class, probabilities
    
    def get_model_version(self) -> str:
        """Get model version"""
        return self.metadata.get("model_version", "unknown")
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get detailed model information"""
        return {
            "model_version": self.get_model_version(),
            "model_type": self.metadata.get("model_type", "unknown"),
            "training_timestamp": self.metadata.get("training_timestamp", "unknown"),
            "feature_count": len(self.feature_columns),
            "target_classes": self.target_classes,
            "feature_columns": self.feature_columns
        }
```

**File: `server/health_check.py`**
```python
#!/usr/bin/env python3
import requests
import sys

def health_check():
    """Simple health check for Docker healthcheck"""
    try:
        response = requests.get("http://localhost:8000/health", timeout=5)
        if response.status_code == 200:
            print("Health check passed")
            return 0
        else:
            print(f"Health check failed with status {response.status_code}")
            return 1
    except Exception as e:
        print(f"Health check failed: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(health_check())
```

### 2.2. Training Container (Optional)

**Create directory structure:**
```
training/
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ train.py
‚îú‚îÄ‚îÄ evaluate.py
‚îî‚îÄ‚îÄ export_model.py
```

**File: `training/Dockerfile`**
```dockerfile
# Training container for retail price sensitivity model
FROM python:3.9

# Set working directory
WORKDIR /opt/ml

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy training scripts
COPY . .

# Set executable permissions
RUN chmod +x train.py evaluate.py export_model.py

# Default command
CMD ["python", "train.py"]
```

**File: `training/requirements.txt`**
```txt
# ML libraries
scikit-learn==1.3.2
xgboost==2.0.2
pandas==2.1.3
numpy==1.25.2

# AWS SDK
boto3==1.34.0

# Experiment tracking
mlflow==2.8.1

# Utilities
joblib==1.3.2
```

{{% notice success %}}
**üéØ Container Images Ready!**

**FastAPI Prediction Service:**
- ‚úÖ Multi-stage Docker build for size optimization
- ‚úÖ Model loader t·ª´ S3 with automatic refresh
- ‚úÖ Health checks for ALB and Kubernetes
- ‚úÖ Comprehensive prediction API with error handling
- ‚úÖ Non-root user for security

**Training Container (Optional):**
- ‚úÖ Custom training environment
- ‚úÖ MLflow integration
- ‚úÖ Model export to S3
{{% /notice %}}

## 3. Build & Push Automation

### 3.1. Local Build and Push Script

**Create file `scripts/build-and-push.sh`:**

```bash
#!/bin/bash

set -e

# Configuration
PROJECT_NAME="mlops-retail-forecast"
ENVIRONMENT="dev"
AWS_REGION="ap-southeast-1"
AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)

# Repository URLs
API_REPO="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/mlops/retail-api"
TRAINING_REPO="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/mlops/train-model"

# Get Git information
GIT_COMMIT=$(git rev-parse --short HEAD)
GIT_BRANCH=$(git rev-parse --abbrev-ref HEAD)
BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ')

# Image tags
if [[ "$GIT_BRANCH" == "main" ]]; then
    IMAGE_TAG="v$(date +%Y%m%d)-${GIT_COMMIT}"
    LATEST_TAG="latest"
elif [[ "$GIT_BRANCH" == "develop" ]]; then
    IMAGE_TAG="dev-${GIT_COMMIT}"
    LATEST_TAG="dev-latest"
else
    IMAGE_TAG="feature-${GIT_BRANCH}-${GIT_COMMIT}"
    LATEST_TAG=""
fi

echo "üöÄ Building and pushing Docker images to ECR..."
echo "API Repository: ${API_REPO}"
echo "Training Repository: ${TRAINING_REPO}"
echo "Git Branch: ${GIT_BRANCH}"
echo "Image Tag: ${IMAGE_TAG}"

# Login to ECR
echo "üîê Logging in to ECR..."
aws ecr get-login-password --region ${AWS_REGION} | \
    docker login --username AWS --password-stdin ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com

# Build API Image
echo "üèóÔ∏è Building FastAPI prediction service..."
docker build \
    --build-arg BUILD_DATE=${BUILD_DATE} \
    --build-arg GIT_COMMIT=${GIT_COMMIT} \
    --build-arg GIT_BRANCH=${GIT_BRANCH} \
    --build-arg VERSION=${IMAGE_TAG} \
    -t mlops-retail-api:${IMAGE_TAG} \
    -f server/Dockerfile \
    server/

# Tag API images
echo "üè∑Ô∏è Tagging API images..."
docker tag mlops-retail-api:${IMAGE_TAG} ${API_REPO}:${IMAGE_TAG}
if [[ -n "$LATEST_TAG" ]]; then
    docker tag mlops-retail-api:${IMAGE_TAG} ${API_REPO}:${LATEST_TAG}
fi

# Push API images
echo "üì§ Pushing API images to ECR..."
docker push ${API_REPO}:${IMAGE_TAG}
if [[ -n "$LATEST_TAG" ]]; then
    docker push ${API_REPO}:${LATEST_TAG}
fi

# Build Training Image (optional)
if [[ -f "training/Dockerfile" ]]; then
    echo "üèóÔ∏è Building training container..."
    docker build \
        --build-arg BUILD_DATE=${BUILD_DATE} \
        --build-arg GIT_COMMIT=${GIT_COMMIT} \
        --build-arg GIT_BRANCH=${GIT_BRANCH} \
        -t mlops-train-model:${IMAGE_TAG} \
        -f training/Dockerfile \
        training/

    # Tag training images
    echo "üè∑Ô∏è Tagging training images..."
    docker tag mlops-train-model:${IMAGE_TAG} ${TRAINING_REPO}:${IMAGE_TAG}
    if [[ -n "$LATEST_TAG" ]]; then
        docker tag mlops-train-model:${IMAGE_TAG} ${TRAINING_REPO}:${LATEST_TAG}
    fi

    # Push training images
    echo "üì§ Pushing training images to ECR..."
    docker push ${TRAINING_REPO}:${IMAGE_TAG}
    if [[ -n "$LATEST_TAG" ]]; then
        docker push ${TRAINING_REPO}:${LATEST_TAG}
    fi
else
    echo "‚ÑπÔ∏è Training Dockerfile not found, skipping training image build"
fi

# Clean up local images
echo "üßπ Cleaning up local images..."
docker rmi mlops-retail-api:${IMAGE_TAG} || true
docker rmi ${API_REPO}:${IMAGE_TAG} || true
if [[ -n "$LATEST_TAG" ]]; then
    docker rmi ${API_REPO}:${LATEST_TAG} || true
fi

if [[ -f "training/Dockerfile" ]]; then
    docker rmi mlops-train-model:${IMAGE_TAG} || true
    docker rmi ${TRAINING_REPO}:${IMAGE_TAG} || true
    if [[ -n "$LATEST_TAG" ]]; then
        docker rmi ${TRAINING_REPO}:${LATEST_TAG} || true
    fi
fi

echo "‚úÖ Build and push completed successfully!"
echo "üìç API Image URLs:"
echo "   - ${API_REPO}:${IMAGE_TAG}"
if [[ -n "$LATEST_TAG" ]]; then
    echo "   - ${API_REPO}:${LATEST_TAG}"
fi

if [[ -f "training/Dockerfile" ]]; then
    echo "üìç Training Image URLs:"
    echo "   - ${TRAINING_REPO}:${IMAGE_TAG}"
    if [[ -n "$LATEST_TAG" ]]; then
        echo "   - ${TRAINING_REPO}:${LATEST_TAG}"
    fi
fi
```

**Make script executable:**
```bash
chmod +x scripts/build-and-push.sh
```

### 3.2. PowerShell Build Script (Windows)

**Create file `scripts/build-and-push.ps1`:**

```powershell
# Build and push script for Windows
param(
    [string]$Environment = "dev",
    [string]$Region = "ap-southeast-1"
)

# Configuration
$ProjectName = "mlops-retail-forecast"
$AwsAccountId = (aws sts get-caller-identity --query Account --output text)
$ApiRepo = "$AwsAccountId.dkr.ecr.$Region.amazonaws.com/mlops/retail-api"
$TrainingRepo = "$AwsAccountId.dkr.ecr.$Region.amazonaws.com/mlops/train-model"

# Get Git information
$GitCommit = (git rev-parse --short HEAD)
$GitBranch = (git rev-parse --abbrev-ref HEAD)
$BuildDate = (Get-Date).ToUniversalTime().ToString("yyyy-MM-ddTHH:mm:ssZ")

# Determine image tags based on branch
if ($GitBranch -eq "main") {
    $ImageTag = "v$(Get-Date -Format 'yyyyMMdd')-$GitCommit"
    $LatestTag = "latest"
} elseif ($GitBranch -eq "develop") {
    $ImageTag = "dev-$GitCommit"
    $LatestTag = "dev-latest"
} else {
    $ImageTag = "feature-$GitBranch-$GitCommit"
    $LatestTag = ""
}

Write-Host "üöÄ Building and pushing Docker images to ECR..." -ForegroundColor Green
Write-Host "API Repository: $ApiRepo" -ForegroundColor Yellow
Write-Host "Training Repository: $TrainingRepo" -ForegroundColor Yellow
Write-Host "Git Branch: $GitBranch" -ForegroundColor Yellow
Write-Host "Image Tag: $ImageTag" -ForegroundColor Yellow

# Login to ECR
Write-Host "üîê Logging in to ECR..." -ForegroundColor Blue
$LoginCommand = aws ecr get-login-password --region $Region
$LoginCommand | docker login --username AWS --password-stdin "$AwsAccountId.dkr.ecr.$Region.amazonaws.com"

if ($LASTEXITCODE -ne 0) {
    Write-Error "Failed to login to ECR"
    exit 1
}

# Build API Image
Write-Host "üèóÔ∏è Building FastAPI prediction service..." -ForegroundColor Blue
docker build `
    --build-arg BUILD_DATE=$BuildDate `
    --build-arg GIT_COMMIT=$GitCommit `
    --build-arg GIT_BRANCH=$GitBranch `
    --build-arg VERSION=$ImageTag `
    -t "mlops-retail-api:$ImageTag" `
    -f server/Dockerfile `
    server/

if ($LASTEXITCODE -ne 0) {
    Write-Error "Failed to build API image"
    exit 1
}

# Tag API images
Write-Host "üè∑Ô∏è Tagging API images..." -ForegroundColor Blue
docker tag "mlops-retail-api:$ImageTag" "${ApiRepo}:$ImageTag"
if ($LatestTag) {
    docker tag "mlops-retail-api:$ImageTag" "${ApiRepo}:$LatestTag"
}

# Push API images
Write-Host "üì§ Pushing API images to ECR..." -ForegroundColor Blue
docker push "${ApiRepo}:$ImageTag"
if ($LatestTag) {
    docker push "${ApiRepo}:$LatestTag"
}

# Build Training Image (optional)
if (Test-Path "training/Dockerfile") {
    Write-Host "üèóÔ∏è Building training container..." -ForegroundColor Blue
    docker build `
        --build-arg BUILD_DATE=$BuildDate `
        --build-arg GIT_COMMIT=$GitCommit `
        --build-arg GIT_BRANCH=$GitBranch `
        -t "mlops-train-model:$ImageTag" `
        -f training/Dockerfile `
        training/

    # Tag training images
    Write-Host "üè∑Ô∏è Tagging training images..." -ForegroundColor Blue
    docker tag "mlops-train-model:$ImageTag" "${TrainingRepo}:$ImageTag"
    if ($LatestTag) {
        docker tag "mlops-train-model:$ImageTag" "${TrainingRepo}:$LatestTag"
    }

    # Push training images
    Write-Host "üì§ Pushing training images to ECR..." -ForegroundColor Blue
    docker push "${TrainingRepo}:$ImageTag"
    if ($LatestTag) {
        docker push "${TrainingRepo}:$LatestTag"
    }
} else {
    Write-Host "‚ÑπÔ∏è Training Dockerfile not found, skipping training image build" -ForegroundColor Yellow
}

# Clean up local images
Write-Host "üßπ Cleaning up local images..." -ForegroundColor Blue
docker rmi "mlops-retail-api:$ImageTag" -ErrorAction SilentlyContinue
docker rmi "${ApiRepo}:$ImageTag" -ErrorAction SilentlyContinue
if ($LatestTag) {
    docker rmi "${ApiRepo}:$LatestTag" -ErrorAction SilentlyContinue
}

if (Test-Path "training/Dockerfile") {
    docker rmi "mlops-train-model:$ImageTag" -ErrorAction SilentlyContinue
    docker rmi "${TrainingRepo}:$ImageTag" -ErrorAction SilentlyContinue
    if ($LatestTag) {
        docker rmi "${TrainingRepo}:$LatestTag" -ErrorAction SilentlyContinue
    }
}

Write-Host "‚úÖ Build and push completed successfully!" -ForegroundColor Green
Write-Host "üìç API Image URLs:" -ForegroundColor Yellow
Write-Host "   - ${ApiRepo}:$ImageTag" -ForegroundColor White
if ($LatestTag) {
    Write-Host "   - ${ApiRepo}:$LatestTag" -ForegroundColor White
}

if (Test-Path "training/Dockerfile") {
    Write-Host "üìç Training Image URLs:" -ForegroundColor Yellow
    Write-Host "   - ${TrainingRepo}:$ImageTag" -ForegroundColor White
    if ($LatestTag) {
        Write-Host "   - ${TrainingRepo}:$LatestTag" -ForegroundColor White
    fi
}
```

### 3.3. Test Local Build

**Run build script:**
```bash
# Linux/Mac
./scripts/build-and-push.sh

# Windows PowerShell
.\scripts\build-and-push.ps1
```

**Verify images in ECR:**
```bash
# List API images
aws ecr list-images \
    --repository-name mlops/retail-api \
    --region ap-southeast-1

# List training images  
aws ecr list-images \
    --repository-name mlops/train-model \
    --region ap-southeast-1
```

{{% notice success %}}
**üéØ Local Build & Push Complete!**

**Results:**
- ‚úÖ FastAPI prediction service image built and pushed
- ‚úÖ Training container image built and pushed (if training/Dockerfile exists)
- ‚úÖ Images tagged with Git commit and branch information
- ‚úÖ Latest tags created for main/develop branches
- ‚úÖ Local cleanup completed
{{% /notice %}}

## 4. CI/CD Integration

### 4.1. GitHub Actions Workflow

**Create file `.github/workflows/build-and-deploy.yml`:**

```yaml
name: Build and Deploy to ECR

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'server/**'
      - 'training/**'
      - '.github/workflows/**'
  pull_request:
    branches: [ main ]
    paths:
      - 'server/**'
      - 'training/**'

env:
  AWS_REGION: ap-southeast-1
  ECR_REGISTRY: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.ap-southeast-1.amazonaws.com
  API_REPOSITORY: mlops/retail-api
  TRAINING_REPOSITORY: mlops/train-model

jobs:
  build-and-push:
    name: Build and Push to ECR
    runs-on: ubuntu-latest
    
    permissions:
      id-token: write
      contents: read

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Get Git information
      id: git-info
      run: |
        echo "commit=$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT
        echo "branch=${GITHUB_REF#refs/heads/}" >> $GITHUB_OUTPUT
        echo "build-date=$(date -u +'%Y-%m-%dT%H:%M:%SZ')" >> $GITHUB_OUTPUT
        
        # Determine image tag based on branch
        if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
          echo "image-tag=v$(date +%Y%m%d)-$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT
          echo "latest-tag=latest" >> $GITHUB_OUTPUT
        elif [[ "${{ github.ref }}" == "refs/heads/develop" ]]; then
          echo "image-tag=dev-$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT
          echo "latest-tag=dev-latest" >> $GITHUB_OUTPUT
        else
          echo "image-tag=feature-${{ github.head_ref }}-$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT
          echo "latest-tag=" >> $GITHUB_OUTPUT
        fi

    - name: Build and push API image
      uses: docker/build-push-action@v5
      with:
        context: ./server
        file: ./server/Dockerfile
        push: true
        tags: |
          ${{ env.ECR_REGISTRY }}/${{ env.API_REPOSITORY }}:${{ steps.git-info.outputs.image-tag }}
          ${{ steps.git-info.outputs.latest-tag && format('{0}/{1}:{2}', env.ECR_REGISTRY, env.API_REPOSITORY, steps.git-info.outputs.latest-tag) || '' }}
        build-args: |
          BUILD_DATE=${{ steps.git-info.outputs.build-date }}
          GIT_COMMIT=${{ steps.git-info.outputs.commit }}
          GIT_BRANCH=${{ steps.git-info.outputs.branch }}
          VERSION=${{ steps.git-info.outputs.image-tag }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

    - name: Build and push training image
      if: hashFiles('training/Dockerfile') != ''
      uses: docker/build-push-action@v5
      with:
        context: ./training
        file: ./training/Dockerfile
        push: true
        tags: |
          ${{ env.ECR_REGISTRY }}/${{ env.TRAINING_REPOSITORY }}:${{ steps.git-info.outputs.image-tag }}
          ${{ steps.git-info.outputs.latest-tag && format('{0}/{1}:{2}', env.ECR_REGISTRY, env.TRAINING_REPOSITORY, steps.git-info.outputs.latest-tag) || '' }}
        build-args: |
          BUILD_DATE=${{ steps.git-info.outputs.build-date }}
          GIT_COMMIT=${{ steps.git-info.outputs.commit }}
          GIT_BRANCH=${{ steps.git-info.outputs.branch }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

    - name: Scan images for vulnerabilities
      run: |
        # Trigger ECR scan for API image
        aws ecr start-image-scan \
          --repository-name ${{ env.API_REPOSITORY }} \
          --image-id imageTag=${{ steps.git-info.outputs.image-tag }} \
          --region ${{ env.AWS_REGION }} || true
          
        # Trigger ECR scan for training image (if exists)
        if [[ -f "training/Dockerfile" ]]; then
          aws ecr start-image-scan \
            --repository-name ${{ env.TRAINING_REPOSITORY }} \
            --image-id imageTag=${{ steps.git-info.outputs.image-tag }} \
            --region ${{ env.AWS_REGION }} || true
        fi

    - name: Update EKS deployment (main branch only)
      if: github.ref == 'refs/heads/main'
      run: |
        # Update Kubernetes deployment with new image
        # This would typically involve updating deployment YAML or using tools like ArgoCD
        echo "Would update EKS deployment with image: ${{ env.ECR_REGISTRY }}/${{ env.API_REPOSITORY }}:${{ steps.git-info.outputs.image-tag }}"

    - name: Output image information
      run: |
        echo "### üöÄ Build Complete!" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**API Image:**" >> $GITHUB_STEP_SUMMARY
        echo "- \`${{ env.ECR_REGISTRY }}/${{ env.API_REPOSITORY }}:${{ steps.git-info.outputs.image-tag }}\`" >> $GITHUB_STEP_SUMMARY
        if [[ -n "${{ steps.git-info.outputs.latest-tag }}" ]]; then
          echo "- \`${{ env.ECR_REGISTRY }}/${{ env.API_REPOSITORY }}:${{ steps.git-info.outputs.latest-tag }}\`" >> $GITHUB_STEP_SUMMARY
        fi
        echo "" >> $GITHUB_STEP_SUMMARY
        if [[ -f "training/Dockerfile" ]]; then
          echo "**Training Image:**" >> $GITHUB_STEP_SUMMARY
          echo "- \`${{ env.ECR_REGISTRY }}/${{ env.TRAINING_REPOSITORY }}:${{ steps.git-info.outputs.image-tag }}\`" >> $GITHUB_STEP_SUMMARY
          if [[ -n "${{ steps.git-info.outputs.latest-tag }}" ]]; then
            echo "- \`${{ env.ECR_REGISTRY }}/${{ env.TRAINING_REPOSITORY }}:${{ steps.git-info.outputs.latest-tag }}\`" >> $GITHUB_STEP_SUMMARY
          fi
        fi
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Git Info:**" >> $GITHUB_STEP_SUMMARY
        echo "- Commit: \`${{ steps.git-info.outputs.commit }}\`" >> $GITHUB_STEP_SUMMARY
        echo "- Branch: \`${{ steps.git-info.outputs.branch }}\`" >> $GITHUB_STEP_SUMMARY
        echo "- Build Date: \`${{ steps.git-info.outputs.build-date }}\`" >> $GITHUB_STEP_SUMMARY
```

### 4.2. GitHub Repository Secrets

**Required secrets in GitHub repository:**
```
AWS_ACCOUNT_ID: 123456789012
AWS_ROLE_ARN: arn:aws:iam::123456789012:role/mlops-cicd-role
```

### 4.3. IAM Role for GitHub Actions (OIDC)

**Create OIDC provider and role:**
```bash
# Create OIDC provider
aws iam create-open-id-connect-provider \
  --url https://token.actions.githubusercontent.com \
  --thumbprint-list 6938fd4d98bab03faadb97b34396831e3780aea1 \
  --client-id-list sts.amazonaws.com

# Create trust policy for GitHub Actions
cat > github-actions-trust-policy.json << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Federated": "arn:aws:iam::123456789012:oidc-provider/token.actions.githubusercontent.com"
      },
      "Action": "sts:AssumeRoleWithWebIdentity",
      "Condition": {
        "StringEquals": {
          "token.actions.githubusercontent.com:aud": "sts.amazonaws.com"
        },
        "StringLike": {
          "token.actions.githubusercontent.com:sub": "repo:your-username/retail-forecast:*"
        }
      }
    }
  ]
}
EOF

# Create IAM role
aws iam create-role \
  --role-name mlops-cicd-role \
  --assume-role-policy-document file://github-actions-trust-policy.json

# Attach ECR permissions
aws iam attach-role-policy \
  --role-name mlops-cicd-role \
  --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryPowerUser
```

{{% notice success %}}
**üéØ CI/CD Integration Complete!**

**Automated Workflow:**
- ‚úÖ GitHub Actions triggered on push to main/develop
- ‚úÖ Multi-stage Docker builds with caching
- ‚úÖ Automatic image tagging based on Git branch/commit
- ‚úÖ ECR vulnerability scanning on push
- ‚úÖ OIDC authentication (no AWS keys in repo)
- ‚úÖ Build summaries with image URLs
{{% /notice %}}

## 5. Verification & Testing

### 5.1. Image Verification

**Check repositories in ECR:**
```bash
# List all repositories
aws ecr describe-repositories --region ap-southeast-1

# List images in API repository
aws ecr list-images \
    --repository-name mlops/retail-api \
    --region ap-southeast-1

# List images in training repository
aws ecr list-images \
    --repository-name mlops/train-model \
    --region ap-southeast-1
```

**Get detailed image information:**
```bash
# Describe specific image
aws ecr describe-images \
    --repository-name mlops/retail-api \
    --image-ids imageTag=latest \
    --region ap-southeast-1

# Check image scan results
aws ecr describe-image-scan-findings \
    --repository-name mlops/retail-api \
    --image-id imageTag=latest \
    --region ap-southeast-1
```

### 5.2. Local Container Testing

**Test API container locally:**
```bash
# Run API container locally
docker run -d \
    --name retail-api-test \
    -p 8000:8000 \
    -e AWS_DEFAULT_REGION=ap-southeast-1 \
    -e MODEL_BUCKET=mlops-retail-forecast-models \
    123456789012.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:latest

# Test health endpoint
curl http://localhost:8000/health

# Test prediction endpoint
curl -X POST http://localhost:8000/predict \
    -H "Content-Type: application/json" \
    -d '{
        "features": {
            "SPEND": 100.0,
            "UNITS": 5.0,
            "VISITS": 2.0
        },
        "customer_id": "test-customer-123"
    }'

# Check logs
docker logs retail-api-test

# Clean up
docker stop retail-api-test
docker rm retail-api-test
```

### 5.3. EKS Integration Testing

**Create test deployment:**
```yaml
# test-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: retail-api-test
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: retail-api-test
  template:
    metadata:
      labels:
        app: retail-api-test
    spec:
      serviceAccountName: retail-forecast-sa
      containers:
      - name: api
        image: 123456789012.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8000
        env:
        - name: AWS_DEFAULT_REGION
          value: "ap-southeast-1"
        - name: MODEL_BUCKET
          value: "mlops-retail-forecast-models"
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: retail-api-test-service
spec:
  selector:
    app: retail-api-test
  ports:
  - port: 80
    targetPort: 8000
  type: ClusterIP
```

**Deploy and test:**
```bash
# Deploy test application
kubectl apply -f test-deployment.yaml

# Check pod status
kubectl get pods -l app=retail-api-test

# Check logs
kubectl logs -l app=retail-api-test

# Port forward for testing
kubectl port-forward svc/retail-api-test-service 8080:80

# Test endpoints
curl http://localhost:8080/health
curl http://localhost:8080/model/info

# Clean up test deployment
kubectl delete -f test-deployment.yaml
```

### 5.4. Security Scan Analysis

**Check vulnerability scan results:**
```bash
# Get scan results summary
aws ecr describe-image-scan-findings \
    --repository-name mlops/retail-api \
    --image-id imageTag=latest \
    --region ap-southeast-1 \
    --query 'imageScanFindings.findingCounts'

# Get detailed vulnerability findings
aws ecr describe-image-scan-findings \
    --repository-name mlops/retail-api \
    --image-id imageTag=latest \
    --region ap-southeast-1 \
    --query 'imageScanFindings.findings[?severity==`HIGH` || severity==`CRITICAL`]'
```

**Security best practices verification:**
```bash
# Check if running as non-root user
docker run --rm 123456789012.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:latest whoami

# Check file permissions
docker run --rm 123456789012.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:latest ls -la /app

# Verify health check works
docker run --rm -p 8000:8000 -d --name health-test \
    123456789012.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:latest

sleep 30
docker exec health-test python health_check.py
docker stop health-test
```

## 6. Cost Monitoring & Optimization

### 6.1. ECR Cost Tracking

**Monthly cost breakdown:**
```bash
# Check repository storage usage
aws ecr describe-repositories \
    --region ap-southeast-1 \
    --query 'repositories[*].{Name:repositoryName,Size:repositorySizeInBytes}' \
    --output table

# Estimate monthly storage cost
# ECR Storage: $0.10 per GB-month
# Typical API image: ~500MB
# Training image: ~1GB
# Total: ~1.5GB * $0.10 = $0.15/month
```

### 6.2. Image Optimization

**Multi-stage build analysis:**
```bash
# Compare image sizes
docker images | grep mlops

# Check layer sizes
docker history 123456789012.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:latest
```

**Optimization strategies:**
1. **Use slim base images**: python:3.9-slim vs python:3.9
2. **Multi-stage builds**: Separate build and runtime environments
3. **Layer caching**: Order Dockerfile commands for better caching
4. **Remove unnecessary packages**: Clean up apt cache, temp files
5. **Use .dockerignore**: Exclude unnecessary files from context

## 7. Troubleshooting

### 7.1. Common Issues

**Issue 1: ECR Authentication Failed**
```bash
# Solution: Refresh ECR login
aws ecr get-login-password --region ap-southeast-1 | \
    docker login --username AWS --password-stdin \
    123456789012.dkr.ecr.ap-southeast-1.amazonaws.com

# Check AWS credentials
aws sts get-caller-identity
```

**Issue 2: Image Pull Failed from EKS**
```bash
# Check node group IAM permissions
kubectl describe node

# Verify ECR repository policy
aws ecr get-repository-policy \
    --repository-name mlops/retail-api \
    --region ap-southeast-1

# Check IRSA configuration
kubectl describe serviceaccount retail-forecast-sa
```

**Issue 3: Container Health Check Failing**
```bash
# Check container logs
kubectl logs -l app=retail-api-test

# Test health endpoint manually
kubectl exec -it <pod-name> -- curl http://localhost:8000/health

# Verify model loading
kubectl exec -it <pod-name> -- python -c "
import boto3
s3 = boto3.client('s3')
s3.head_object(Bucket='mlops-retail-forecast-models', Key='models/retail-price-sensitivity/model.pkl')
"
```

**Issue 4: Build Timeout in CI/CD**
```yaml
# Increase timeout in GitHub Actions
jobs:
  build-and-push:
    timeout-minutes: 30  # Increase from default 360
```

### 7.2. Performance Optimization

**Build performance:**
```bash
# Use BuildKit for faster builds
export DOCKER_BUILDKIT=1
docker build ...

# Enable build cache
docker build --cache-from mlops-retail-api:latest ...
```

**Runtime performance:**
```bash
# Monitor container resource usage
kubectl top pods -l app=retail-api

# Check API response times
kubectl exec -it <pod-name> -- curl -w "@curl-format.txt" http://localhost:8000/predict
```

## üëâ K·∫øt qu·∫£ Task 6

‚úÖ **ECR Repositories** - mlops/retail-api v√† mlops/train-model repositories  
‚úÖ **Container Images** - FastAPI prediction service v√† training containers  
‚úÖ **CI/CD Integration** - GitHub Actions automated build-push-deploy  
‚úÖ **Security** - Image scanning, non-root users, OIDC authentication  
‚úÖ **Cost Optimization** - Lifecycle policies, multi-stage builds, ~$0.15/month  

### Architecture Delivered

```
‚úÖ ECR Container Registry:
   - mlops/retail-api: FastAPI prediction service
   - mlops/train-model: Custom training environment
   - Tag immutability enabled
   - Vulnerability scanning on push

‚úÖ CI/CD Automation:
   - GitHub Actions workflow
   - Automatic builds on code changes  
   - Git-based image tagging (v20241009-abc123)
   - OIDC authentication (no AWS keys)

‚úÖ Security & Compliance:
   - Non-root containers
   - Multi-stage builds
   - Vulnerability scanning
   - IAM-based access control
```

{{% notice success %}}
**üéØ Task 6 Complete - Production-Ready Container Registry!**

**Container Strategy**: FastAPI API + optional training containers  
**CI/CD Automation**: Build ‚Üí Push ‚Üí Deploy on Git changes  
**Security**: Image scanning, non-root users, access control  
**Cost Efficient**: ~$0.15/month storage + lifecycle cleanup  
**Demo Ready**: API container ready for ALB + EKS deployment  
{{% /notice %}}

{{% notice tip %}}
**üöÄ Next Steps:**
- **Task 7**: EKS cluster deployment trong hybrid VPC
- **Task 8**: EKS node groups v·ªõi ECR image pull permissions
- **Task 10**: Deploy API container v·ªõi ALB integration
- **Task 11**: ALB ingress controller cho public API access

**Build & Deploy Commands:**
```bash
# Local build and push
./scripts/build-and-push.sh

# CI/CD triggers automatically on:
git push origin main        # ‚Üí v20241009-abc123 + latest
git push origin develop     # ‚Üí dev-abc123 + dev-latest  
git push origin feature/x   # ‚Üí feature-x-abc123
```
{{% /notice %}}

{{% notice info %}}
**ÔøΩ Production Benchmarks Achieved:**
- **Image Size**: FastAPI ~500MB (optimized multi-stage)
- **Build Time**: ~3-5 minutes (with caching)
- **Storage Cost**: ~$0.15/month (1.5GB total)
- **Security**: Non-root, vulnerability scanned
- **Availability**: Multi-tag strategy (latest, commit, branch)
- **CI/CD**: Automated on every commit
{{% /notice %}}

---

**Next Step**: [Task 7: EKS Cluster Setup](../7-eks-cluster/)