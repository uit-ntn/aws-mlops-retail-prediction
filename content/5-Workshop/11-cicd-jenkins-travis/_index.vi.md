---
title: "CI/CD Pipeline"
date: 2024-01-01T00:00:00Z
weight: 12
chapter: false
pre: "<b>11. </b>"
---

{{% notice info %}}
**üéØ M·ª•c ti√™u Task 13:**
{{% /notice %}}

Thi·∫øt l·∫≠p pipeline CI/CD t·ª± ƒë·ªông cho to√†n b·ªô v√≤ng ƒë·ªùi d·ª± √°n MLOps Retail Prediction:

- CI (Continuous Integration): build & test code, build Docker image, push ECR
- CD (Continuous Delivery): hu·∫•n luy·ªán l·∫°i model, c·∫≠p nh·∫≠t Model Registry, deploy phi√™n b·∫£n m·ªõi l√™n EKS
- Monitoring hook: rollback khi API ho·∫∑c model c√≥ l·ªói (CloudWatch trigger)

‚Üí ƒê·∫£m b·∫£o tri·ªÉn khai li√™n t·ª•c, gi·∫£m l·ªói th·ªß c√¥ng, ti·∫øt ki·ªám th·ªùi gian v√† chi ph√≠.

üì• **Input t·ª´ c√°c Task tr∆∞·ªõc:**
- **Task 6 (ECR Container Registry):** Repository URIs, lifecycle policies, image scanning and push commands; credentials and ECR access for CI runners
- **Task 8 (API Deployment on EKS):** Kubernetes manifests, service & deployment names, healthcheck endpoints, HPA and ServiceAccount/IRSA details used by CD stage
- **Task 10 (CloudWatch Monitoring):** Log groups, alarms, dashboards and Container Insights configuration used for deployment verification and automated rollback triggers
- **Task 2 (IAM Roles & Audit):** IAM roles, OIDC provider configuration and least-privilege policies for CI/CD runners (GitHub Actions / Jenkins) and SageMaker execution

## 1. C·∫•u tr√∫c pipeline t·ªïng qu√°t

Pipeline CI/CD c·ªßa d·ª± √°n Retail Prediction s·∫Ω t·ª± ƒë·ªông h√≥a to√†n b·ªô quy tr√¨nh t·ª´ commit code ƒë·∫øn deploy l√™n production, bao g·ªìm c·∫£ vi·ªác hu·∫•n luy·ªán l·∫°i model khi c·∫ßn thi·∫øt.

Pipeline chia th√†nh 3 m√¥i tr∆∞·ªùng:
- **DEV**: Build, test v√† validate code
- **STAGING**: Hu·∫•n luy·ªán v√† ƒë√°nh gi√° model
- **PROD**: Deploy v√† monitor tr√™n production

### 2. IAM Roles v√† Permissions

#### 2.1 Create CI/CD Service Role

```bash
# Create trust policy for CI/CD role
cat > cicd-trust-policy.json << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": [
          "ec2.amazonaws.com",
          "codebuild.amazonaws.com",
          "codepipeline.amazonaws.com"
        ]
      },
      "Action": "sts:AssumeRole"
    },
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::YOUR_ACCOUNT_ID:user/jenkins-user"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF

# Create the role
aws iam create-role \
  --role-name RetailForecastCICDRole \
  --assume-role-policy-document file://cicd-trust-policy.json \
  --description "Role for Retail Forecast CI/CD Pipeline"
```

#### 2.2 CI/CD IAM Policy

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject",
        "s3:DeleteObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::retail-forecast-data-bucket",
        "arn:aws:s3:::retail-forecast-data-bucket/*",
        "arn:aws:s3:::retail-forecast-artifacts-bucket",
        "arn:aws:s3:::retail-forecast-artifacts-bucket/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "ecr:GetAuthorizationToken",
        "ecr:BatchCheckLayerAvailability",
        "ecr:GetDownloadUrlForLayer",
        "ecr:BatchGetImage",
        "ecr:PutImage",
        "ecr:InitiateLayerUpload",
        "ecr:UploadLayerPart",
        "ecr:CompleteLayerUpload"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "sagemaker:CreateTrainingJob",
        "sagemaker:DescribeTrainingJob",
        "sagemaker:StopTrainingJob",
        "sagemaker:CreateModel",
        "sagemaker:CreateModelPackage",
        "sagemaker:CreateModelPackageGroup",
        "sagemaker:DescribeModelPackage",
        "sagemaker:UpdateModelPackage",
        "sagemaker:ListModelPackages"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "eks:DescribeCluster",
        "eks:ListClusters",
        "eks:DescribeNodegroup"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "sts:GetCallerIdentity",
        "sts:AssumeRole"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "kms:Decrypt",
        "kms:GenerateDataKey",
        "kms:CreateGrant"
      ],
      "Resource": [
        "arn:aws:kms:us-east-1:YOUR_ACCOUNT_ID:key/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents"
      ],
      "Resource": "*"
    }
  ]
}
```

#### 2.3 Attach Policies

```bash
# Create and attach the policy
aws iam create-policy \
  --policy-name RetailForecastCICDPolicy \
  --policy-document file://cicd-policy.json

aws iam attach-role-policy \
  --role-name RetailForecastCICDRole \
  --policy-arn arn:aws:iam::YOUR_ACCOUNT_ID:policy/RetailForecastCICDPolicy

# Attach additional managed policies
aws iam attach-role-policy \
  --role-name RetailForecastCICDRole \
  --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy

aws iam attach-role-policy \
  --role-name RetailForecastCICDRole \
  --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
```

### 3. Jenkins Pipeline Setup

#### 3.1 Jenkins Installation on EC2

```bash
# Launch EC2 instance for Jenkins
aws ec2 run-instances \
  --image-id ami-0c55b159cbfafe1d0 \
  --instance-type t3.medium \
  --key-name your-key-pair \
  --security-group-ids sg-jenkins \
  --subnet-id subnet-12345678 \
  --iam-instance-profile Name=JenkinsInstanceProfile \
  --user-data file://jenkins-install.sh \
  --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=Jenkins-Server},{Key=Project,Value=RetailForecast}]'
```

#### 3.2 Jenkins Installation Script

```bash
#!/bin/bash
# jenkins-install.sh

# Update system
yum update -y

# Install Docker
yum install -y docker
systemctl start docker
systemctl enable docker
usermod -a -G docker ec2-user

# Install Docker Compose
curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
chmod +x /usr/local/bin/docker-compose

# Install Jenkins
wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo
rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key
yum upgrade -y
yum install -y java-11-openjdk jenkins

# Start Jenkins
systemctl start jenkins
systemctl enable jenkins

# Install kubectl
curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.21.2/2021-07-05/bin/linux/amd64/kubectl
chmod +x ./kubectl
mv ./kubectl /usr/local/bin

# Install AWS CLI v2
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
./aws/install

# Install Python and dependencies
yum install -y python3 python3-pip
pip3 install boto3 sagemaker pandas scikit-learn

# Configure Jenkins user for Docker
usermod -a -G docker jenkins
systemctl restart jenkins

echo "Jenkins installation completed!"
echo "Access Jenkins at: http://$(curl -s http://169.254.169.254/latest/meta-data/public-ip):8080"
echo "Initial admin password: $(cat /var/lib/jenkins/secrets/initialAdminPassword)"
```

#### 3.3 Jenkinsfile for ML Pipeline

```groovy
// Jenkinsfile
pipeline {
    agent any
    
    environment {
        AWS_DEFAULT_REGION = 'us-east-1'
        AWS_ACCOUNT_ID = '123456789012'
        ECR_REPOSITORY = 'retail-forecast'
        EKS_CLUSTER_NAME = 'retail-forecast-cluster'
        NAMESPACE = 'mlops'
        S3_DATA_BUCKET = 'retail-forecast-data-bucket'
        S3_ARTIFACTS_BUCKET = 'retail-forecast-artifacts-bucket'
        MODEL_PACKAGE_GROUP = 'retail-forecast-model-group'
    }
    
    parameters {
        choice(
            name: 'DEPLOY_ENVIRONMENT',
            choices: ['dev', 'staging', 'prod'],
            description: 'Target deployment environment'
        )
        booleanParam(
            name: 'RETRAIN_MODEL',
            defaultValue: false,
            description: 'Force model retraining'
        )
        booleanParam(
            name: 'SKIP_TESTS',
            defaultValue: false,
            description: 'Skip test execution'
        )
    }
    
    stages {
        stage('Setup') {
            steps {
                script {
                    // Clean workspace
                    cleanWs()
                    
                    // Checkout code
                    checkout scm
                    
                    // Set build info
                    env.BUILD_VERSION = "${env.BUILD_NUMBER}-${env.GIT_COMMIT.take(7)}"
                    env.IMAGE_TAG = "v${env.BUILD_VERSION}"
                    
                    echo "Build Version: ${env.BUILD_VERSION}"
                    echo "Image Tag: ${env.IMAGE_TAG}"
                }
            }
        }
        
        stage('Environment Setup') {
            steps {
                script {
                    // Install Python dependencies
                    sh '''
                        python3 -m venv venv
                        source venv/bin/activate
                        pip install --upgrade pip
                        pip install -r requirements.txt
                        pip install pytest pytest-cov flake8
                    '''
                    
                    // Configure AWS credentials
                    sh '''
                        aws sts get-caller-identity
                        aws configure set region $AWS_DEFAULT_REGION
                    '''
                    
                    // Configure kubectl
                    sh '''
                        aws eks update-kubeconfig --name $EKS_CLUSTER_NAME --region $AWS_DEFAULT_REGION
                        kubectl cluster-info
                    '''
                }
            }
        }
        
        stage('Code Quality & Testing') {
            when {
                not { params.SKIP_TESTS }
            }
            parallel {
                stage('Linting') {
                    steps {
                        sh '''
                            source venv/bin/activate
                            flake8 src/ --max-line-length=88 --exclude=venv
                        '''
                    }
                }
                
                stage('Unit Tests') {
                    steps {
                        sh '''
                            source venv/bin/activate
                            pytest tests/unit/ -v --cov=src --cov-report=xml --cov-report=html
                        '''
                        
                        // Publish test results
                        publishTestResults testResultsPattern: 'test-results.xml'
                        publishCoverage adapters: [coberturaAdapter('coverage.xml')], sourceFileResolver: sourceFiles('STORE_LAST_BUILD')
                    }
                }
                
                stage('Integration Tests') {
                    steps {
                        sh '''
                            source venv/bin/activate
                            pytest tests/integration/ -v
                        '''
                    }
                }
            }
        }
        
        stage('Data Validation') {
            steps {
                script {
                    sh '''
                        source venv/bin/activate
                        python scripts/validate_data.py \
                            --bucket $S3_DATA_BUCKET \
                            --key training-data/train.csv \
                            --output data-validation-report.json
                    '''
                    
                    // Archive validation report
                    archiveArtifacts artifacts: 'data-validation-report.json', fingerprint: true
                }
            }
        }
        
        stage('Model Training') {
            when {
                anyOf {
                    params.RETRAIN_MODEL
                    changeset "src/training/**"
                    changeset "data/**"
                }
            }
            steps {
                script {
                    sh '''
                        source venv/bin/activate
                        python scripts/trigger_training.py \
                            --job-name "retail-forecast-training-${BUILD_VERSION}" \
                            --data-bucket $S3_DATA_BUCKET \
                            --output-bucket $S3_ARTIFACTS_BUCKET \
                            --role-arn arn:aws:iam::$AWS_ACCOUNT_ID:role/SageMakerExecutionRole
                    '''
                    
                    // Wait for training completion
                    sh '''
                        source venv/bin/activate
                        python scripts/wait_for_training.py \
                            --job-name "retail-forecast-training-${BUILD_VERSION}" \
                            --timeout 3600
                    '''
                }
            }
        }
        
        stage('Model Validation & Registration') {
            when {
                anyOf {
                    params.RETRAIN_MODEL
                    changeset "src/training/**"
                }
            }
            steps {
                script {
                    // Validate model performance
                    sh '''
                        source venv/bin/activate
                        python scripts/validate_model.py \
                            --job-name "retail-forecast-training-${BUILD_VERSION}" \
                            --baseline-accuracy 0.85 \
                            --output model-validation-report.json
                    '''
                    
                    // Register model if validation passes
                    sh '''
                        source venv/bin/activate
                        python scripts/register_model.py \
                            --job-name "retail-forecast-training-${BUILD_VERSION}" \
                            --model-package-group $MODEL_PACKAGE_GROUP \
                            --approval-status "PendingManualApproval" \
                            --model-version $BUILD_VERSION
                    '''
                    
                    archiveArtifacts artifacts: 'model-validation-report.json', fingerprint: true
                }
            }
        }
        
        stage('Docker Build') {
            steps {
                script {
                    // Build Docker image
                    sh '''
                        # Get latest approved model
                        MODEL_URI=$(python scripts/get_latest_model.py --model-package-group $MODEL_PACKAGE_GROUP)
                        echo "Using model: $MODEL_URI"
                        
                        # Build image with model URI
                        docker build \
                            --build-arg MODEL_URI=$MODEL_URI \
                            --build-arg BUILD_VERSION=$BUILD_VERSION \
                            -t $ECR_REPOSITORY:$IMAGE_TAG \
                            -t $ECR_REPOSITORY:latest \
                            .
                    '''
                    
                    // Security scan
                    sh '''
                        docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
                            -v $(pwd):/root/.cache/ \
                            aquasec/trivy:latest image \
                            --exit-code 1 \
                            --severity HIGH,CRITICAL \
                            $ECR_REPOSITORY:$IMAGE_TAG
                    '''
                }
            }
        }
        
        stage('Push to ECR') {
            steps {
                script {
                    sh '''
                        # Login to ECR
                        aws ecr get-login-password --region $AWS_DEFAULT_REGION | \
                            docker login --username AWS --password-stdin \
                            $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com
                        
                        # Tag images
                        docker tag $ECR_REPOSITORY:$IMAGE_TAG \
                            $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$ECR_REPOSITORY:$IMAGE_TAG
                        
                        docker tag $ECR_REPOSITORY:latest \
                            $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$ECR_REPOSITORY:latest
                        
                        # Push images
                        docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$ECR_REPOSITORY:$IMAGE_TAG
                        docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$ECR_REPOSITORY:latest
                    '''
                }
            }
        }
        
        stage('Deploy to EKS') {
            steps {
                script {
                    // Update Kubernetes deployment
                    sh '''
                        # Update deployment image
                        kubectl set image deployment/retail-forecast-api \
                            retail-forecast-api=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$ECR_REPOSITORY:$IMAGE_TAG \
                            -n $NAMESPACE
                        
                        # Wait for rollout
                        kubectl rollout status deployment/retail-forecast-api -n $NAMESPACE --timeout=600s
                        
                        # Verify deployment
                        kubectl get pods -n $NAMESPACE -l app=retail-forecast-api
                    '''
                }
            }
        }
        
        stage('Health Check') {
            steps {
                script {
                    sh '''
                        # Get service endpoint
                        ENDPOINT=$(kubectl get service retail-forecast-service -n $NAMESPACE -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
                        
                        if [ -z "$ENDPOINT" ]; then
                            ENDPOINT=$(kubectl get service retail-forecast-service -n $NAMESPACE -o jsonpath='{.spec.clusterIP}')
                            kubectl port-forward service/retail-forecast-service 8080:80 -n $NAMESPACE &
                            ENDPOINT="localhost:8080"
                            PORT_FORWARD_PID=$!
                        fi
                        
                        # Wait for service to be ready
                        echo "Waiting for service to be ready..."
                        for i in {1..30}; do
                            if curl -f http://$ENDPOINT/healthz; then
                                echo "Service is healthy!"
                                break
                            fi
                            echo "Attempt $i/30 failed, retrying in 10 seconds..."
                            sleep 10
                        done
                        
                        # Test prediction endpoint
                        curl -X POST http://$ENDPOINT/predict \
                            -H "Content-Type: application/json" \
                            -d '{"features": {"store_id": 1, "product_id": 123, "price": 29.99}}'
                        
                        # Kill port-forward if used
                        if [ ! -z "$PORT_FORWARD_PID" ]; then
                            kill $PORT_FORWARD_PID
                        fi
                    '''
                }
            }
        }
        
        stage('Performance Testing') {
            when {
                environment name: 'DEPLOY_ENVIRONMENT', value: 'prod'
            }
            steps {
                script {
                    sh '''
                        # Run load test
                        python scripts/load_test.py \
                            --endpoint http://$ENDPOINT \
                            --duration 300 \
                            --concurrent-users 10 \
                            --output load-test-report.json
                    '''
                    
                    archiveArtifacts artifacts: 'load-test-report.json', fingerprint: true
                }
            }
        }
    }
    
    post {
        always {
            // Clean up
            sh '''
                docker system prune -f
                rm -rf venv
            '''
            
            // Archive logs
            archiveArtifacts artifacts: 'logs/**', allowEmptyArchive: true
        }
        
        success {
            script {
                // Send success notification
                sh '''
                    aws sns publish \
                        --topic-arn arn:aws:sns:$AWS_DEFAULT_REGION:$AWS_ACCOUNT_ID:deployment-notifications \
                        --message "‚úÖ Deployment successful for build $BUILD_VERSION" \
                        --subject "Retail Forecast Deployment Success"
                '''
            }
        }
        
        failure {
            script {
                // Send failure notification
                sh '''
                    aws sns publish \
                        --topic-arn arn:aws:sns:$AWS_DEFAULT_REGION:$AWS_ACCOUNT_ID:deployment-notifications \
                        --message "‚ùå Deployment failed for build $BUILD_VERSION. Check Jenkins logs." \
                        --subject "Retail Forecast Deployment Failed"
                '''
                
                // Rollback on production failure
                if (params.DEPLOY_ENVIRONMENT == 'prod') {
                    sh '''
                        echo "Rolling back production deployment..."
                        kubectl rollout undo deployment/retail-forecast-api -n $NAMESPACE
                        kubectl rollout status deployment/retail-forecast-api -n $NAMESPACE
                    '''
                }
            }
        }
    }
}
```

## 2. GitHub Actions CI/CD Pipeline

Ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng GitHub Actions ƒë·ªÉ x√¢y d·ª±ng pipeline CI/CD cho d·ª± √°n MLOps Retail Prediction v√¨ kh·∫£ nƒÉng t√≠ch h·ª£p s·∫µn v·ªõi GitHub repository v√† t√≠nh linh ho·∫°t cao.

### 2.1 C·∫•u h√¨nh workflow file

T·∫°o file `.github/workflows/mlops-pipeline.yml`:

```yaml
# .github/workflows/mlops-pipeline.yml
name: MLOps Retail Prediction Pipeline

on:
  push:
    branches: [ main, develop ]
    paths-ignore:
      - '**.md'
      - 'docs/**'
  pull_request:
    branches: [ main ]
  schedule:
    # Ch·∫°y m·ªói tu·∫ßn v√†o th·ª© 2 ƒë·ªÉ ki·ªÉm tra ƒë·ªô ch√≠nh x√°c c·ªßa model
    - cron: '0 2 * * 1'
  workflow_dispatch:
    inputs:
      environment:
        description: 'M√¥i tr∆∞·ªùng tri·ªÉn khai'
        required: true
        default: 'dev'
        type: choice
        options:
        - dev
        - staging
        - prod
      retrain_model:
        description: 'Hu·∫•n luy·ªán l·∫°i model'
        required: false
        default: false
        type: boolean
      deploy_only:
        description: 'Ch·ªâ tri·ªÉn khai, kh√¥ng build/hu·∫•n luy·ªán m·ªõi'
        required: false
        default: false
        type: boolean

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY: retail-forecast
  EKS_CLUSTER_NAME: retail-forecast-cluster
  S3_DATA_BUCKET: retail-forecast-data
  S3_MODEL_BUCKET: retail-forecast-models
  MODEL_PACKAGE_GROUP: retail-forecast-models

permissions:
  id-token: write   # C·∫ßn thi·∫øt cho OIDC v·ªõi AWS
  contents: read    # C·∫ßn thi·∫øt ƒë·ªÉ checkout code

jobs:
  setup:
    name: Setup Pipeline
    runs-on: ubuntu-latest
    outputs:
      build-id: ${{ steps.generate-id.outputs.build_id }}
      environment: ${{ steps.set-env.outputs.environment }}
      should-train: ${{ steps.set-env.outputs.should_train }}
      should-deploy: ${{ steps.set-env.outputs.should_deploy }}
    
    steps:
    - name: Generate build ID
      id: generate-id
      run: |
        BUILD_ID="build-${GITHUB_RUN_NUMBER}-${GITHUB_SHA::7}"
        echo "build_id=$BUILD_ID" >> $GITHUB_OUTPUT
        echo "Build ID: $BUILD_ID"
    
    - name: Set environment variables
      id: set-env
      run: |
        # X√°c ƒë·ªãnh m√¥i tr∆∞·ªùng
        if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
          ENV="${{ github.event.inputs.environment }}"
        elif [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
          ENV="prod"
        elif [[ "${{ github.ref }}" == "refs/heads/develop" ]]; then
          ENV="staging"
        else
          ENV="dev"
        fi
        echo "environment=$ENV" >> $GITHUB_OUTPUT
        
        # X√°c ƒë·ªãnh c√≥ n√™n hu·∫•n luy·ªán model hay kh√¥ng
        if [[ "${{ github.event.inputs.retrain_model }}" == "true" ]] || \
           [[ "${{ github.event_name }}" == "schedule" ]]; then
          SHOULD_TRAIN="true"
        else
          SHOULD_TRAIN="false"
        fi
        echo "should_train=$SHOULD_TRAIN" >> $GITHUB_OUTPUT
        
        # X√°c ƒë·ªãnh c√≥ n√™n deploy hay kh√¥ng
        if [[ "${{ github.event.inputs.deploy_only }}" == "true" ]] || \
           [[ "$ENV" == "prod" && "${{ github.ref }}" == "refs/heads/main" ]] || \
           [[ "$ENV" == "staging" && "${{ github.ref }}" == "refs/heads/develop" ]]; then
          SHOULD_DEPLOY="true"
        else
          SHOULD_DEPLOY="false"
        fi
        echo "should_deploy=$SHOULD_DEPLOY" >> $GITHUB_OUTPUT
        
        # In th√¥ng tin
        echo "Environment: $ENV"
        echo "Should train model: $SHOULD_TRAIN"
        echo "Should deploy: $SHOULD_DEPLOY"

  test:
    name: Code Quality & Testing
    needs: setup
    runs-on: ubuntu-latest
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r core/requirements.txt
        pip install -r server/requirements.txt
        pip install pytest pytest-cov pylint black
    
    - name: Code formatting check
      run: |
        black --check core/ server/
    
    - name: Lint code
      run: |
        pylint --disable=C0111,C0103 core/ server/
    
    - name: Run tests
      run: |
        pytest tests/ -v --cov=core --cov=server --cov-report=xml
    
    - name: Upload coverage report
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: false

  data_validation:
    name: Data Validation
    needs: [setup, test]
    runs-on: ubuntu-latest
    if: needs.setup.outputs.should_train == 'true'
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole
        aws-region: ${{ env.AWS_REGION }}
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        pip install pandas boto3 great_expectations
    
    - name: Validate training data
      run: |
        python aws/script/validate_data.py \
          --bucket ${{ env.S3_DATA_BUCKET }} \
          --key training/sales_data.csv \
          --output validation_report.json
    
    - name: Upload validation report
      uses: actions/upload-artifact@v3
      with:
        name: data-validation-report
        path: validation_report.json

  model_training:
    name: Model Training
    needs: [setup, test, data_validation]
    runs-on: ubuntu-latest
    if: needs.setup.outputs.should_train == 'true' && success()
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole
        aws-region: ${{ env.AWS_REGION }}
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        pip install boto3 sagemaker pandas scikit-learn
    
    - name: Create training job
      id: training
      run: |
        python aws/script/create_training_job.py \
          --job-name "retail-forecast-${{ needs.setup.outputs.build-id }}" \
          --data-bucket ${{ env.S3_DATA_BUCKET }} \
          --output-bucket ${{ env.S3_MODEL_BUCKET }} \
          --instance-type ml.m5.large \
          --hyperparameters "{\"n_estimators\":\"200\",\"max_depth\":\"10\"}"
      
    - name: Wait for training completion
      run: |
        aws sagemaker wait training-job-completed-or-stopped \
          --training-job-name "retail-forecast-${{ needs.setup.outputs.build-id }}"
        
        STATUS=$(aws sagemaker describe-training-job \
          --training-job-name "retail-forecast-${{ needs.setup.outputs.build-id }}" \
          --query 'TrainingJobStatus' --output text)
          
        if [ "$STATUS" != "Completed" ]; then
          echo "Training failed with status: $STATUS"
          exit 1
        fi

  model_evaluation:
    name: Model Evaluation & Registration
    needs: [setup, model_training]
    runs-on: ubuntu-latest
    outputs:
      model_approved: ${{ steps.evaluate.outputs.model_approved }}
      model_version: ${{ steps.register.outputs.model_version }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole
        aws-region: ${{ env.AWS_REGION }}
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        pip install boto3 sagemaker pandas scikit-learn matplotlib seaborn
    
    - name: Evaluate model
      id: evaluate
      run: |
        python aws/script/processing_evaluate.py \
          --job-name "retail-forecast-${{ needs.setup.outputs.build-id }}" \
          --evaluation-data s3://${{ env.S3_DATA_BUCKET }}/validation/sales_data.csv \
          --baseline-metrics s3://${{ env.S3_MODEL_BUCKET }}/baselines/metrics.json \
          --threshold 0.85
          
        # Check result
        if [ -f "model_approved.txt" ]; then
          MODEL_APPROVED=$(cat model_approved.txt)
          echo "model_approved=$MODEL_APPROVED" >> $GITHUB_OUTPUT
          echo "Model evaluation result: $MODEL_APPROVED"
        else
          echo "model_approved=false" >> $GITHUB_OUTPUT
          echo "Model evaluation failed"
          exit 1
        fi
    
    - name: Register model
      id: register
      if: steps.evaluate.outputs.model_approved == 'true'
      run: |
        MODEL_VERSION=$(python aws/script/register_model.py \
          --job-name "retail-forecast-${{ needs.setup.outputs.build-id }}" \
          --model-package-group ${{ env.MODEL_PACKAGE_GROUP }} \
          --approval-status "Approved")
          
        echo "model_version=$MODEL_VERSION" >> $GITHUB_OUTPUT
        echo "Model registered with version: $MODEL_VERSION"
    
    - name: Upload evaluation results
      uses: actions/upload-artifact@v3
      with:
        name: model-evaluation-report
        path: |
          evaluation_results.json
          plots/*.png

  build_docker:
    name: Build & Push Docker Image
    needs: [setup, test]
    runs-on: ubuntu-latest
    outputs:
      image_tag: ${{ steps.build.outputs.image_tag }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole
        aws-region: ${{ env.AWS_REGION }}
    
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v1
    
    - name: Build and push Docker image
      id: build
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
      run: |
        # Set tag
        IMAGE_TAG="${{ needs.setup.outputs.build-id }}"
        echo "image_tag=$IMAGE_TAG" >> $GITHUB_OUTPUT
        
        # Get latest model URI (or use placeholder for dev environment)
        if [[ "${{ needs.setup.outputs.environment }}" == "dev" ]]; then
          MODEL_URI="placeholder"
        else
          MODEL_URI=$(aws sagemaker list-model-packages \
            --model-package-group-name ${{ env.MODEL_PACKAGE_GROUP }} \
            --sort-by CreationTime --sort-order Descending --max-items 1 \
            --query 'ModelPackageSummaries[0].ModelPackageArn' --output text)
        fi
        
        echo "Using model URI: $MODEL_URI"
        
        # Build image
        docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG \
          --build-arg MODEL_URI=$MODEL_URI \
          --build-arg BUILD_ID=$IMAGE_TAG \
          --build-arg ENV=${{ needs.setup.outputs.environment }} \
          server/
        
        # Also tag as latest-{environment}
        docker tag $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG \
          $ECR_REGISTRY/$ECR_REPOSITORY:latest-${{ needs.setup.outputs.environment }}
        
        # Security scan
        docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
          aquasec/trivy:latest image --severity HIGH,CRITICAL \
          $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
        
        # Push images
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest-${{ needs.setup.outputs.environment }}
        
        echo "Image built and pushed: $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG"

  deploy_eks:
    name: Deploy to EKS
    needs: [setup, test, build_docker, model_evaluation]
    runs-on: ubuntu-latest
    if: needs.setup.outputs.should_deploy == 'true'
    environment:
      name: ${{ needs.setup.outputs.environment }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole
        aws-region: ${{ env.AWS_REGION }}
    
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v1
    
    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}
    
    - name: Check if need to deploy new model
      id: check-model
      run: |
        if [[ "${{ needs.model_evaluation.outputs.model_approved }}" == "true" ]]; then
          echo "deploy_new_model=true" >> $GITHUB_OUTPUT
        else
          echo "deploy_new_model=false" >> $GITHUB_OUTPUT
        fi
    
    - name: Deploy to EKS
      run: |
        # Set environment variables
        NAMESPACE="retail-forecast-${{ needs.setup.outputs.environment }}"
        IMAGE_TAG="${{ needs.build_docker.outputs.image_tag }}"
        ECR_REGISTRY="${{ steps.login-ecr.outputs.registry }}"
        
        # Update kustomization file with new image
        cd aws/k8s
        kustomize edit set image retail-forecast-api=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
        
        # Apply changes
        kubectl apply -k overlays/${{ needs.setup.outputs.environment }}/ --namespace $NAMESPACE
        
        # Wait for deployment to complete
        kubectl rollout status deployment/retail-forecast-api -n $NAMESPACE --timeout=300s
    
    - name: Create SageMaker endpoint (if new model approved)
      if: steps.check-model.outputs.deploy_new_model == 'true'
      run: |
        python aws/script/deploy_endpoint.py \
          --model-package-arn "arn:aws:sagemaker:${{ env.AWS_REGION }}:${{ secrets.AWS_ACCOUNT_ID }}:model-package/${{ env.MODEL_PACKAGE_GROUP }}/${{ needs.model_evaluation.outputs.model_version }}" \
          --endpoint-name "retail-forecast-${{ needs.setup.outputs.environment }}" \
          --instance-type ml.t2.medium \
          --instance-count 1
    
    - name: Health check
      run: |
        # Get service endpoint
        NAMESPACE="retail-forecast-${{ needs.setup.outputs.environment }}"
        SERVICE_HOST=$(kubectl get ingress -n $NAMESPACE retail-forecast-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
        
        # If running locally, use port-forwarding
        if [ -z "$SERVICE_HOST" ]; then
          echo "No external hostname found, using port-forwarding"
          kubectl port-forward svc/retail-forecast-service 8080:80 -n $NAMESPACE &
          sleep 5
          SERVICE_HOST="localhost:8080"
        fi
        
        # Check health endpoint
        echo "Testing health endpoint: http://$SERVICE_HOST/health"
        HEALTH_STATUS=$(curl -s -o /dev/null -w "%{http_code}" http://$SERVICE_HOST/health)
        
        if [ "$HEALTH_STATUS" -eq 200 ]; then
          echo "Health check passed: $HEALTH_STATUS"
        else
          echo "Health check failed: $HEALTH_STATUS"
          exit 1
        fi
        
        # Test prediction endpoint
        echo "Testing prediction endpoint"
        PREDICTION_RESULT=$(curl -s -X POST \
          -H "Content-Type: application/json" \
          -d '{"store_id": 1, "item_id": 123, "date": "2023-06-15"}' \
          http://$SERVICE_HOST/predict)
        
        echo "Prediction result: $PREDICTION_RESULT"
        
        # Check if response contains prediction
        if [[ $PREDICTION_RESULT == *"prediction"* ]]; then
          echo "Prediction endpoint working correctly"
        else
          echo "Prediction endpoint not returning expected response"
          exit 1
        fi

  monitoring:
    name: Setup Monitoring
    needs: [setup, deploy_eks]
    runs-on: ubuntu-latest
    if: always() && needs.deploy_eks.result == 'success'
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole
        aws-region: ${{ env.AWS_REGION }}
    
    - name: Setup CloudWatch alarms for new deployment
      run: |
        # Create/update CloudWatch alarms for API and model performance
        NAMESPACE="retail-forecast-${{ needs.setup.outputs.environment }}"
        DEPLOYMENT_ID="${{ needs.setup.outputs.build-id }}"
        
        aws cloudwatch put-metric-alarm \
          --alarm-name "RetailForecast-API-Error-Rate-$NAMESPACE" \
          --alarm-description "Monitor error rate for Retail Forecast API" \
          --metric-name "ErrorRate" \
          --namespace "RetailForecast/$NAMESPACE" \
          --statistic "Average" \
          --period 60 \
          --threshold 5 \
          --comparison-operator "GreaterThanThreshold" \
          --evaluation-periods 3 \
          --alarm-actions "arn:aws:sns:${{ env.AWS_REGION }}:${{ secrets.AWS_ACCOUNT_ID }}:retail-forecast-alerts" \
          --dimensions "Name=DeploymentId,Value=$DEPLOYMENT_ID"
        
        aws cloudwatch put-metric-alarm \
          --alarm-name "RetailForecast-API-Latency-$NAMESPACE" \
          --alarm-description "Monitor latency for Retail Forecast API" \
          --metric-name "Latency" \
          --namespace "RetailForecast/$NAMESPACE" \
          --statistic "Average" \
          --period 60 \
          --threshold 1000 \
          --comparison-operator "GreaterThanThreshold" \
          --evaluation-periods 3 \
          --alarm-actions "arn:aws:sns:${{ env.AWS_REGION }}:${{ secrets.AWS_ACCOUNT_ID }}:retail-forecast-alerts" \
          --dimensions "Name=DeploymentId,Value=$DEPLOYMENT_ID"
        
        echo "CloudWatch alarms configured successfully"

  notify:
    name: Send Notifications
    needs: [setup, deploy_eks, monitoring]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Determine workflow status
      id: status
      run: |
        if [[ "${{ needs.deploy_eks.result }}" == "success" ]]; then
          echo "result=success" >> $GITHUB_OUTPUT
          echo "message=‚úÖ Deployment successful for ${{ needs.setup.outputs.environment }} environment (Build ${{ needs.setup.outputs.build-id }})" >> $GITHUB_OUTPUT
        elif [[ "${{ needs.deploy_eks.result }}" == "failure" ]]; then
          echo "result=failure" >> $GITHUB_OUTPUT
          echo "message=‚ùå Deployment failed for ${{ needs.setup.outputs.environment }} environment (Build ${{ needs.setup.outputs.build-id }})" >> $GITHUB_OUTPUT
        else
          echo "result=skipped" >> $GITHUB_OUTPUT
          echo "message=‚ÑπÔ∏è Deployment skipped for ${{ needs.setup.outputs.environment }} environment (Build ${{ needs.setup.outputs.build-id }})" >> $GITHUB_OUTPUT
        fi
    
    - name: Configure AWS credentials
      if: steps.status.outputs.result != 'skipped'
      uses: aws-actions/configure-aws-credentials@v2
      with:
        role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole
        aws-region: ${{ env.AWS_REGION }}
    
    - name: Send SNS notification
      if: steps.status.outputs.result != 'skipped'
      run: |
        aws sns publish \
          --topic-arn "arn:aws:sns:${{ env.AWS_REGION }}:${{ secrets.AWS_ACCOUNT_ID }}:retail-forecast-alerts" \
          --subject "Retail Forecast Deployment: ${{ steps.status.outputs.result }}" \
          --message "${{ steps.status.outputs.message }}"
```

## 3. IAM Role v√† Quy·ªÅn h·∫°n

ƒê·ªÉ GitHub Actions c√≥ th·ªÉ t∆∞∆°ng t√°c v·ªõi c√°c d·ªãch v·ª• AWS, ch√∫ng ta c·∫ßn thi·∫øt l·∫≠p IAM Role v·ªõi quy·ªÅn h·∫°n ph√π h·ª£p v√† s·ª≠ d·ª•ng OIDC ƒë·ªÉ x√°c th·ª±c.

### 3.1 T·∫°o IAM Role cho GitHub Actions

```bash
# T·∫°o IAM Role cho GitHub Actions
cat > trust-policy.json << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Federated": "arn:aws:iam::<ACCOUNT_ID>:oidc-provider/token.actions.githubusercontent.com"
      },
      "Action": "sts:AssumeRoleWithWebIdentity",
      "Condition": {
        "StringEquals": {
          "token.actions.githubusercontent.com:aud": "sts.amazonaws.com"
        },
        "StringLike": {
          "token.actions.githubusercontent.com:sub": "repo:<GITHUB_ORG>/<REPO_NAME>:*"
        }
      }
    }
  ]
}
EOF

# T·∫°o role
aws iam create-role --role-name GitHubActionsRole \
  --assume-role-policy-document file://trust-policy.json \
  --description "Role for GitHub Actions CI/CD pipeline"
```

### 3.2 IAM Policy cho CI/CD Pipeline

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::retail-forecast-data*",
        "arn:aws:s3:::retail-forecast-data*/*",
        "arn:aws:s3:::retail-forecast-models*",
        "arn:aws:s3:::retail-forecast-models*/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "sagemaker:CreateTrainingJob",
        "sagemaker:DescribeTrainingJob",
        "sagemaker:StopTrainingJob",
        "sagemaker:ListTrainingJobs",
        "sagemaker:CreateProcessingJob",
        "sagemaker:DescribeProcessingJob",
        "sagemaker:StopProcessingJob",
        "sagemaker:CreateModel",
        "sagemaker:DeleteModel",
        "sagemaker:DescribeModel",
        "sagemaker:CreateEndpoint",
        "sagemaker:DescribeEndpoint",
        "sagemaker:DeleteEndpoint",
        "sagemaker:UpdateEndpoint",
        "sagemaker:CreateEndpointConfig",
        "sagemaker:DeleteEndpointConfig",
        "sagemaker:DescribeEndpointConfig",
        "sagemaker:CreateModelPackageGroup",
        "sagemaker:DescribeModelPackageGroup",
        "sagemaker:ListModelPackageGroups",
        "sagemaker:CreateModelPackage",
        "sagemaker:DescribeModelPackage",
        "sagemaker:ListModelPackages",
        "sagemaker:UpdateModelPackage"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "ecr:GetAuthorizationToken",
        "ecr:BatchCheckLayerAvailability",
        "ecr:GetDownloadUrlForLayer",
        "ecr:BatchGetImage",
        "ecr:InitiateLayerUpload",
        "ecr:UploadLayerPart",
        "ecr:CompleteLayerUpload",
        "ecr:PutImage",
        "ecr:ListImages",
        "ecr:DescribeImages"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "eks:DescribeCluster",
        "eks:ListClusters"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "cloudwatch:PutMetricData",
        "cloudwatch:GetMetricData",
        "cloudwatch:PutMetricAlarm",
        "cloudwatch:DescribeAlarms",
        "cloudwatch:DeleteAlarms"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents",
        "logs:DescribeLogStreams"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "sns:Publish"
      ],
      "Resource": "arn:aws:sns:*:*:retail-forecast-*"
    }
  ]
}
```

### 3.3 G·∫Øn Policy cho Role

```bash
# T·∫°o v√† g·∫Øn policy
aws iam create-policy \
  --policy-name GitHubActionsPolicy \
  --policy-document file://github-actions-policy.json

aws iam attach-role-policy \
  --role-name GitHubActionsRole \
  --policy-arn arn:aws:iam::<ACCOUNT_ID>:policy/GitHubActionsPolicy

# G·∫Øn th√™m c√°c managed policy c·∫ßn thi·∫øt
aws iam attach-role-policy \
  --role-name GitHubActionsRole \
  --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy

aws iam attach-role-policy \
  --role-name GitHubActionsRole \
  --policy-arn arn:aws:iam::aws:policy/AmazonECR-FullAccess
```

## 4. Script h·ªó tr·ª£ cho Pipeline

### 4.1 Script t·∫°o Training Job

```python
# aws/script/create_training_job.py
import boto3
import argparse
import json
import os
from datetime import datetime

def create_training_job(job_name, data_bucket, output_bucket, 
                        instance_type='ml.m5.large', 
                        hyperparameters=None):
    """
    T·∫°o SageMaker training job cho Retail Forecast model
    """
    sagemaker = boto3.client('sagemaker')
    
    if hyperparameters is None:
        hyperparameters = {
            'n_estimators': '100',
            'max_depth': '10',
            'random_state': '42'
        }
    elif isinstance(hyperparameters, str):
        hyperparameters = json.loads(hyperparameters)
    
    # L·∫•y SageMaker execution role t·ª´ m√¥i tr∆∞·ªùng ho·∫∑c t·∫°o m·ªõi
    role_arn = os.environ.get('SAGEMAKER_ROLE_ARN')
    if not role_arn:
        # T√¨m role m·∫∑c ƒë·ªãnh
        iam = boto3.client('iam')
        paginator = iam.get_paginator('list_roles')
        for page in paginator.paginate():
            for role in page['Roles']:
                if 'AmazonSageMaker-ExecutionRole' in role['RoleName']:
                    role_arn = role['Arn']
                    break
    
    if not role_arn:
        raise ValueError("Kh√¥ng t√¨m th·∫•y SageMaker execution role")
    
    # C·∫•u h√¨nh training job
    training_params = {
        'TrainingJobName': job_name,
        'HyperParameters': hyperparameters,
        'AlgorithmSpecification': {
            'TrainingImage': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:1.0-1-cpu-py3',
            'TrainingInputMode': 'File'
        },
        'RoleArn': role_arn,
        'InputDataConfig': [
            {
                'ChannelName': 'train',
                'DataSource': {
                    'S3DataSource': {
                        'S3DataType': 'S3Prefix',
                        'S3Uri': f's3://{data_bucket}/training/',
                        'S3DataDistributionType': 'FullyReplicated'
                    }
                },
                'ContentType': 'text/csv'
            },
            {
                'ChannelName': 'validation',
                'DataSource': {
                    'S3DataSource': {
                        'S3DataType': 'S3Prefix',
                        'S3Uri': f's3://{data_bucket}/validation/',
                        'S3DataDistributionType': 'FullyReplicated'
                    }
                },
                'ContentType': 'text/csv'
            }
        ],
        'OutputDataConfig': {
            'S3OutputPath': f's3://{output_bucket}/models/'
        },
        'ResourceConfig': {
            'InstanceType': instance_type,
            'InstanceCount': 1,
            'VolumeSizeInGB': 30
        },
        'StoppingCondition': {
            'MaxRuntimeInSeconds': 3600
        },
        'Tags': [
            {'Key': 'Project', 'Value': 'RetailForecast'},
            {'Key': 'CreatedBy', 'Value': 'GitHubActions'},
            {'Key': 'Environment', 'Value': os.environ.get('ENVIRONMENT', 'dev')}
        ]
    }
    
    # T·∫°o training job
    response = sagemaker.create_training_job(**training_params)
    print(f"Training job created: {job_name}")
    print(f"ARN: {response['TrainingJobArn']}")
    
    return response['TrainingJobArn']

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Create SageMaker training job')
    parser.add_argument('--job-name', type=str, required=True, 
                        help='Name for the training job')
    parser.add_argument('--data-bucket', type=str, required=True,
                        help='S3 bucket containing training data')
    parser.add_argument('--output-bucket', type=str, required=True,
                        help='S3 bucket for output artifacts')
    parser.add_argument('--instance-type', type=str, default='ml.m5.large',
                        help='SageMaker instance type')
    parser.add_argument('--hyperparameters', type=str, default=None,
                        help='JSON string of hyperparameters')
    
    args = parser.parse_args()
    
    create_training_job(
        args.job_name,
        args.data_bucket,
        args.output_bucket,
        args.instance_type,
        args.hyperparameters
    )
```

### 4.2 Script ƒë√°nh gi√° Model

```python
# aws/script/processing_evaluate.py
import boto3
import argparse
import json
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def evaluate_model(job_name, evaluation_data, baseline_metrics=None, threshold=0.85):
    """ƒê√°nh gi√° model v√† so s√°nh v·ªõi baseline metrics"""
    
    # T·∫°o th∆∞ m·ª•c cho plots
    os.makedirs('plots', exist_ok=True)
    
    sagemaker = boto3.client('sagemaker')
    s3 = boto3.client('s3')
    
    # L·∫•y th√¥ng tin training job
    response = sagemaker.describe_training_job(TrainingJobName=job_name)
    if response['TrainingJobStatus'] != 'Completed':
        raise Exception(f"Training job {job_name} ch∆∞a ho√†n th√†nh")
    
    model_artifacts = response['ModelArtifacts']['S3ModelArtifacts']
    print(f"Model artifacts: {model_artifacts}")
    
    # Download d·ªØ li·ªáu ƒë√°nh gi√° t·ª´ S3
    if evaluation_data.startswith('s3://'):
        bucket, key = evaluation_data.replace('s3://', '').split('/', 1)
        local_path = 'evaluation_data.csv'
        s3.download_file(bucket, key, local_path)
    else:
        local_path = evaluation_data
    
    # Load d·ªØ li·ªáu ƒë√°nh gi√°
    eval_data = pd.read_csv(local_path)
    
    # Trong d·ª± √°n th·ª±c t·∫ø, ·ªü ƒë√¢y s·∫Ω load model t·ª´ S3 v√† ƒë√°nh gi√°
    # V√≠ d·ª• ƒë∆°n gi·∫£n n√†y gi·∫£ ƒë·ªãnh ch√∫ng ta ƒë√£ c√≥ k·∫øt qu·∫£ d·ª± ƒëo√°n
    
    # Gi·∫£ ƒë·ªãnh k·∫øt qu·∫£ ƒë√°nh gi√°
    # Trong th·ª±c t·∫ø, b·∫°n s·∫Ω load model v√† th·ª±c hi·ªán d·ª± ƒëo√°n
    y_true = eval_data['target'].values
    y_pred = y_true * 0.9 + np.random.normal(0, 0.2, size=y_true.shape)
    
    # T√≠nh to√°n metrics
    metrics = {
        'mse': mean_squared_error(y_true, y_pred),
        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
        'mae': mean_absolute_error(y_true, y_pred),
        'r2_score': r2_score(y_true, y_pred),
        'accuracy': 0.88  # Gi·∫£ ƒë·ªãnh cho forecasting task
    }
    
    # So s√°nh v·ªõi baseline metrics n·∫øu c√≥
    baseline = {}
    if baseline_metrics:
        if baseline_metrics.startswith('s3://'):
            bucket, key = baseline_metrics.replace('s3://', '').split('/', 1)
            local_baseline = 'baseline_metrics.json'
            try:
                s3.download_file(bucket, key, local_baseline)
                with open(local_baseline, 'r') as f:
                    baseline = json.load(f)
            except:
                print(f"Kh√¥ng t√¨m th·∫•y baseline metrics: {baseline_metrics}")
        else:
            try:
                with open(baseline_metrics, 'r') as f:
                    baseline = json.load(f)
            except:
                print(f"Kh√¥ng t√¨m th·∫•y baseline metrics: {baseline_metrics}")
    
    # Quy·∫øt ƒë·ªãnh model c√≥ ƒë∆∞·ª£c ch·∫•p nh·∫≠n hay kh√¥ng
    approved = True
    if baseline:
        print("So s√°nh v·ªõi baseline:")
        for key in metrics:
            if key in baseline:
                improvement = (metrics[key] - baseline[key]) / baseline[key] * 100
                print(f"{key}: {metrics[key]:.4f} (baseline: {baseline[key]:.4f}, {improvement:+.2f}%)")
                
                # Ki·ªÉm tra ti√™u ch√≠ c·∫£i thi·ªán
                if key == 'accuracy' and metrics[key] < threshold:
                    approved = False
                elif key in ['mse', 'rmse', 'mae'] and metrics[key] > baseline[key] * 1.1:  # Cho ph√©p t·ªá h∆°n 10%
                    approved = False
    else:
        print("Kh√¥ng c√≥ baseline ƒë·ªÉ so s√°nh")
    
    # T·∫°o visualizations
    plt.figure(figsize=(10, 6))
    plt.scatter(y_true, y_pred, alpha=0.5)
    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')
    plt.xlabel('Actual')
    plt.ylabel('Predicted')
    plt.title('Actual vs Predicted Values')
    plt.savefig('plots/actual_vs_predicted.png')
    
    # Residual plot
    residuals = y_true - y_pred
    plt.figure(figsize=(10, 6))
    plt.scatter(y_pred, residuals, alpha=0.5)
    plt.hlines(y=0, xmin=y_pred.min(), xmax=y_pred.max(), colors='r', linestyles='--')
    plt.xlabel('Predicted')
    plt.ylabel('Residuals')
    plt.title('Residual Plot')
    plt.savefig('plots/residuals.png')
    
    # L∆∞u k·∫øt qu·∫£ ƒë√°nh gi√°
    result = {
        'job_name': job_name,
        'metrics': metrics,
        'baseline': baseline,
        'approved': approved,
        'model_uri': model_artifacts
    }
    
    with open('evaluation_results.json', 'w') as f:
        json.dump(result, f, indent=2)
    
    # L∆∞u k·∫øt qu·∫£ approved ƒë·ªÉ GitHub Actions c√≥ th·ªÉ ƒë·ªçc
    with open('model_approved.txt', 'w') as f:
        f.write(str(approved).lower())
    
    print(f"Model approved: {approved}")
    return approved

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Evaluate trained model')
    parser.add_argument('--job-name', type=str, required=True,
                        help='Name of the SageMaker training job')
    parser.add_argument('--evaluation-data', type=str, required=True,
                        help='S3 URI or local path to evaluation data')
    parser.add_argument('--baseline-metrics', type=str, default=None,
                        help='S3 URI or local path to baseline metrics')
    parser.add_argument('--threshold', type=float, default=0.85,
                        help='Accuracy threshold for model approval')
    
    args = parser.parse_args()
    
    evaluate_model(
        args.job_name,
        args.evaluation_data,
        args.baseline_metrics,
        args.threshold
    )
```

### 4.3 Script ƒëƒÉng k√Ω Model Registry

```python
# aws/script/register_model.py
import boto3
import argparse
import json
import time

def register_model(job_name, model_package_group, approval_status='PendingManualApproval'):
    """ƒêƒÉng k√Ω model v√†o Model Registry"""
    
    sagemaker = boto3.client('sagemaker')
    
    # Ki·ªÉm tra xem Model Package Group ƒë√£ t·ªìn t·∫°i ch∆∞a
    try:
        sagemaker.describe_model_package_group(ModelPackageGroupName=model_package_group)
        print(f"Model Package Group {model_package_group} ƒë√£ t·ªìn t·∫°i")
    except sagemaker.exceptions.ResourceNotFound:
        print(f"T·∫°o Model Package Group m·ªõi: {model_package_group}")
        sagemaker.create_model_package_group(
            ModelPackageGroupName=model_package_group,
            ModelPackageGroupDescription="Retail Forecast Models",
            Tags=[
                {'Key': 'Project', 'Value': 'RetailForecast'}
            ]
        )
    
    # L·∫•y th√¥ng tin training job
    training_job = sagemaker.describe_training_job(TrainingJobName=job_name)
    model_data_url = training_job['ModelArtifacts']['S3ModelArtifacts']
    image_uri = training_job['AlgorithmSpecification']['TrainingImage']
    
    # ƒêƒÉng k√Ω model package
    model_package_name = f"{model_package_group}-{int(time.time())}"
    
    create_model_package_input_dict = {
        'ModelPackageGroupName': model_package_group,
        'ModelPackageDescription': f"Model trained by job {job_name}",
        'InferenceSpecification': {
            'Containers': [
                {
                    'Image': image_uri,
                    'ModelDataUrl': model_data_url
                }
            ],
            'SupportedContentTypes': ['text/csv'],
            'SupportedResponseMIMETypes': ['text/csv']
        },
        'ModelApprovalStatus': approval_status,
        'CustomerMetadataProperties': {
            'TrainingJobName': job_name,
            'CreatedBy': 'GitHubActionsCI'
        },
        'Tags': [
            {'Key': 'Project', 'Value': 'RetailForecast'}
        ]
    }
    
    # Th√™m model metrics n·∫øu c√≥
    try:
        with open('evaluation_results.json', 'r') as f:
            eval_results = json.load(f)
        
        model_metrics = []
        for metric_name, value in eval_results.get('metrics', {}).items():
            model_metrics.append({
                'Name': metric_name,
                'Value': float(value)
            })
        
        if model_metrics:
            create_model_package_input_dict['ModelMetrics'] = {
                'ModelQuality': {
                    'Statistics': {
                        'ContentType': 'application/json',
                        'Values': model_metrics
                    }
                }
            }
    except Exception as e:
        print(f"Kh√¥ng th·ªÉ ƒë·ªçc k·∫øt qu·∫£ ƒë√°nh gi√°: {e}")
    
    response = sagemaker.create_model_package(**create_model_package_input_dict)
    
    print(f"Model package ƒë√£ ƒë∆∞·ª£c t·∫°o: {response['ModelPackageArn']}")
    
    # L·∫•y version t·ª´ ARN
    model_version = response['ModelPackageArn'].split('/')[-1]
    print(f"Model version: {model_version}")
    
    return model_version

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Register model to SageMaker Model Registry')
    parser.add_argument('--job-name', type=str, required=True,
                        help='Name of the training job')
    parser.add_argument('--model-package-group', type=str, required=True,
                        help='Model package group name')
    parser.add_argument('--approval-status', type=str, default='PendingManualApproval',
                        choices=['Approved', 'Rejected', 'PendingManualApproval'],
                        help='Initial model approval status')
    
    args = parser.parse_args()
    
    model_version = register_model(
        args.job_name,
        args.model_package_group,
        args.approval_status
    )
    
    print(model_version)
```

### 4.4 Script tri·ªÉn khai Endpoint

```python
# aws/script/deploy_endpoint.py
import boto3
import argparse
import time
import uuid

def deploy_endpoint(model_package_arn, endpoint_name, instance_type, instance_count=1):
    """Deploy model to SageMaker endpoint"""
    
    sagemaker = boto3.client('sagemaker')
    timestamp = int(time.time())
    
    # T·∫°o model
    model_name = f"{endpoint_name}-model-{timestamp}"
    print(f"Creating model {model_name} from {model_package_arn}")
    
    model_response = sagemaker.create_model(
        ModelName=model_name,
        PrimaryContainer={
            'ModelPackageName': model_package_arn
        },
        ExecutionRoleArn=sagemaker.get_caller_identity()['RoleArn']
    )
    
    # T·∫°o endpoint config
    config_name = f"{endpoint_name}-config-{timestamp}"
    print(f"Creating endpoint config {config_name}")
    
    endpoint_config_response = sagemaker.create_endpoint_config(
        EndpointConfigName=config_name,
        ProductionVariants=[
            {
                'VariantName': 'default',
                'ModelName': model_name,
                'InstanceType': instance_type,
                'InitialInstanceCount': instance_count,
                'InitialVariantWeight': 1.0
            }
        ],
        Tags=[
            {'Key': 'Project', 'Value': 'RetailForecast'}
        ]
    )
    
    # Ki·ªÉm tra xem endpoint ƒë√£ t·ªìn t·∫°i ch∆∞a
    endpoint_exists = False
    try:
        response = sagemaker.describe_endpoint(EndpointName=endpoint_name)
        endpoint_exists = True
    except sagemaker.exceptions.ClientError:
        endpoint_exists = False
    
    # T·∫°o ho·∫∑c c·∫≠p nh·∫≠t endpoint
    if endpoint_exists:
        print(f"Updating endpoint {endpoint_name}")
        endpoint_response = sagemaker.update_endpoint(
            EndpointName=endpoint_name,
            EndpointConfigName=config_name
        )
    else:
        print(f"Creating endpoint {endpoint_name}")
        endpoint_response = sagemaker.create_endpoint(
            EndpointName=endpoint_name,
            EndpointConfigName=config_name,
            Tags=[
                {'Key': 'Project', 'Value': 'RetailForecast'}
            ]
        )
    
    # ƒê·ª£i endpoint s·∫µn s√†ng
    print(f"Waiting for endpoint {endpoint_name} to be in service...")
    waiter = sagemaker.get_waiter('endpoint_in_service')
    waiter.wait(EndpointName=endpoint_name)
    
    # L·∫•y th√¥ng tin endpoint
    endpoint_info = sagemaker.describe_endpoint(EndpointName=endpoint_name)
    print(f"Endpoint {endpoint_name} is ready (status: {endpoint_info['EndpointStatus']})")
    
    return {
        'endpoint_name': endpoint_name,
        'endpoint_arn': endpoint_info['EndpointArn'],
        'status': endpoint_info['EndpointStatus']
    }

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Deploy model to SageMaker endpoint')
    parser.add_argument('--model-package-arn', type=str, required=True,
                        help='ARN of model package to deploy')
    parser.add_argument('--endpoint-name', type=str, required=True,
                        help='Name of the endpoint')
    parser.add_argument('--instance-type', type=str, default='ml.t2.medium',
                        help='Instance type for inference')
    parser.add_argument('--instance-count', type=int, default=1,
                        help='Number of instances')
    
    args = parser.parse_args()
    
    result = deploy_endpoint(
        args.model_package_arn,
        args.endpoint_name,
        args.instance_type,
        args.instance_count
    )
    
    print(f"Endpoint deployment complete: {result}")
```

### 4.5 Script ki·ªÉm tra hi·ªáu su·∫•t Endpoint

```python
# aws/script/autoscaling_endpoint.py
import boto3
import argparse
import json
import time
import datetime

def setup_autoscaling(endpoint_name, min_capacity=1, max_capacity=4, 
                      target_value=70.0, scale_in_cooldown=300, 
                      scale_out_cooldown=60):
    """Thi·∫øt l·∫≠p autoscaling cho SageMaker endpoint"""
    
    # L·∫•y variant name v√† ARN c·ªßa endpoint
    sm = boto3.client('sagemaker')
    endpoint = sm.describe_endpoint(EndpointName=endpoint_name)
    endpoint_arn = endpoint['EndpointArn']
    
    config_name = endpoint['EndpointConfigName']
    config = sm.describe_endpoint_config(EndpointConfigName=config_name)
    variant_name = config['ProductionVariants'][0]['VariantName']
    
    # Chu·∫©n b·ªã resource ID cho autoscaling
    resource_id = f"endpoint/{endpoint_name}/variant/{variant_name}"
    
    # Thi·∫øt l·∫≠p application autoscaling
    aas = boto3.client('application-autoscaling')
    
    # ƒêƒÉng k√Ω scalable target
    aas.register_scalable_target(
        ServiceNamespace='sagemaker',
        ResourceId=resource_id,
        ScalableDimension='sagemaker:variant:DesiredInstanceCount',
        MinCapacity=min_capacity,
        MaxCapacity=max_capacity
    )
    
    # T·∫°o scaling policy
    response = aas.put_scaling_policy(
        PolicyName=f"{endpoint_name}-scaling-policy",
        ServiceNamespace='sagemaker',
        ResourceId=resource_id,
        ScalableDimension='sagemaker:variant:DesiredInstanceCount',
        PolicyType='TargetTrackingScaling',
        TargetTrackingScalingPolicyConfiguration={
            'TargetValue': target_value,
            'PredefinedMetricSpecification': {
                'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance'
            },
            'ScaleInCooldown': scale_in_cooldown,
            'ScaleOutCooldown': scale_out_cooldown
        }
    )
    
    print(f"Autoscaling configured for endpoint {endpoint_name}")
    print(f"Min capacity: {min_capacity}, Max capacity: {max_capacity}")
    print(f"Target value: {target_value} invocations per instance")
    
    return {
        'endpoint_name': endpoint_name,
        'resource_id': resource_id,
        'scaling_policy_arn': response['PolicyARN']
    }

def load_test_endpoint(endpoint_name, test_data_path, duration=60, rate=10):
    """Test t·∫£i endpoint ƒë·ªÉ ki·ªÉm tra hi·ªáu su·∫•t v√† autoscaling"""
    
    sagemaker_runtime = boto3.client('sagemaker-runtime')
    
    # Load test data
    with open(test_data_path, 'r') as f:
        test_data = json.load(f)
    
    # N·∫øu test data l√† list, l·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n
    if isinstance(test_data, list):
        sample = test_data[0]
    else:
        sample = test_data
    
    # Chu·∫©n b·ªã test
    start_time = time.time()
    end_time = start_time + duration
    request_count = 0
    success_count = 0
    latencies = []
    
    print(f"Starting load test on endpoint {endpoint_name}")
    print(f"Duration: {duration} seconds, Rate: {rate} requests/second")
    
    # Th·ª±c hi·ªán load test
    while time.time() < end_time:
        batch_start = time.time()
        for _ in range(rate):
            if time.time() >= end_time:
                break
                
            try:
                # G·ª≠i request
                request_start = time.time()
                response = sagemaker_runtime.invoke_endpoint(
                    EndpointName=endpoint_name,
                    ContentType='application/json',
                    Body=json.dumps(sample)
                )
                
                # ƒêo latency
                latency = (time.time() - request_start) * 1000  # milliseconds
                latencies.append(latency)
                
                # ƒê·ªçc k·∫øt qu·∫£
                result = json.loads(response['Body'].read().decode())
                
                # C·∫≠p nh·∫≠t counter
                request_count += 1
                success_count += 1
                
                if request_count % 50 == 0:
                    print(f"Processed {request_count} requests...")
                
            except Exception as e:
                request_count += 1
                print(f"Error invoking endpoint: {e}")
        
        # ƒê·ª£i ƒë·∫øn ƒë·∫ßu gi√¢y ti·∫øp theo
        elapsed = time.time() - batch_start
        if elapsed < 1.0:
            time.sleep(1.0 - elapsed)
    
    # T√≠nh to√°n k·∫øt qu·∫£
    total_time = time.time() - start_time
    avg_rate = request_count / total_time
    success_rate = (success_count / request_count) * 100 if request_count > 0 else 0
    
    if latencies:
        avg_latency = sum(latencies) / len(latencies)
        min_latency = min(latencies)
        max_latency = max(latencies)
        p95_latency = sorted(latencies)[int(len(latencies) * 0.95)]
        p99_latency = sorted(latencies)[int(len(latencies) * 0.99)]
    else:
        avg_latency = min_latency = max_latency = p95_latency = p99_latency = 0
    
    # In k·∫øt qu·∫£
    results = {
        'endpoint': endpoint_name,
        'duration_seconds': total_time,
        'request_count': request_count,
        'success_count': success_count,
        'success_rate_percent': success_rate,
        'avg_requests_per_second': avg_rate,
        'latency_ms': {
            'avg': avg_latency,
            'min': min_latency,
            'max': max_latency,
            'p95': p95_latency,
            'p99': p99_latency
        }
    }
    
    print("\nLoad Test Results:")
    print(json.dumps(results, indent=2))
    
    with open('load_test_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    return results

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Setup autoscaling and test SageMaker endpoint')
    subparsers = parser.add_subparsers(dest='command')
    
    # Subparser cho autoscaling
    autoscaling_parser = subparsers.add_parser('autoscale')
    autoscaling_parser.add_argument('--endpoint-name', type=str, required=True)
    autoscaling_parser.add_argument('--min-capacity', type=int, default=1)
    autoscaling_parser.add_argument('--max-capacity', type=int, default=4)
    autoscaling_parser.add_argument('--target-value', type=float, default=70.0)
    
    # Subparser cho load testing
    loadtest_parser = subparsers.add_parser('loadtest')
    loadtest_parser.add_argument('--endpoint-name', type=str, required=True)
    loadtest_parser.add_argument('--test-data', type=str, required=True)
    loadtest_parser.add_argument('--duration', type=int, default=60)
    loadtest_parser.add_argument('--rate', type=int, default=10)
    
    args = parser.parse_args()
    
    if args.command == 'autoscale':
        setup_autoscaling(
            args.endpoint_name,
            args.min_capacity,
            args.max_capacity,
            args.target_value
        )
    elif args.command == 'loadtest':
        load_test_endpoint(
            args.endpoint_name,
            args.test_data,
            args.duration,
            args.rate
        )
    else:
        parser.print_help()
```

## 5. Gi√°m s√°t v√† Th√¥ng b√°o

### 5.1 Thi·∫øt l·∫≠p SNS Topic

```bash
# T·∫°o SNS topic cho th√¥ng b√°o deployment
aws sns create-topic --name retail-forecast-alerts

# ƒêƒÉng k√Ω email nh·∫≠n th√¥ng b√°o
aws sns subscribe \
  --topic-arn arn:aws:sns:us-east-1:<ACCOUNT_ID>:retail-forecast-alerts \
  --protocol email \
  --notification-endpoint team@example.com

# ƒêƒÉng k√Ω webhook Slack 
aws sns subscribe \
  --topic-arn arn:aws:sns:us-east-1:<ACCOUNT_ID>:retail-forecast-alerts \
  --protocol https \
  --notification-endpoint https://hooks.slack.com/services/XXXX/YYYY/ZZZZ
```

### 5.2 CloudWatch Dashboard cho CI/CD Pipeline

```bash
# T·∫°o CloudWatch dashboard cho pipeline
aws cloudwatch put-dashboard \
  --dashboard-name RetailForecast-CI-CD-Pipeline \
  --dashboard-body '{
    "widgets": [
        {
            "type": "metric",
            "x": 0,
            "y": 0,
            "width": 12,
            "height": 6,
            "properties": {
                "metrics": [
                    [ "AWS/SageMaker", "TrainingJobsCompleted", { "stat": "Sum", "period": 86400 } ],
                    [ "AWS/SageMaker", "TrainingJobsFailed", { "stat": "Sum", "period": 86400 } ]
                ],
                "view": "timeSeries",
                "stacked": false,
                "region": "us-east-1",
                "title": "SageMaker Training Jobs",
                "period": 300,
                "stat": "Sum"
            }
        },
        {
            "type": "metric",
            "x": 12,
            "y": 0,
            "width": 12,
            "height": 6,
            "properties": {
                "metrics": [
                    [ "AWS/ApplicationELB", "TargetResponseTime", "LoadBalancer", "app/retail-forecast/abcdef" ],
                    [ ".", "HTTPCode_Target_5XX_Count", ".", "." ],
                    [ ".", "HTTPCode_Target_4XX_Count", ".", "." ]
                ],
                "view": "timeSeries",
                "stacked": false,
                "region": "us-east-1",
                "title": "API Performance",
                "period": 300,
                "stat": "Average"
            }
        },
        {
            "type": "metric",
            "x": 0,
            "y": 6,
            "width": 12,
            "height": 6,
            "properties": {
                "metrics": [
                    [ "AWS/SageMaker", "Invocations", "EndpointName", "retail-forecast-prod", { "stat": "Sum", "period": 60 } ],
                    [ ".", "InvocationsPerInstance", ".", "." ]
                ],
                "view": "timeSeries",
                "stacked": false,
                "region": "us-east-1",
                "title": "SageMaker Endpoint Invocations",
                "period": 300
            }
        },
        {
            "type": "metric",
            "x": 12,
            "y": 6,
            "width": 12,
            "height": 6,
            "properties": {
                "metrics": [
                    [ "RetailForecast/Pipeline", "DeploymentFrequency" ],
                    [ ".", "FailureRate" ],
                    [ ".", "LeadTime" ]
                ],
                "view": "timeSeries",
                "stacked": false,
                "region": "us-east-1",
                "title": "CI/CD Metrics",
                "period": 86400,
                "stat": "Average"
            }
        }
    ]
}'
```

### 5.3 C·∫•u h√¨nh Pipeline Metrics

```python
# aws/script/pipeline_metrics.py
import boto3
import json
import argparse
from datetime import datetime, timedelta

def publish_pipeline_metrics(deploy_success=True, lead_time=None):
    """Publish CI/CD pipeline metrics to CloudWatch"""
    
    cloudwatch = boto3.client('cloudwatch')
    
    # Get date parts for daily metrics
    now = datetime.utcnow()
    today = now.strftime('%Y-%m-%d')
    
    # Increment deployment count
    cloudwatch.put_metric_data(
        Namespace='RetailForecast/Pipeline',
        MetricData=[
            {
                'MetricName': 'DeploymentCount',
                'Dimensions': [
                    {
                        'Name': 'Date',
                        'Value': today
                    }
                ],
                'Value': 1.0,
                'Unit': 'Count'
            }
        ]
    )
    
    # Add deployment success/failure
    cloudwatch.put_metric_data(
        Namespace='RetailForecast/Pipeline',
        MetricData=[
            {
                'MetricName': 'DeploymentSuccess',
                'Dimensions': [
                    {
                        'Name': 'Date',
                        'Value': today
                    }
                ],
                'Value': 1.0 if deploy_success else 0.0,
                'Unit': 'Count'
            }
        ]
    )
    
    # Add lead time if provided
    if lead_time is not None:
        cloudwatch.put_metric_data(
            Namespace='RetailForecast/Pipeline',
            MetricData=[
                {
                    'MetricName': 'LeadTimeMinutes',
                    'Dimensions': [
                        {
                            'Name': 'Date',
                            'Value': today
                        }
                    ],
                    'Value': lead_time,
                    'Unit': 'Minutes'
                }
            ]
        )
    
    print(f"Pipeline metrics published successfully")
    
def calculate_pipeline_kpis(days=30):
    """Calculate KPIs for CI/CD pipeline"""
    
    cloudwatch = boto3.client('cloudwatch')
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(days=days)
    
    # Get deployment frequency
    deploy_count_response = cloudwatch.get_metric_statistics(
        Namespace='RetailForecast/Pipeline',
        MetricName='DeploymentCount',
        StartTime=start_time,
        EndTime=end_time,
        Period=86400 * days,  # entire period
        Statistics=['Sum']
    )
    
    # Get deployment success/failure
    deploy_success_response = cloudwatch.get_metric_statistics(
        Namespace='RetailForecast/Pipeline',
        MetricName='DeploymentSuccess',
        StartTime=start_time,
        EndTime=end_time,
        Period=86400 * days,  # entire period
        Statistics=['Sum']
    )
    
    # Get lead time
    lead_time_response = cloudwatch.get_metric_statistics(
        Namespace='RetailForecast/Pipeline',
        MetricName='LeadTimeMinutes',
        StartTime=start_time,
        EndTime=end_time,
        Period=86400 * days,  # entire period
        Statistics=['Average']
    )
    
    # Calculate KPIs
    deploy_count = deploy_count_response['Datapoints'][0]['Sum'] if deploy_count_response['Datapoints'] else 0
    deploy_success = deploy_success_response['Datapoints'][0]['Sum'] if deploy_success_response['Datapoints'] else 0
    
    # Calculate metrics
    deployments_per_day = deploy_count / days if days > 0 else 0
    failure_rate = (deploy_count - deploy_success) / deploy_count * 100 if deploy_count > 0 else 0
    avg_lead_time = lead_time_response['Datapoints'][0]['Average'] if lead_time_response['Datapoints'] else 0
    
    kpis = {
        'period_days': days,
        'total_deployments': deploy_count,
        'successful_deployments': deploy_success,
        'deployments_per_day': deployments_per_day,
        'failure_rate_percent': failure_rate,
        'avg_lead_time_minutes': avg_lead_time
    }
    
    print(json.dumps(kpis, indent=2))
    return kpis

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Manage CI/CD pipeline metrics')
    subparsers = parser.add_subparsers(dest='command')
    
    # Subparser cho publish metrics
    publish_parser = subparsers.add_parser('publish')
    publish_parser.add_argument('--success', type=bool, default=True)
    publish_parser.add_argument('--lead-time', type=float, help='Lead time in minutes')
    
    # Subparser cho calculate KPIs
    kpi_parser = subparsers.add_parser('kpi')
    kpi_parser.add_argument('--days', type=int, default=30)
    
    args = parser.parse_args()
    
    if args.command == 'publish':
        publish_pipeline_metrics(args.success, args.lead_time)
    elif args.command == 'kpi':
        calculate_pipeline_kpis(args.days)
    else:
        parser.print_help()
```

## 6. Ki·ªÉm tra v√† X√°c th·ª±c

### 6.1 Checklist Ho√†n th√†nh

C√°c th√†nh ph·∫ßn ch√≠nh ƒë√£ tri·ªÉn khai:

- **Pipeline GitHub Actions**: Workflow ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh v·ªõi c√°c job ƒë·∫ßy ƒë·ªß
- **IAM Role & Permissions**: Role cho GitHub Actions v·ªõi OIDC x√°c th·ª±c
- **Automated Testing**: Unit tests, code quality check, data validation
- **Model Training**: T·ª± ƒë·ªông trigger SageMaker training jobs 
- **Model Evaluation & Registry**: ƒê√°nh gi√° model v√† ƒëƒÉng k√Ω v√†o Model Registry
- **Docker Build**: T·ª± ƒë·ªông build v√† push image l√™n ECR
- **EKS Deployment**: T·ª± ƒë·ªông c·∫≠p nh·∫≠t Kubernetes deployment
- **Health Check**: Ki·ªÉm tra v√† x√°c nh·∫≠n API ho·∫°t ƒë·ªông sau deployment
- **CloudWatch Alarms**: T·ª± ƒë·ªông c·∫•u h√¨nh alert cho deployment m·ªõi
- **Notifications**: Th√¥ng b√°o k·∫øt qu·∫£ deployment qua SNS

### 6.2 Ki·ªÉm tra Pipeline

ƒê·ªÉ x√°c nh·∫≠n pipeline ho·∫°t ƒë·ªông ch√≠nh x√°c, th·ª±c hi·ªán c√°c b∆∞·ªõc sau:

1. **K√≠ch ho·∫°t pipeline th√¥ng qua commit m·ªõi**
   ```bash
   # Commit code m·ªõi
   echo "# Test CI/CD pipeline" >> README.md
   git add README.md
   git commit -m "Test: Trigger CI/CD pipeline"
   git push origin main
   
   # Ki·ªÉm tra GitHub Actions
   # Pipeline s·∫Ω t·ª± ƒë·ªông k√≠ch ho·∫°t trong v√≤ng 1 ph√∫t
   ```

2. **K√≠ch ho·∫°t pipeline th·ªß c√¥ng v·ªõi hu·∫•n luy·ªán model**
   ```bash
   # S·ª≠ d·ª•ng GitHub UI ƒë·ªÉ k√≠ch ho·∫°t workflow v·ªõi c√°c t√πy ch·ªçn:
   # - environment: prod
   # - retrain_model: true
   ```

3. **Ki·ªÉm tra ECR repository**
   ```bash
   # Ki·ªÉm tra image m·ªõi tr√™n ECR
   aws ecr describe-images \
     --repository-name retail-forecast \
     --query 'sort_by(imageDetails,& imagePushedAt)[-5:]'
   
   # Ki·ªÉm tra tag m·ªõi nh·∫•t
   aws ecr list-images \
     --repository-name retail-forecast \
     --filter "tagStatus=TAGGED" \
     --query 'imageIds[?contains(imageTag, `latest`)]'
   ```

4. **Ki·ªÉm tra SageMaker model v√† deployment**
   ```bash
   # Ki·ªÉm tra training job m·ªõi nh·∫•t
   aws sagemaker list-training-jobs \
     --sort-by CreationTime \
     --sort-order Descending \
     --max-items 5
   
   # Ki·ªÉm tra model package m·ªõi nh·∫•t
   aws sagemaker list-model-packages \
     --model-package-group-name retail-forecast-models \
     --sort-by CreationTime \
     --sort-order Descending \
     --max-items 5
   
   # Ki·ªÉm tra endpoint
   aws sagemaker describe-endpoint \
     --endpoint-name retail-forecast-prod
   ```

5. **Ki·ªÉm tra EKS deployment**
   ```bash
   # Ki·ªÉm tra deployment m·ªõi
   kubectl get deployments -n retail-forecast-prod
   
   # Ki·ªÉm tra pods
   kubectl get pods -n retail-forecast-prod
   
   # Ki·ªÉm tra image ƒë∆∞·ª£c s·ª≠ d·ª•ng
   kubectl describe deployment retail-forecast-api -n retail-forecast-prod | grep Image:
   
   # Ki·ªÉm tra history rollout
   kubectl rollout history deployment/retail-forecast-api -n retail-forecast-prod
   ```

6. **Test API endpoint**
   ```bash
   # L·∫•y endpoint URL
   ENDPOINT=$(kubectl get ingress -n retail-forecast-prod retail-forecast-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
   
   # Ki·ªÉm tra health endpoint
   curl -v http://$ENDPOINT/health
   
   # Test prediction endpoint
   curl -X POST \
     -H "Content-Type: application/json" \
     -d '{"store_id": 1, "item_id": 123, "date": "2023-12-01"}' \
     http://$ENDPOINT/predict
   ```

### 6.3 Monitoring Commands

```bash
# Ki·ªÉm tra CloudWatch metrics c·ªßa API
aws cloudwatch get-metric-statistics \
  --namespace "RetailForecast/prod" \
  --metric-name "Latency" \
  --dimensions Name=DeploymentId,Value=<latest-deployment-id> \
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \
  --period 60 \
  --statistics Average

# Ki·ªÉm tra SageMaker endpoint metrics
aws cloudwatch get-metric-statistics \
  --namespace "AWS/SageMaker" \
  --metric-name "ModelLatency" \
  --dimensions Name=EndpointName,Value=retail-forecast-prod \
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ) \
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \
  --period 60 \
  --statistics Average

# Ki·ªÉm tra Pipeline metrics
aws cloudwatch get-metric-statistics \
  --namespace "RetailForecast/Pipeline" \
  --metric-name "DeploymentSuccess" \
  --start-time $(date -u -d '7 days ago' +%Y-%m-%dT%H:%M:%SZ) \
  --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \
  --period 86400 \
  --statistics Sum

# Ki·ªÉm tra CloudWatch Logs cho pipeline
aws logs get-log-events \
  --log-group-name "/aws/sagemaker/TrainingJobs" \
  --log-stream-name <latest-training-job-name>
```

## 7. Best Practices & T·ªëi ∆∞u

### 7.1 Pipeline Optimization

- **Parallel Execution**: Ch·∫°y song song c√°c job kh√¥ng ph·ª• thu·ªôc nhau
- **Caching**: S·ª≠ d·ª•ng caching cho pip v√† Docker layers
- **Conditional Execution**: Ch·ªâ ch·∫°y c√°c job c·∫ßn thi·∫øt d·ª±a tr√™n lo·∫°i thay ƒë·ªïi
- **Matrix Builds**: Test tr√™n nhi·ªÅu phi√™n b·∫£n Python ho·∫∑c m√¥i tr∆∞·ªùng
- **Artifact Sharing**: S·ª≠ d·ª•ng artifacts ƒë·ªÉ chia s·∫ª d·ªØ li·ªáu gi·ªØa c√°c job

### 7.2 Security Best Practices

- **OIDC Integration**: S·ª≠ d·ª•ng federation thay v√¨ access keys
- **Least Privilege IAM**: Role v·ªõi quy·ªÅn h·∫°n t·ªëi thi·ªÉu c·∫ßn thi·∫øt
- **Secret Management**: S·ª≠ d·ª•ng GitHub Secrets ho·∫∑c AWS Secrets Manager
- **Image Scanning**: Ki·ªÉm tra l·ªói b·∫£o m·∫≠t trong container images
- **Network Security**: Gi·ªõi h·∫°n network access trong pipeline

### 7.3 Quality Gates

- **Code Quality Checks**: Linting, formatting, static analysis
- **Unit & Integration Tests**: Test t·ª± ƒë·ªông cho m·ªçi thay ƒë·ªïi code
- **Data Validation**: Ki·ªÉm tra t√≠nh to√†n v·∫πn v√† ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu
- **Model Evaluation**: ƒê√°nh gi√° hi·ªáu su·∫•t model v·ªõi baseline metrics
- **Performance Testing**: Ki·ªÉm tra ƒë·ªô tr·ªÖ v√† kh·∫£ nƒÉng x·ª≠ l√Ω t·∫£i c·ªßa API

### 7.4 Observability

- **Pipeline Metrics**: Theo d√µi t·∫ßn su·∫•t deployment, th·ªùi gian th·ª±c hi·ªán
- **Model Monitoring**: Theo d√µi model drift v√† data drift
- **API Metrics**: Latency, error rate, throughput c·ªßa endpoints
- **Logging**: Log t·∫≠p trung v·ªõi c·∫•u tr√∫c th·ªëng nh·∫•t
- **Alerting**: C·∫£nh b√°o s·ªõm khi ph√°t hi·ªán v·∫•n ƒë·ªÅ

## T·ªïng k·∫øt

CI/CD pipeline ƒë√£ ƒë∆∞·ª£c thi·∫øt l·∫≠p ƒë·∫ßy ƒë·ªß s·ª≠ d·ª•ng GitHub Actions ƒë·ªÉ t·ª± ƒë·ªông h√≥a to√†n b·ªô quy tr√¨nh MLOps cho d·ª± √°n Retail Prediction:

1. **T√≠ch h·ª£p li√™n t·ª•c (CI)**: T·ª± ƒë·ªông test, lint, v√† build Docker image.
2. **Hu·∫•n luy·ªán t·ª± ƒë·ªông**: K√≠ch ho·∫°t SageMaker training jobs khi c·∫ßn thi·∫øt.
3. **Model Registry**: ƒê√°nh gi√° v√† ƒëƒÉng k√Ω model v·ªõi qu·∫£n l√Ω version.
4. **Deployment li√™n t·ª•c (CD)**: T·ª± ƒë·ªông tri·ªÉn khai l√™n EKS cluster.
5. **Gi√°m s√°t**: CloudWatch metrics v√† alerts cho to√†n b·ªô h·ªá th·ªëng.

Pipeline n√†y ƒë·∫£m b·∫£o quy tr√¨nh MLOps ƒë√°ng tin c·∫≠y, t·ª± ƒë·ªông, v√† c√≥ kh·∫£ nƒÉng m·ªü r·ªông, gi√∫p team c√≥ th·ªÉ t·∫≠p trung v√†o vi·ªác c·∫£i thi·ªán model v√† t√≠nh nƒÉng thay v√¨ c√¥ng vi·ªác v·∫≠n h√†nh th·ªß c√¥ng.

{{% notice success %}}
**üéØ Task 11 Complete - CI/CD Pipeline**
- **GitHub Actions workflow** configured v·ªõi ƒë·∫ßy ƒë·ªß CI/CD stages
- **IAM Role & OIDC** setup cho secure authentication
- **Automated testing** v√† quality gates
- **SageMaker integration** cho model training & registry
- **EKS deployment** automation v·ªõi health checks
- **CloudWatch monitoring** v√† notifications
{{% /notice %}}

## 8. Clean Up CI/CD Resources

### 8.1 X√≥a GitHub Actions Workflow

```bash
# Disable GitHub Actions workflows
# Th·ª±c hi·ªán tr·ª±c ti·∫øp tr√™n GitHub repository ho·∫∑c qua GitHub CLI

# N·∫øu s·ª≠ d·ª•ng GitHub CLI
gh workflow disable mlops-pipeline.yml

# List t·∫•t c·∫£ workflow runs
gh run list --limit 50

# Cancel running workflows
gh run list --status in_progress --json databaseId --jq '.[].databaseId' | xargs -I {} gh run cancel {}

# X√≥a workflow artifacts (c√≥ th·ªÉ t·ªën ph√≠ storage)
gh api repos/:owner/:repo/actions/artifacts --paginate | jq -r '.artifacts[] | select(.expired == false) | .id' | xargs -I {} gh api --method DELETE repos/:owner/:repo/actions/artifacts/{}
```

### 8.2 X√≥a IAM Roles v√† Policies

```bash
# List t·∫•t c·∫£ IAM roles li√™n quan ƒë·∫øn CI/CD
aws iam list-roles \
  --query 'Roles[?contains(RoleName, `GitHub`) || contains(RoleName, `CICD`) || contains(RoleName, `RetailForecast`)].RoleName' \
  --output table

# Detach policies tr∆∞·ªõc khi x√≥a role
aws iam list-attached-role-policies \
  --role-name GitHubActionsRole \
  --query 'AttachedPolicies[].PolicyArn' \
  --output text | tr '\t' '\n' | while read policy_arn; do
    echo "Detaching policy: $policy_arn"
    aws iam detach-role-policy --role-name GitHubActionsRole --policy-arn "$policy_arn"
done

# X√≥a custom policies
aws iam delete-policy \
  --policy-arn arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):policy/GitHubActionsPolicy

aws iam delete-policy \
  --policy-arn arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):policy/RetailForecastCICDPolicy

# X√≥a roles
aws iam delete-role --role-name GitHubActionsRole
aws iam delete-role --role-name RetailForecastCICDRole

# X√≥a instance profiles (n·∫øu c√≥)
aws iam remove-role-from-instance-profile \
  --instance-profile-name JenkinsInstanceProfile \
  --role-name JenkinsRole || true

aws iam delete-instance-profile \
  --instance-profile-name JenkinsInstanceProfile || true

aws iam delete-role --role-name JenkinsRole || true
```

### 8.3 X√≥a Jenkins Infrastructure (n·∫øu s·ª≠ d·ª•ng)

```bash
# Terminate Jenkins EC2 instances
aws ec2 describe-instances \
  --filters "Name=tag:Name,Values=Jenkins-Server" "Name=instance-state-name,Values=running" \
  --query 'Reservations[].Instances[].InstanceId' \
  --output text | tr '\t' '\n' | while read instance_id; do
    echo "Terminating Jenkins instance: $instance_id"
    aws ec2 terminate-instances --instance-ids "$instance_id"
done

# X√≥a Jenkins security group
JENKINS_SG_ID=$(aws ec2 describe-security-groups \
  --filters "Name=group-name,Values=sg-jenkins" \
  --query 'SecurityGroups[0].GroupId' \
  --output text)

if [ "$JENKINS_SG_ID" != "None" ]; then
    aws ec2 delete-security-group --group-id "$JENKINS_SG_ID"
fi

# X√≥a Jenkins key pair
aws ec2 delete-key-pair --key-name jenkins-key-pair || true
```

### 8.4 X√≥a SNS Topics v√† Subscriptions

```bash
# List t·∫•t c·∫£ SNS topics li√™n quan
aws sns list-topics \
  --query 'Topics[?contains(TopicArn, `retail-forecast`) || contains(TopicArn, `mlops`)].TopicArn' \
  --output table

# X√≥a subscriptions tr∆∞·ªõc
SNS_TOPIC_ARN=$(aws sns list-topics \
  --query 'Topics[?contains(TopicArn, `retail-forecast-alerts`)].TopicArn' \
  --output text)

if [ "$SNS_TOPIC_ARN" != "" ]; then
    # List v√† x√≥a subscriptions
    aws sns list-subscriptions-by-topic \
      --topic-arn "$SNS_TOPIC_ARN" \
      --query 'Subscriptions[].SubscriptionArn' \
      --output text | tr '\t' '\n' | while read sub_arn; do
        if [ "$sub_arn" != "PendingConfirmation" ]; then
            echo "Unsubscribing: $sub_arn"
            aws sns unsubscribe --subscription-arn "$sub_arn"
        fi
    done
    
    # X√≥a topic
    aws sns delete-topic --topic-arn "$SNS_TOPIC_ARN"
fi
```

### 8.5 X√≥a CloudWatch Resources cho CI/CD

```bash
# X√≥a CloudWatch dashboard cho CI/CD
aws cloudwatch delete-dashboards \
  --dashboard-names "RetailForecast-CI-CD-Pipeline" || true

# X√≥a custom metrics namespace
aws cloudwatch list-metrics \
  --namespace "RetailForecast/Pipeline" \
  --query 'Metrics[].MetricName' \
  --output text | tr '\t' '\n' | while read metric; do
    echo "Found pipeline metric: $metric"
    # Note: CloudWatch metrics t·ª± ƒë·ªông expire sau 15 th√°ng
done

# X√≥a alarms li√™n quan ƒë·∫øn CI/CD
aws cloudwatch describe-alarms \
  --alarm-name-prefix "Pipeline-" \
  --query 'MetricAlarms[].AlarmName' \
  --output text | tr '\t' '\n' | while read alarm; do
    echo "Deleting pipeline alarm: $alarm"
    aws cloudwatch delete-alarms --alarm-names "$alarm"
done

# X√≥a log groups cho CI/CD
aws logs delete-log-group \
  --log-group-name "/aws/codebuild/retail-forecast-build" || true

aws logs delete-log-group \
  --log-group-name "/aws/codepipeline/retail-forecast-pipeline" || true
```

### 8.6 X√≥a ECR Images v√† Tags

```bash
# List t·∫•t c·∫£ images trong ECR repository
aws ecr describe-images \
  --repository-name mlops/retail-api \
  --region ap-southeast-1 \
  --query 'imageDetails[].imageTags[]' \
  --output table

# X√≥a specific CI/CD tags (gi·ªØ l·∫°i production tags)
aws ecr batch-delete-image \
  --repository-name mlops/retail-api \
  --region ap-southeast-1 \
  --image-ids imageTag=dev imageTag=staging imageTag=feature-* || true

# X√≥a untagged images
aws ecr describe-images \
  --repository-name mlops/retail-api \
  --region ap-southeast-1 \
  --filter tagStatus=UNTAGGED \
  --query 'imageDetails[].imageDigest' \
  --output text | tr '\t' '\n' | while read digest; do
    echo "Deleting untagged image: $digest"
    aws ecr batch-delete-image \
      --repository-name mlops/retail-api \
      --region ap-southeast-1 \
      --image-ids imageDigest="$digest"
done
```

### 8.7 Clean Up SageMaker Resources

```bash
# List training jobs created by CI/CD
aws sagemaker list-training-jobs \
  --name-contains "retail-forecast" \
  --max-results 50 \
  --query 'TrainingJobSummaries[].TrainingJobName' \
  --output table

# Stop running training jobs
aws sagemaker list-training-jobs \
  --status-equals InProgress \
  --name-contains "retail-forecast" \
  --query 'TrainingJobSummaries[].TrainingJobName' \
  --output text | tr '\t' '\n' | while read job_name; do
    echo "Stopping training job: $job_name"
    aws sagemaker stop-training-job --training-job-name "$job_name"
done

# X√≥a model versions trong Model Registry (gi·ªØ approved models)
aws sagemaker list-model-packages \
  --model-package-group-name "retail-forecast-models" \
  --model-approval-status PendingManualApproval \
  --query 'ModelPackageSummaryList[].ModelPackageArn' \
  --output text | tr '\t' '\n' | while read package_arn; do
    echo "Deleting pending model package: $package_arn"
    aws sagemaker delete-model-package --model-package-name "$package_arn"
done

# X√≥a endpoints t·ª´ failed deployments
aws sagemaker list-endpoints \
  --name-contains "retail-forecast-dev" \
  --query 'Endpoints[?EndpointStatus==`Failed`].EndpointName' \
  --output text | tr '\t' '\n' | while read endpoint; do
    echo "Deleting failed endpoint: $endpoint"
    aws sagemaker delete-endpoint --endpoint-name "$endpoint"
done
```

### 8.8 Verification

```bash
# Verify IAM resources ƒë√£ b·ªã x√≥a
aws iam get-role --role-name GitHubActionsRole 2>/dev/null || echo "GitHubActionsRole deleted"

# Verify SNS topics ƒë√£ b·ªã x√≥a
aws sns list-topics \
  --query 'Topics[?contains(TopicArn, `retail-forecast-alerts`)]' || echo "SNS topics cleaned"

# Verify CloudWatch resources
aws cloudwatch describe-dashboards \
  --dashboard-name-prefix "RetailForecast-CI-CD" || echo "Dashboards cleaned"

# Verify no running training jobs
aws sagemaker list-training-jobs \
  --status-equals InProgress \
  --name-contains "retail-forecast" \
  --query 'TrainingJobSummaries' || echo "No running training jobs"

# Check GitHub Actions status
gh run list --limit 5 --status completed
```

## 9. B·∫£ng gi√° CI/CD Pipeline (ap-southeast-1)

### 9.1. GitHub Actions Pricing

| Plan | Included Minutes | Price per minute | Storage |
|------|------------------|------------------|---------|
| **Free (Public repos)** | Unlimited | $0 | 500MB |
| **Free (Private repos)** | 2,000 min/month | $0.008 | 500MB |
| **Pro** | 3,000 min/month | $0.008 | 1GB |
| **Team** | 10,000 min/month | $0.008 | 2GB |
| **Enterprise** | 50,000 min/month | $0.008 | 50GB |

**Runner costs:**
- Ubuntu: Standard rate
- macOS: 10x Standard rate  
- Windows: 2x Standard rate
- Self-hosted: Free compute, infrastructure cost only

### 9.2. AWS IAM v√† Security Costs

| Service | Cost | Description |
|---------|------|-------------|
| **IAM Roles & Policies** | Free | Unlimited roles and policies |
| **STS AssumeRole calls** | $0.002/1000 calls | OIDC authentication |
| **AWS Config (compliance)** | $0.003/configuration item | Policy compliance tracking |

**Example calculation:**
- 100 CI/CD runs/month √ó 5 STS calls = 500 calls = $0.001/month

### 9.3. SageMaker Training Costs trong CI/CD

| Instance Type | Cost per Hour | Typical Job Duration | Cost per Run |
|---------------|---------------|---------------------|--------------|
| **ml.m5.large** | $0.134 | 15 minutes | $0.034 |
| **ml.m5.xlarge** | $0.269 | 10 minutes | $0.045 |
| **ml.c5.xlarge** | $0.238 | 8 minutes | $0.032 |
| **ml.p3.2xlarge** | $4.284 | 5 minutes | $0.357 |

**Monthly costs by frequency:**
- Daily training: 30 runs √ó $0.045 = $1.35
- Weekly training: 4 runs √ó $0.045 = $0.18  
- On-demand training: 2 runs √ó $0.045 = $0.09

### 9.4. ECR Storage v√† Transfer Costs

| Component | Cost | Volume | Monthly Cost |
|-----------|------|---------|--------------|
| **Storage** | $0.10/GB/month | 5GB images | $0.50 |
| **Data Transfer IN** | Free | Upload images | $0 |
| **Data Transfer OUT** | $0.12/GB | Download to EKS | Variable |

**Image management costs:**
```bash
# Example: 10 images √ó 500MB each = 5GB storage
# Monthly cost: 5GB √ó $0.10 = $0.50
# Transfer to EKS: 5GB √ó $0.12 = $0.60 (one-time per deployment)
```

### 9.5. CloudWatch Monitoring cho CI/CD

| Metric Type | Quantity | Unit Cost | Monthly Cost |
|-------------|----------|-----------|--------------|
| **Custom Metrics** | 20 metrics | $0.30/metric | $6.00 |
| **API Calls** | 100K calls | $0.01/1K calls | $1.00 |
| **Alarms** | 10 alarms | $0.10/alarm | $1.00 |
| **Dashboard** | 1 dashboard | $3.00/dashboard | $3.00 |
| **Total Monitoring** | | | **$11.00** |

### 9.6. SNS Notification Costs

| Notification Type | Volume | Cost per Message | Monthly Cost |
|-------------------|---------|------------------|--------------|
| **Email** | 200 notifications | $0.75/million | $0.0002 |
| **SMS** | 50 notifications | $0.8/message | $40.00 |
| **Slack Webhook** | 200 notifications | $0.75/million | $0.0002 |
| **Push Mobile** | 100 notifications | $0.75/million | $0.0001 |

### 9.7. Jenkins Infrastructure Costs (n·∫øu self-hosted)

| Component | Instance Type | Monthly Hours | Monthly Cost |
|-----------|---------------|---------------|--------------|
| **Jenkins Master** | t3.medium | 730 hours | $30.37 |
| **Build Agents** | t3.large (2 agents) | 100 hours | $13.25 |
| **EBS Storage** | 100GB gp3 | - | $8.00 |
| **Data Transfer** | 50GB/month | $0.12/GB | $6.00 |
| **Total Jenkins** | | | **$57.62** |

### 9.8. CI/CD Pipeline Scenarios

**Scenario 1: Small Team (GitHub Actions)**

| Component | Usage | Monthly Cost |
|-----------|-------|--------------|
| GitHub Actions (private) | 2,000 min included | $0 |
| SageMaker training | 4 runs/month | $0.18 |
| ECR storage | 2GB images | $0.20 |
| CloudWatch basic | 5 metrics, 3 alarms | $1.80 |
| SNS notifications | Email only | $0.0002 |
| **Total Small Team** | | **$2.18/month** |

**Scenario 2: Medium Team (GitHub Actions Pro)**

| Component | Usage | Monthly Cost |
|-----------|-------|--------------|
| GitHub Actions Pro | 3,000 min + 500 extra | $4.00 |
| SageMaker training | 12 runs/month | $0.54 |
| ECR storage | 8GB images | $0.80 |
| CloudWatch full | 15 metrics, 8 alarms | $5.30 |
| SNS notifications | Email + Slack | $0.0004 |
| **Total Medium Team** | | **$10.64/month** |

**Scenario 3: Enterprise (Self-hosted Jenkins)**

| Component | Usage | Monthly Cost |
|-----------|-------|--------------|
| Jenkins infrastructure | t3.medium + agents | $57.62 |
| SageMaker training | 60 runs/month | $2.70 |
| ECR storage | 20GB images | $2.00 |
| CloudWatch enterprise | 50 metrics, 25 alarms | $17.50 |
| SNS notifications | Multi-channel | $40.50 |
| **Total Enterprise** | | **$120.32/month** |

### 9.9. Cost Optimization Strategies

**GitHub Actions Optimization:**
```yaml
# Use matrix strategy ƒë·ªÉ gi·∫£m runtime
strategy:
  matrix:
    python-version: [3.8, 3.9, 3.10]
    
# Cache dependencies
- uses: actions/cache@v3
  with:
    path: ~/.cache/pip
    key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
    
# Conditional jobs
if: github.ref == 'refs/heads/main'
```

**SageMaker Training Optimization:**
```python
# S·ª≠ d·ª•ng Spot instances cho training
training_params = {
    'TrainingJobName': job_name,
    'ResourceConfig': {
        'InstanceType': 'ml.m5.large',
        'InstanceCount': 1,
        'VolumeSizeInGB': 30,
        'UseSpotInstances': True,  # 90% cost savings
        'MaxRuntimeInSeconds': 3600
    }
}
```

**ECR Cost Optimization:**
```bash
# Lifecycle policy ƒë·ªÉ t·ª± ƒë·ªông x√≥a old images
aws ecr put-lifecycle-policy \
  --repository-name mlops/retail-api \
  --lifecycle-policy-text '{
    "rules": [
      {
        "rulePriority": 1,
        "selection": {
          "tagStatus": "untagged",
          "countType": "sinceImagePushed",
          "countUnit": "days",
          "countNumber": 7
        },
        "action": {
          "type": "expire"
        }
      }
    ]
  }'
```

### 9.10. ROI Analysis cho CI/CD Investment

| Benefit | Manual Process | Automated CI/CD | Time Saved | Cost Benefit |
|---------|----------------|-----------------|------------|--------------|
| **Code Testing** | 2 hours/week | 5 minutes | 1.9 hours | $95/week |
| **Model Training** | 30 min setup | Automatic | 2 hours/month | $100/month |
| **Deployment** | 1 hour/deploy | 5 minutes | 55 min/deploy | $45/deploy |
| **Rollback** | 2 hours | 5 minutes | 1.9 hours | $95/incident |

**Annual ROI calculation:**
- **Investment:** $127.68/month √ó 12 = $1,532
- **Savings:** (2 hours/week √ó 52 weeks + 2 hours/month √ó 12) √ó $50/hour = $6,400
- **ROI:** 322% return on investment

### 9.11. Monitoring CI/CD Costs

```bash
# Track GitHub Actions usage
gh api /repos/:owner/:repo/actions/billing/usage

# Monitor SageMaker training costs
aws ce get-cost-and-usage \
  --time-period Start=2024-01-01,End=2024-01-31 \
  --granularity MONTHLY \
  --metrics BlendedCost \
  --group-by Type=DIMENSION,Key=SERVICE \
  --filter '{"Dimensions":{"Key":"SERVICE","Values":["Amazon SageMaker"]}}'

# ECR storage costs
aws ecr describe-registry-statistics --region ap-southeast-1

# CloudWatch costs
aws ce get-cost-and-usage \
  --time-period Start=2024-01-01,End=2024-01-31 \
  --granularity MONTHLY \
  --metrics BlendedCost \
  --group-by Type=DIMENSION,Key=SERVICE \
  --filter '{"Dimensions":{"Key":"SERVICE","Values":["Amazon CloudWatch"]}}'
```

{{% notice info %}}
**üí∞ Cost Summary cho Task 11:**
- **Small Team (GitHub Free):** $2.18/month
- **Medium Team (GitHub Pro):** $10.64/month  
- **Enterprise (Self-hosted):** $120.32/month
- **ROI:** 322% v·ªõi automation benefits
- **Break-even point:** ~3 months cho medium team setup
{{% /notice %}}

---

**Next Step**: [Task 12: Cost Optimization & Teardown](../12-cost-&-teardown/)
