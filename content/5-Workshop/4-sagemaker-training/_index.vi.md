---
title: "End-to-End Model Training Pipeline"
weight: 4
chapter: false
pre: "<b>4. </b>"
---

## M·ª•c ti√™u Task 4

Hu·∫•n luy·ªán m√¥ h√¨nh d·ª± b√°o **BASKET_PRICE_SENSITIVITY** (Low/Medium/High) b·∫±ng Amazon SageMaker v·ªõi pipeline t·ª± ƒë·ªông ETL ‚Üí Training ‚Üí Model Registry.

‚Üí **Tr√°i tim c·ªßa MLOps pipeline** - t·ª´ d·ªØ li·ªáu th√¥ ƒë·∫øn model production-ready.

**Input**

- AWS Account v·ªõi quy·ªÅn SageMaker/S3/CloudWatch
- S3 bucket v·ªõi d·ªØ li·ªáu (t·ª´ Task 3)
- IAM Role SageMaker (t·ª´ Task 2)

**K·∫øt qu·∫£**

- Model ƒë·∫°t accuracy ‚â• 80%, F1 ‚â• 0.7
- Model ƒë∆∞·ª£c ƒëƒÉng k√Ω trong Model Registry
- Artifacts l∆∞u tr·ªØ trong S3

**Chi ph√≠**: ~**$0.3-0.5/job** (ml.m5.large, 10-15 ph√∫t)

{{% notice info %}}
**üí° Task 4 - MLOps Core Pipeline:**

- **ETL t·ª± ƒë·ªông** - Raw data ‚Üí Features
- **Model training** - Random Forest classifier
- **Model evaluation** - Accuracy, F1, Confusion Matrix
- **Model Registry** - Version control v√† approval
  {{% /notice %}}

## 1. Chu·∫©n b·ªã m√¥i tr∆∞·ªùng v√† ki·ªÉm tra prerequisites

### 1.1. Ki·ªÉm tra S3 bucket (t·ª´ Task 3)

**AWS Console ‚Üí S3:**

1. T√¨m bucket: `mlops-retail-prediction-dev-[account-id]`
2. Ki·ªÉm tra c·∫•u tr√∫c th·ª±c t·∫ø:

   ```
   raw/           # transactions.csv + _select/ folder
   silver/        # shop_week partitions (200607-200619)
   gold/          # features ƒë√£ x·ª≠ l√Ω (s·∫Ω t·∫°o t·ª´ silver/)
   artifacts/     # model outputs (s·∫Ω t·∫°o)
   ```

![S3 bucket placeholder](/images/4-sagemake-training/01-s3-bucket.png)

### 1.2. X√°c minh IAM Role (t·ª´ Task 2)

**AWS Console ‚Üí IAM ‚Üí Roles:**

1. T√¨m role: `mlops-retail-prediction-dev-sagemaker-execution`
2. Ki·ªÉm tra permissions:
   - ‚úÖ `AmazonSageMakerFullAccess`
   - ‚úÖ `AmazonS3FullAccess`
   - ‚úÖ `CloudWatchLogsFullAccess`

![IAM role placeholder](/images/4-sagemake-training/02-iam-role.png)

{{% notice warning %}}
Warning: N·∫øu bucket c·ªßa b·∫°n d√πng SSE-KMS, role c·∫ßn c√≥ quy·ªÅn decrypt/encrypt v·ªõi KMS key; n·∫øu d√πng cross-account S3, ki·ªÉm tra th√™m trust policy.
{{% /notice %}}

## 2. T·∫°o SageMaker Domain v√† Project

### 2.1. Truy c·∫≠p SageMaker Unified Studio

**AWS Console ‚Üí SageMaker:**

1. Truy c·∫≠p URL: `https://[domain-id].studio.sagemaker.[region].amazonaws.com`
2. Ho·∫∑c t·ª´ SageMaker Console ‚Üí **Studio** ‚Üí **Open Studio**
3. Ch·ªçn authentication method:
   - **Sign in with SSO** (n·∫øu c√≥ setup SSO)
   - **Sign in with AWS IAM** (d√πng IAM user/role)

![SageMaker Unified Studio Login](/images/4-sagemake-training/04.1-domain.png)

4. Sau khi ƒëƒÉng nh·∫≠p, b·∫°n s·∫Ω th·∫•y giao di·ªán **SageMaker Unified Studio**
5. Dashboard hi·ªÉn th·ªã:
   - **"Good morning"** greeting
   - **Search bar**: "Search for data products, assets, and projects"
   - **Discover section**: Catalog, Generative AI playground, Shared generative AI assets
   - **Build section**: ML and generative AI model development, Generative AI app development
   - **Browse all projects** v√† **Create project** buttons

![SageMaker Unified Studio Dashboard](/images/4-sagemake-training/04.2-domain.png)

{{% notice info %}}
**üí° SageMaker Unified Studio:**

- **Unified interface** cho data analytics, ML, v√† generative AI
- **Project-based workspace** v·ªõi shared resources
- **Built-in collaboration** v·ªõi team members v√† approval workflows
- **Integrated tools**: Notebooks, Visual ETL, Workflows, Chat agents
  {{% /notice %}}

### 2.2. T·∫°o Project trong Unified Studio

**Trong SageMaker Unified Studio dashboard:**

**B∆∞·ªõc 1: Truy c·∫≠p Create Project**

1. Trong **Build** section, click **"Create project"** (n√∫t xanh)
2. Ho·∫∑c click **"Browse all projects"** ‚Üí **"Create project"**

![Project Name and Description](/images/4-sagemake-training/05.2.png)

**B∆∞·ªõc 2: ƒêi·ªÅn th√¥ng tin Project (Step 1)**

1. **Project name**: `retail-ml-training`
2. **Description**: `Retail price sensitivity model training`
3. Click **Next** ƒë·ªÉ chuy·ªÉn t·ªõi Step 2

![Project Name and Description](/images/4-sagemake-training/05.3.png)

**B∆∞·ªõc 2.5: Ch·ªçn Project Profile (Step 2)**

1. **Project profile**: Ch·ªçn **"All capabilities"** (highlighted in blue)
   - **Description**: "Analyze data and build machine learning and generative AI models and applications powered by Amazon Bedrock, Amazon EMR, AWS Glue, Amazon Athena, Amazon SageMaker AI and Amazon SageMaker Lakehouse"
   - **Tooling**: LakeHouse Database, Workflows
   - **+ 12 more** capabilities
2. C√°c options kh√°c: **Generative AI application development**, **SQL analytics**

![Project Profile Selection](/images/4-sagemake-training/05.4.png)

**B∆∞·ªõc 3: Blueprint Parameters**

- **S3 location**: `s3://amazon-sagemaker-[account-id]-ap-southeast-1-[random-id]`
- **Retention**: 731 days
- **Enable Project Repository Auto Sync**: false
- **Lakehouse Database**: `glue_db`

![Blueprint Parameters](/images/4-sagemake-training/05.5.png)

**B∆∞·ªõc 4: Create Project**

- Review c√°c settings v√† click **"Create project"**

![Project Creation Final](/images/4-sagemake-training/05.5.png)

- ƒê·ª£i 2-3 ph√∫t ƒë·ªÉ Project ƒë∆∞·ª£c provisioned

### 2.3. Truy c·∫≠p Project Workspace

**Sau khi Project `retail-ml-training` t·∫°o th√†nh c√¥ng:**

**B∆∞·ªõc 1: V√†o Project Overview**

1. Project s·∫Ω hi·ªÉn th·ªã trong danh s√°ch v·ªõi status **"Created"**
2. Click v√†o project name `retail-ml-training` ƒë·ªÉ v√†o **Project overview**
3. Project overview hi·ªÉn th·ªã:
   - **Project title**: `retail-ml-training`
   - **Description**: "Retail price sensitivity model training"
   - **Project files (6)**: `.ipynb_checkpoints`, `workflows`, `.libs.json`, `.temp_sagemaker_unified_studio_debugging_info`, `README.md`, `getting_started.ipynb`
   - **S3 path**: `/dzd-5kultpj28sm31d/cu2gr2js1w1wv` (project workspace path)
   - **Actions** v√† **New** dropdown buttons
     - **Project overview** (active)
     - **Data** - data assets v√† connections
     - **Compute** - compute resources v√† environments
     - **Members** - team collaboration
     - **Project catalog** (expandable)
     - **Assets**, **Subscription requests**, **Data sources**, **Metadata entities**

**B∆∞·ªõc 2: T·∫°o Notebook**

1. Click **"New"** dropdown (n√∫t xanh) ‚Üí ch·ªçn **"Notebook"**
2. **New** dropdown hi·ªÉn th·ªã c√°c options (theo th·ª© t·ª± trong h√¨nh):

![Project Overview](/images/4-sagemake-training/05.6.png)

![New Notebook Creation](/images/4-sagemake-training/06.1.png)

- **Notebook** (ch·ªçn option n√†y)

**B∆∞·ªõc 3: Project Welcome**
Trong project overview, b·∫°n c≈©ng th·∫•y **Readme** section hi·ªÉn th·ªã **"Welcome"** v·ªõi h∆∞·ªõng d·∫´n b·∫Øt ƒë·∫ßu s·ª≠ d·ª•ng project.

### 2.4. X√°c minh EC2 Permissions (ƒë√£ c√≥ t·ª´ Task 2)

**EC2 permissions ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh ƒë·∫ßy ƒë·ªß trong Task 2**, bao g·ªìm inline policy `SageMakerEC2Access` trong role `mlops-retail-prediction-dev-sagemaker-execution`.

**X√°c minh EC2 permissions ƒë√£ c√≥:**

```powershell
# Ki·ªÉm tra inline policy ƒë√£ ƒë∆∞·ª£c th√™m t·ª´ Task 2
aws iam get-role-policy --role-name mlops-retail-prediction-dev-sagemaker-execution --policy-name SageMakerEC2Access

# Test quy·ªÅn EC2 describe
aws ec2 describe-vpcs --region us-east-1
```

**Role ƒë√£ c√≥ ƒë·ªß 4 policies t·ª´ Task 2:**

- ‚úÖ `AmazonSageMakerFullAccess` (AWS managed)
- ‚úÖ `AmazonS3FullAccess` (AWS managed)
- ‚úÖ `CloudWatchLogsFullAccess` (AWS managed)
- ‚úÖ `SageMakerEC2Access` (inline policy cho Project creation)

{{% notice success %}}
**Project creation ready:** Role ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh ƒë·∫ßy ƒë·ªß t·ª´ Task 2, c√≥ th·ªÉ t·∫°o Project ngay l·∫≠p t·ª©c.
{{% /notice %}}

### 2.5. Khuy·∫øn ngh·ªã Region cho Task 4

**T√≥m t·∫Øt:** N·∫øu d·ªØ li·ªáu `gold/` v√† `artifacts/` hi·ªán ƒëang n·∫±m trong bucket `mlops-retail-prediction-dev-842676018087` (region `us-east-1`), khuy·∫øn ngh·ªã l√† **t·∫°o SageMaker Domain / Project ·ªü c√πng `us-east-1`** ƒë·ªÉ tr√°nh l·ªói cross-region (S3 301), ph·ª©c t·∫°p v·ªõi KMS v√† endpoint.

- **L·ª£i √≠ch khi t·∫°o Project ·ªü `us-east-1`:**

  - Lo·∫°i b·ªè l·ªói 'bucket must be addressed using the specified endpoint' khi SageMaker t·∫£i d·ªØ li·ªáu t·ª´ S3.
  - Kh√¥ng c·∫ßn duy tr√¨ KMS keys ho·∫∑c IAM policies ·ªü nhi·ªÅu region.
  - √çt r·ªßi ro khi ch·∫°y training jobs t·ª´ Studio/Project.

- **Khi c·∫ßn t·∫°o Project ·ªü `ap-southeast-1` (ho·∫∑c region kh√°c):**
  - Ph·∫£i **chuy·ªÉn** ho·∫∑c **sao ch√©p** d·ªØ li·ªáu `gold/` v√† `artifacts/` sang bucket ·ªü region ƒë√≥ ho·∫∑c c·∫•u h√¨nh Cross-Region Replication (CRR).
  - T·∫°o KMS keys t∆∞∆°ng ·ª©ng v√† c·∫≠p nh·∫≠t policies/roles cho bucket m·ªõi.

---

N·∫øu b·∫°n mu·ªën gi·ªØ Project trong `ap-southeast-1`, ƒë√¢y l√† v√≠ d·ª• l·ªánh ƒë·ªÉ t·∫°o bucket v√† sao ch√©p d·ªØ li·ªáu (PowerShell / CloudShell):

```powershell
# T·∫°o bucket m·ªõi ·ªü ap-southeast-1
aws s3 mb s3://mlops-retail-prediction-dev-842676018087-apse1 --region ap-southeast-1

# ƒê·ªìng b·ªô gold/ v√† artifacts/ sang bucket m·ªõi
aws s3 sync s3://mlops-retail-prediction-dev-842676018087/gold/ s3://mlops-retail-prediction-dev-842676018087-apse1/gold/ --acl bucket-owner-full-control --exact-timestamps
aws s3 sync s3://mlops-retail-prediction-dev-842676018087/artifacts/ s3://mlops-retail-prediction-dev-842676018087-apse1/artifacts/ --acl bucket-owner-full-control --exact-timestamps

# (Optional) T·∫°o KMS key ·ªü ap-southeast-1 v√† c·∫≠p nh·∫≠t bucket policy / IAM role
# aws kms create-key --region ap-southeast-1 --description "KMS for mlops retail ap-southeast-1"
```

Ghi ch√∫:

- N·∫øu bucket ngu·ªìn d√πng SSE-KMS, ƒë·∫£m b·∫£o b·∫°n t·∫°o t∆∞∆°ng ·ª©ng KMS key ·ªü region ƒë√≠ch v√† c·∫≠p nh·∫≠t c·∫£ bucket policy v√† role `mlops-retail-prediction-dev-sagemaker-execution`.
- N·∫øu mu·ªën resolution nhanh v√† √≠t thay ƒë·ªïi IAM, ch·ªçn t·∫°o Project/Domain ·ªü `us-east-1` (n∆°i bucket hi·ªán c√≥) ‚Äî ƒë√¢y l√† ph∆∞∆°ng √°n khuy·∫øn ngh·ªã cho lab v√† ch·∫°y training nhanh.

### 3. ETL - Chu·∫©n b·ªã d·ªØ li·ªáu trong SageMaker Studio

**üéØ M·ª•c ti√™u:** ƒê·ªçc T·∫§T C·∫¢ shop_week partitions t·ª´ S3 silver/ v√† t·∫°o train/test/validation splits

**Input:** `silver/shop_week=200607/` ƒë·∫øn `silver/shop_week=200619/` (14 partitions)  
**Output:** `gold/train.parquet`, `gold/test.parquet`, `gold/validation.parquet`

#### **B∆∞·ªõc 1: T·∫°o ETL Notebook trong Project**

**T·ª´ Project overview:**

1. Click **"New"** dropdown ‚Üí ch·ªçn **"Notebook"**
2. Notebook s·∫Ω m·ªü trong browser tab m·ªõi
3. Ch·ªçn kernel: **Python 3 (Data Science 3.0)**
4. ƒê·∫∑t t√™n notebook: **File** ‚Üí **Rename** ‚Üí `retail-etl-pipeline.ipynb`
5. Notebook s·∫Ω t·ª± ƒë·ªông l∆∞u v√†o S3 project path

{{% notice info %}}
**L∆∞u √Ω:**

- Notebook ch·∫°y tr√™n managed compute instance c·ªßa SageMaker
- Files t·ª± ƒë·ªông sync v·ªõi S3 project storage
- C√≥ th·ªÉ chia s·∫ª v·ªõi team members trong project
  {{% /notice %}}

#### **B∆∞·ªõc 2: Th·ª±c hi·ªán ETL Pipeline**

T·∫°o v√† ch·∫°y c√°c cells sau theo th·ª© t·ª±:

**Cell 1: C√†i ƒë·∫∑t dependencies**

```bash
# Install all required packages
pip install pandas pyarrow s3fs scikit-learn xgboost sagemaker boto3 joblib
```

**Cell 2: Thi·∫øt l·∫≠p c·∫•u h√¨nh**

```python
import boto3
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import json
from datetime import datetime

# Configuration - update bucket name theo project c·ªßa b·∫°n
# N·∫øu d√πng project S3: amazon-sagemaker-[account-id]-ap-southeast-1-[random-id]
# Ho·∫∑c bucket t·ª´ Task 3: mlops-retail-prediction-dev-[account-id]
bucket_name = 'amazon-sagemaker-842676018087-ap-southeast-1-f6cd5056a835'  # <-- S·ª¨A theo project c·ªßa b·∫°n
raw_prefix = 'silver/'
gold_prefix = 'gold/'

# Initialize AWS clients
s3 = boto3.client('s3')
print(f'‚úÖ AWS clients initialized. Bucket: {bucket_name}')
```

**Cell 3: Load d·ªØ li·ªáu t·ª´ t·∫•t c·∫£ partitions**

```python
print(f'üìä Loading all partitioned data from s3://{bucket_name}/{raw_prefix}...')

# Discover all parquet files in silver/
response = s3.list_objects_v2(Bucket=bucket_name, Prefix=raw_prefix)
parquet_files = [obj['Key'] for obj in response.get('Contents', [])
                if obj['Key'].endswith('.parquet') and obj['Size'] > 0]

print(f'üìÅ Found {len(parquet_files)} parquet files')

# Load all files into one DataFrame
all_dataframes = []
total_rows = 0

for i, key in enumerate(parquet_files[:10]):  # Limit to first 10 files for demo
    s3_path = f's3://{bucket_name}/{key}'

    try:
        df = pd.read_parquet(s3_path)
        all_dataframes.append(df)
        total_rows += len(df)
        print(f'  ‚úÖ File {i+1}: {len(df):,} rows from {key.split("/")[-1]}')
    except Exception as e:
        print(f'  ‚ùå Failed to load {key}: {e}')

# Combine all data
combined_data = pd.concat(all_dataframes, ignore_index=True)
print(f'\nüéØ Combined dataset: {combined_data.shape}')
print(f'üìã Columns: {list(combined_data.columns)}')
```

**Cell 4: T·∫°o features v√† target variable**

```python
print("üìå STEP 1 ‚Äî Columns in combined_data:")
print(list(combined_data.columns))

# Force lowercase column names for safety
combined_data.columns = [c.lower() for c in combined_data.columns]
print("\nüìå STEP 2 ‚Äî Columns after lowercase normalization:")
print(list(combined_data.columns))

print("\nüìå STEP 3 ‚Äî Checking required columns...")

if 'basket_id' in combined_data.columns and 'spend' in combined_data.columns:
    print("‚úÖ Found basket_id and spend ‚Äî proceeding with basket-level feature engineering.")

    print("\nüìå STEP 4 ‚Äî Converting numeric columns...")
    for col in ['spend', 'quantity']:
        if col in combined_data.columns:
            combined_data[col] = pd.to_numeric(combined_data[col], errors='coerce')
            print(f"  - Converted column '{col}' to numeric.")

    print("\nüìå STEP 5 ‚Äî Starting groupby aggregation...")
    print("‚öôÔ∏è Aggregating, this may take a moment...")

    features = combined_data.groupby('basket_id').agg({
        'spend': ['sum', 'mean', 'count'],
        'quantity': ['sum', 'mean'] if 'quantity' in combined_data.columns else []
    }).reset_index()

    print("‚úÖ Aggregation complete.")
    print(f"üìä Features raw shape: {features.shape}")

    # Flatten column names
    print("\nüìå STEP 6 ‚Äî Flattening column names...")
    features.columns = (
        ['basket_id', 'spend_sum', 'spend_mean', 'spend_count'] +
        (['quantity_sum', 'quantity_mean'] if 'quantity' in combined_data.columns else [])
    )
    print("üìã New feature columns:", list(features.columns))

    print("\nüìå STEP 7 ‚Äî Creating price_sensitivity target variable...")
    median_spend = features['spend_sum'].median()
    print(f"Median spend = {median_spend}")

    features['price_sensitivity'] = (features['spend_sum'] > median_spend).astype(int)

else:
    print("‚ùå Could NOT find required columns for basket-level engineering.")
    print("Available columns:", list(combined_data.columns))

    print("\nüìå Fallback: Using transaction-level feature engineering...")
    features = combined_data.copy()

    if 'spend' in features.columns:
        features['price_sensitivity'] = (
            features['spend'] > features['spend'].median()
        ).astype(int)
        print("‚úÖ Created price_sensitivity for transaction-level data.")
    else:
        raise RuntimeError("‚ùå spend column not found ‚Äî cannot create price_sensitivity.")

print("\nüìå STEP 8 ‚Äî Final feature table shape:")
print(features.shape)

print("\nüìå STEP 9 ‚Äî Target distribution:")
print(features['price_sensitivity'].value_counts(dropna=False))

print("\nüìå STEP 10 ‚Äî Sample output:")
print(features.head())
```

**Cell 5: T·∫°o train/test/validation splits v√† l∆∞u v√†o S3**

```python
print('üìã Creating train/test/validation splits...')

# Create stratified splits
train_data, temp_data = train_test_split(
    features, test_size=0.3, random_state=42,
    stratify=features['price_sensitivity']
)

test_data, val_data = train_test_split(
    temp_data, test_size=0.33, random_state=42,
    stratify=temp_data['price_sensitivity']
)

splits = {
    'train': train_data,
    'test': test_data,
    'validation': val_data
}

# Save to S3
print(f'üíæ Saving splits to s3://{bucket_name}/{gold_prefix}...')

for split_name, split_data in splits.items():
    s3_path = f's3://{bucket_name}/{gold_prefix}{split_name}.parquet'
    split_data.to_parquet(s3_path, index=False)
    print(f'  ‚úÖ {split_name}: {len(split_data):,} rows saved to {split_name}.parquet')

print('\nüéâ ETL Complete! Data ready for training.')

# Verify files created
response = s3.list_objects_v2(Bucket=bucket_name, Prefix=gold_prefix)
if 'Contents' in response:
    print(f'\nüìÅ Files in gold/:')
    for obj in response['Contents']:
        size_mb = obj['Size'] / (1024*1024)
        print(f'  üìÑ {obj["Key"]}: {size_mb:.2f} MB')
```

## 4. Training - Hu·∫•n luy·ªán Model

**üéØ M·ª•c ti√™u:** Hu·∫•n luy·ªán m√¥ h√¨nh Random Forest ƒë·ªÉ ph√¢n lo·∫°i ƒë·ªô nh·∫°y gi√° kh√°ch h√†ng

**Input:** S3 `gold/train.parquet`, `gold/test.parquet`, `gold/validation.parquet`  
**Output:** Trained Random Forest model trong S3 artifacts/ v·ªõi performance metrics

#### **B∆∞·ªõc 1: T·∫°o Training Notebook trong Project**

1. Trong Studio interface, click **File** ‚Üí **New** ‚Üí **Notebook**
2. Ch·ªçn **conda_python3** kernel (ho·∫∑c **Python 3 (Data Science)**)
3. ƒê·∫∑t t√™n notebook: `notebooks/retail-model-training.ipynb`
4. Click **Create**

![Create notebook](/images/4-sagemake-training/05.7.png)

**üí° L∆∞u √Ω:** Notebook s·∫Ω ƒë∆∞·ª£c l∆∞u trong Project repository v√† c√≥ th·ªÉ commit v√†o CodeCommit.

#### **B∆∞·ªõc 2: Th·ª±c hi·ªán Model Training**

T·∫°o v√† ch·∫°y c√°c cells sau theo th·ª© t·ª±:

**Cell 1: Setup SageMaker Configuration**

```python
import sagemaker
import boto3
import os
from sagemaker.sklearn.estimator import SKLearn

# Initialize SageMaker session and get execution role
sagemaker_session = sagemaker.Session()
role = sagemaker.get_execution_role()

# Configuration - update bucket name if different
bucket_name = 'mlops-retail-prediction-dev-842676018087'
gold_prefix = 'gold/'
artifacts_prefix = 'artifacts/'

print(f'‚úÖ SageMaker Role: {role}')
print(f'üìä Training Data: s3://{bucket_name}/{gold_prefix}')
print(f'üì¶ Model Artifacts: s3://{bucket_name}/{artifacts_prefix}')
```

**Cell 2: T·∫°o Training Script**

```python
%%writefile train_retail_model.py
import pandas as pd
import joblib
import os
import json
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report

def main():
    # ƒê∆∞·ªùng d·∫´n chu·∫©n c·ªßa SageMaker
    train_dir = os.environ.get("SM_CHANNEL_TRAIN", "/opt/ml/input/data/train")
    model_dir = os.environ.get("SM_MODEL_DIR", "/opt/ml/model")

    # 1. Load data
    train_path = os.path.join(train_dir, "train.parquet")
    print(f"üìñ Loading training data from: {train_path}")
    df = pd.read_parquet(train_path)

    print(f"üìä Dataset shape: {df.shape}")
    print(f"üìã Columns: {list(df.columns)}")

    # 2. Chu·∫©n b·ªã features & target
    target_col = "price_sensitivity" if "price_sensitivity" in df.columns else df.columns[-1]
    feature_cols = [c for c in df.columns if c not in [target_col, "basket_id"]]

    X = df[feature_cols]
    y = df[target_col]

    print(f"üî¢ Features: {len(feature_cols)} columns")
    print(f"üéØ Target: {target_col}")
    print(f"üìà Target distribution: {dict(y.value_counts())}")

    # 3. Train/validation split (stratified)
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # 4. Train Random Forest model
    print("\nüå≤ Training Random Forest model...")
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42
    )

    model.fit(X_train, y_train)

    # 5. Evaluate model
    y_pred = model.predict(X_val)

    accuracy = accuracy_score(y_val, y_pred)
    f1 = f1_score(y_val, y_pred, average="macro")
    precision = precision_score(y_val, y_pred, average="macro")
    recall = recall_score(y_val, y_pred, average="macro")

    print(f"\nüìä Model Performance:")
    print(f"  ‚úÖ Accuracy:  {accuracy:.4f}")
    print(f"  ‚úÖ Precision: {precision:.4f}")
    print(f"  ‚úÖ Recall:    {recall:.4f}")
    print(f"  ‚úÖ F1-Score:  {f1:.4f}")

    # 6. Save model and results
    os.makedirs(model_dir, exist_ok=True)

    # Save Random Forest model
    model_path = os.path.join(model_dir, 'model.joblib')
    joblib.dump(model, model_path)
    print(f'üå≤ Random Forest model saved: {model_path}')

    # Save training results
    results_path = os.path.join(model_dir, 'training_results.json')
    training_summary = {
        'model_name': 'RandomForest',
        'accuracy': float(accuracy),
        'precision': float(precision),
        'recall': float(recall),
        'f1_score': float(f1),
        'feature_count': len(feature_cols),
        'training_samples': len(X_train),
        'validation_samples': len(X_val),
        'feature_names': feature_cols,
        'classification_report': classification_report(y_val, y_pred, output_dict=True)

    with open(results_path, 'w') as f:
        json.dump(training_summary, f, indent=2)

    print(f'üìã Results saved: {results_path}')
    print(f'üì¶ Model artifacts: 1 model + 1 results file')

    # Validation checks
    target_accuracy = 0.80
    target_f1 = 0.70

    print(f'\nüéØ Performance Validation:')
    print(f'  üìä Accuracy ‚â• {target_accuracy}: {"‚úÖ" if accuracy >= target_accuracy else "‚ùå"} ({accuracy:.3f})')
    print(f'  üìä F1-Score ‚â• {target_f1}: {"‚úÖ" if f1 >= target_f1 else "‚ùå"} ({f1:.3f})')

if __name__ == '__main__':
    main()
'''

# Write training script to file
with open('train_retail_model.py', 'w') as f:
    f.write(train_script)

```

**Cell 3: Submit SageMaker Training Job**

```python
print("üöÄ Submitting SageMaker Training Job...")

# Pre-flight: ki·ªÉm tra region + data trong gold/
s3_client = boto3.client("s3")

bucket_location = s3_client.get_bucket_location(Bucket=bucket_name)
bucket_region = bucket_location["LocationConstraint"] or "us-east-1"
current_region = sagemaker_session.boto_region_name

print(f"üìç Bucket region:    {bucket_region}")
print(f"üìç SageMaker region: {current_region}")

# Ki·ªÉm tra cross-region issue
if bucket_region != current_region:
    print(f"‚ö†Ô∏è Region mismatch detected!")
    print(f"   Bucket: {bucket_name} in {bucket_region}")
    print(f"   SageMaker: {current_region}")
    print(f"   This may cause S3 301 redirect errors during training")
    print(f"   Consider using project bucket in same region or configure cross-region access")

    # Option 1: Use project bucket in same region
    print(f"\nüí° Option 1: Use project bucket (same region):")
    print(f"   bucket_name = '{project_bucket}'")
    print(f"   (But need to copy gold/ data to this bucket first)")

    # Option 2: Continue with cross-region
    print(f"\nüí° Option 2: Continue with cross-region (may need additional config)")

    # For demo, we'll continue but warn user
    import warnings
    warnings.warn(f"Cross-region S3 access: {bucket_region} -> {current_region}")
else:
    print(f"‚úÖ Same region - no cross-region issues")

train_s3_uri = f"s3://{bucket_name}/{gold_prefix}"

resp = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=gold_prefix)
data_files = [o["Key"] for o in resp.get("Contents", []) if o["Key"].endswith(".parquet")]

if not data_files:
    raise ValueError(f"‚ùå No .parquet files found in {train_s3_uri}. Run ETL tr∆∞·ªõc ƒë√£.")

print(f"‚úÖ Found {len(data_files)} training files in {train_s3_uri}")

# C·∫•u h√¨nh estimator
estimator = SKLearn(
    entry_point="train_retail_model.py",
    role=role,
    instance_type="ml.m5.xlarge",
    instance_count=1,
    framework_version="1.0-1",
    py_version="py3",
    base_job_name="retail-prediction-training",
    sagemaker_session=sagemaker_session,
)

print(f"üìä Training data location: {train_s3_uri}")

# Ch·∫°y training job
estimator.fit({"train": train_s3_uri}, wait=True)

job_name = estimator.latest_training_job.name
model_artifacts = estimator.model_data

print("\nüéâ Training job completed!")
print("üìù Job name:       ", job_name)
print("üíæ Model artifacts:", model_artifacts)

except Exception as e:
    print(f'‚ùå Pre-flight check failed: {e}')
    print('üí° Solutions:')
    print('  1. Run ETL notebook first to create gold/ data')
    print('  2. Make sure you ran debug cell (B∆∞·ªõc 4) to fix regions')
    print('  3. Check IAM role has S3 access')
    raise

# Configure estimator for Unified Studio project
estimator = SKLearn(
    entry_point='train_retail_model.py',
    role=role,
    instance_type='ml.m5.xlarge',  # Larger instance for better performance
    instance_count=1,
    framework_version='1.0-1',
    py_version='py3',
    base_job_name='retail-prediction-training',
    sagemaker_session=sagemaker_session,
    # Output path s·∫Ω ƒë∆∞·ª£c set t·ª± ƒë·ªông v√†o project S3 location
    output_path=f's3://{bucket_name}/{artifacts_prefix}'
)

print(f'üìä Training data location: {train_s3_uri}')

# Start training job with error handling
try:
    print('‚è≥ Starting training job (this will take 5-10 minutes)...')
    estimator.fit({'train': train_s3_uri}, wait=True)

    # Get job results
    job_name = estimator.latest_training_job.name
    model_artifacts = estimator.model_data

    print(f'\\nüéâ Training job completed!')
    print(f'üìù Job name: {job_name}')
    print(f'üíæ Model artifacts: {model_artifacts}')

except Exception as e:
    print(f'‚ùå Training job failed: {e}')
    print('üí° Troubleshooting:')
    print('  - Check CloudWatch logs for detailed error')
    print('  - Verify IAM role permissions')
    print('  - Ensure training data format is correct')
    raise
```

**Cell 4: Download & ƒê·ªçc Training Results**

```python
import os
import tarfile
import json
import boto3

print("üìä Analyzing training results...")

# model_artifacts l·∫•y t·ª´ Cell 3 (estimator.model_data)
print("üì¶ Artifact path:", model_artifacts)

# T√°ch bucket + key
art_parts = model_artifacts.replace("s3://", "").split("/", 1)
art_bucket = art_parts[0]
art_key = art_parts[1]

s3 = boto3.client("s3")

# T·∫£i file model.tar.gz v·ªÅ local
local_tar = "model.tar.gz"
s3.download_file(art_bucket, art_key, local_tar)
print("üì• Downloaded", local_tar)

# Gi·∫£i n√©n v√†o th∆∞ m·ª•c model_artifacts/
extract_dir = "model_artifacts"
os.makedirs(extract_dir, exist_ok=True)

with tarfile.open(local_tar, "r:gz") as tar:
    tar.extractall(extract_dir)

print("\nüìÇ Files inside model:")
print(os.listdir(extract_dir))

# ƒê·ªçc training_results.json
results_path = os.path.join(extract_dir, "training_results.json")
with open(results_path, "r") as f:
    results = json.load(f)

print("\nüå≤ RANDOM FOREST TRAINING RESULTS:")
print(f"  ü§ñ Model:      {results['model_name']}")
print(f"  üìä Accuracy:   {results['accuracy']:.4f}")
print(f"  üìä Precision:  {results['precision']:.4f}")
print(f"  üìä Recall:     {results['recall']:.4f}")
print(f"  üìä F1-Score:   {results['f1_score']:.4f}")
print(f"  üî¢ Features:   {results['feature_count']}")
print(f"  üìö Train rows: {results['training_samples']:,}")
print(f"  üß™ Val rows:   {results['validation_samples']:,}")

print("\nüìã Per-class Performance:")
class_report = results['classification_report']
for class_name, metrics in class_report.items():
    if isinstance(metrics, dict) and 'f1-score' in metrics:
        print(f"  {class_name:>12}: Precision={metrics['precision']:.3f}, Recall={metrics['recall']:.3f}, F1={metrics['f1-score']:.3f}")

# Validate target
acc_target = 0.80
f1_target = 0.70

print("\nüéØ Performance validation:")
print(f"  üìä Accuracy ‚â• {acc_target}: {'‚úÖ' if results['accuracy'] >= acc_target else '‚ùå'} ({results['accuracy']:.3f})")
print(f"  üìä F1-score ‚â• {f1_target}: {'‚úÖ' if results['f1_score'] >= f1_target else '‚ùå'} ({results['f1_score']:.3f})")
```

**K·∫øt qu·∫£**
![K·∫øt qu·∫£ training](/images/4-sagemake-training/00.png)

{{% notice success %}}
‚úÖ **Training Ho√†n th√†nh!** Model ƒë·∫°t target performance v√† s·∫µn s√†ng cho Model Registry.
{{% /notice %}}

## 5. Monitoring & Results

### 5.1. Theo d√µi Training Job trong Studio

Trong **SageMaker Studio (Unified Studio)**:

1. M·ªü m·ª•c **Build** ·ªü thanh b√™n tr√°i

2. Ch·ªçn **Training jobs**

![Training logs example](/images/4-sagemake-training/08.1.png)

3. T√¨m job c√≥ t√™n b·∫Øt ƒë·∫ßu b·∫±ng: retail-prediction-training-

4. Nh·∫•p v√†o **t√™n Training Job** ƒë·ªÉ m·ªü chi ti·∫øt

![Training logs example](/images/4-sagemake-training/08.2.png)

5. Ch·ªçn tab **Logs** ƒë·ªÉ xem log real-time

6. (T√πy ch·ªçn) B·∫•m **‚ÄúOpen in CloudWatch‚Äù** ƒë·ªÉ xem log ƒë·∫ßy ƒë·ªß

![Training logs example](/images/4-sagemake-training/08.3.png)

![Training logs example](/images/4-sagemake-training/08.4.png)

{{% notice info %}}
**Info:**  
CloudWatch logs cho Training Job ƒë∆∞·ª£c l∆∞u d∆∞·ªõi d·∫°ng: /aws/sagemaker/TrainingJobs/<job-name> N·∫øu job b·ªã l·ªói, l·ªói Python s·∫Ω n·∫±m ·ªü cu·ªëi log.
{{% /notice %}}

## 6. Model Registry (Giao di·ªán m·ªõi trong Project)

Sau khi training job ho√†n th√†nh v√† t·∫°o ra `model.tar.gz`, b∆∞·ªõc ti·∫øp theo l√† ƒëƒÉng k√Ω m√¥ h√¨nh trong **Model Registry**.  
·ªû giao di·ªán SageMaker Unified Studio m·ªõi, Model Registry n·∫±m **b√™n trong t·ª´ng Project**, kh√¥ng c√≤n t√°ch ri√™ng nh∆∞ tr∆∞·ªõc.

---

### 6.1. M·ªü Model Registry trong Project

**SageMaker Studio ‚Üí Projects ‚Üí mlops-retail-prediction ‚Üí Models ‚Üí Registered models**

![Model registry](/images/4-sagemake-training/09.1.png)

![Model registry](/images/4-sagemake-training/09.2.png)

### 6.2. T·∫°o Model Group

1. B·∫•m **Register model group**
2. ƒêi·ªÅn:
   - **Name**: `retail-price-sensitivity-models`
   - **Description**: `Model group for retail price sensitivity prediction`
3. Nh·∫•n **Register model group**

![Model registry](/images/4-sagemake-training/09.3.png)

Model Group s·∫Ω xu·∫•t hi·ªán trong danh s√°ch.

![Model registry](/images/4-sagemake-training/09.4.png)

---

### 6.3. ƒêƒÉng k√Ω Model Version sau khi Training

![Model registry](/images/4-sagemake-training/09.5.png)

![Model registry](/images/4-sagemake-training/09.6.png)

1. V√†o **Models ‚Üí Registered models versions ‚Üí Model groups**

![Model registry](/images/4-sagemake-training/09.7.png)

2. Ch·ªçn nh√≥m: **retail-price-sensitivity-models**
3. Nh·∫•n **Register model**
4. Nh·∫≠p th√¥ng tin:
   - **Model artifact location (S3)**  
     V√≠ d·ª•:
     ```
     s3://amazon-sagemaker-842676018087-us-east-1-xxxx/output/model.tar.gz
     ```
   - **Version description**: `Retail prediction model v1.0`
   - **Approval status**: `Pending manual approval`
5. Nh·∫•n **Register**

![Model registry](/images/4-sagemake-training/09.8.png)

M·ªôt _Model Version_ m·ªõi s·∫Ω ƒë∆∞·ª£c t·∫°o.

![Model registry](/images/4-sagemake-training/09.9.png)

![Model registry](/images/4-sagemake-training/09.10.png)

---

### 6.4. Approve Model Version

1. M·ªü **Model group ‚Üí retail-price-sensitivity-models**
2. Ch·ªçn **Version 1**
3. Nh·∫•n **Update status**
4. ƒê·∫∑t:
   - **Approved**
5. Save

![Model registry](/images/4-sagemake-training/09.11.png)

### Ho√†n th√†nh Task 4

**üìÅ Notebook th·ª±c thi:** `notebooks/sagemaker-retail-etl-training.ipynb`

**ƒê√£ th√†nh c√¥ng:**

- ‚úÖ **T·∫°o SageMaker Domain** v√† c·∫•u h√¨nh
- ‚úÖ **T·∫°o Project** v√† m·ªü Studio workspace
- ‚úÖ **ETL to√†n b·ªô dataset** - All shop_week partitions ‚Üí Gold Parquet
- ‚úÖ **Auto-detect partitions** - Scan t·∫•t c·∫£ shop_week c√≥ s·∫µn
- ‚úÖ **Train Random Forest** v·ªõi optimal hyperparameters
- ‚úÖ **Ch·ªçn single model** focus ƒë·ªÉ t·ªëi ∆∞u performance
- ‚úÖ **Spot instances** - Cost optimization v·ªõi auto-scaling
- ‚úÖ **Complete notebook** - 4 cells v·ªõi detailed logging

## 7. Clean Up Resources (AWS CLI)

### 7.1. X√≥a SageMaker Training Jobs

```bash
# Li·ªát k√™ training jobs
aws sagemaker list-training-jobs --name-contains "retail-prediction-training" --query 'TrainingJobSummaries[*].[TrainingJobName,TrainingJobStatus]' --output table

# D·ª´ng training job ƒëang ch·∫°y (n·∫øu c√≥)
aws sagemaker stop-training-job --training-job-name <job-name>

# Training jobs t·ª± ƒë·ªông cleanup sau khi ho√†n th√†nh (kh√¥ng c·∫ßn x√≥a manual)
```

### 7.2. X√≥a Model Registry

```bash
# Li·ªát k√™ model packages
aws sagemaker list-model-packages --model-package-group-name retail-price-sensitivity-models --query 'ModelPackageSummaryList[*].[ModelPackageArn,ModelPackageStatus]' --output table

# X√≥a t·ª´ng model package version
aws sagemaker delete-model-package --model-package-name <model-package-arn>

# X√≥a model package group
aws sagemaker delete-model-package-group --model-package-group-name retail-price-sensitivity-models
```

### 7.3. X√≥a SageMaker Domain v√† Project

```bash
# Li·ªát k√™ domains
aws sagemaker list-domains --query 'Domains[*].[DomainId,DomainName,Status]' --output table

# X√≥a user profiles tr∆∞·ªõc
aws sagemaker list-user-profiles --domain-id <domain-id> --query 'UserProfiles[*].UserProfileName' --output text

# X√≥a t·ª´ng user profile
aws sagemaker delete-user-profile --domain-id <domain-id> --user-profile-name <user-profile-name>

# X√≥a domain (sau khi x√≥a h·∫øt user profiles)
aws sagemaker delete-domain --domain-id <domain-id>
```

### 7.4. Clean Up S3 Artifacts

```bash
# X√≥a model artifacts
aws s3 rm s3://amazon-sagemaker-<account-id>-<region>-<random-id>/artifacts/ --recursive

# X√≥a gold datasets
aws s3 rm s3://amazon-sagemaker-<account-id>-<region>-<random-id>/gold/ --recursive

# Ki·ªÉm tra project bucket c√≤n g√¨
aws s3 ls s3://amazon-sagemaker-<account-id>-<region>-<random-id>/ --recursive
```

### 7.5. X√≥a CloudWatch Logs

```bash
# Li·ªát k√™ log groups c·ªßa SageMaker
aws logs describe-log-groups --log-group-name-prefix "/aws/sagemaker/TrainingJobs" --query 'logGroups[*].logGroupName'

# X√≥a training job logs
aws logs delete-log-group --log-group-name "/aws/sagemaker/TrainingJobs/retail-prediction-training-<timestamp>"
```

---

## 8. B·∫£ng gi√° SageMaker

### 8.1. Chi ph√≠ Training Instances

| Instance Type     | vCPU | RAM   | Gi√° (USD/hour) | Ph√π h·ª£p cho                 |
| ----------------- | ---- | ----- | -------------- | --------------------------- |
| **ml.m5.large**   | 2    | 8 GB  | $0.138         | Small datasets, prototyping |
| **ml.m5.xlarge**  | 4    | 16 GB | $0.276         | Medium datasets (ƒë√£ d√πng)   |
| **ml.m5.2xlarge** | 8    | 32 GB | $0.552         | Large datasets              |
| **ml.c5.xlarge**  | 4    | 8 GB  | $0.238         | CPU-intensive training      |
| **ml.p3.2xlarge** | 8    | 61 GB | $4.284         | GPU deep learning           |

### 8.2. Chi ph√≠ SageMaker Studio

| Component            | Gi√° (USD)        | Ghi ch√∫              |
| -------------------- | ---------------- | -------------------- |
| **Studio Notebooks** | $0.0582/hour     | ml.t3.medium default |
| **Domain Setup**     | Free             | One-time setup       |
| **Data Wrangler**    | $0.42/hour       | Visual data prep     |
| **Processing Jobs**  | Instance pricing | Same as training     |

### 8.3. Chi ph√≠ Model Registry & Endpoints

| Service                  | Gi√° (USD)             | Ghi ch√∫           |
| ------------------------ | --------------------- | ----------------- |
| **Model Registry**       | Free                  | Model versioning  |
| **Real-time Endpoint**   | $0.076/hour           | ml.t2.medium      |
| **Batch Transform**      | Instance pricing      | Pay per job       |
| **Multi-model Endpoint** | $0.076/hour + storage | Cost optimization |

### 8.4. ∆Ø·ªõc t√≠nh chi ph√≠ cho Task 4

**Training Job th·ª±c t·∫ø:**

- Instance: ml.m5.xlarge
- Duration: ~10-15 minutes
- **Training cost:** $0.276 √ó 0.25h = **$0.07**

**SageMaker Studio:**

- Notebook development: ~2 hours
- Instance: ml.t3.medium
- **Studio cost:** $0.0582 √ó 2h = **$0.12**

**Storage & Model Registry:**

- Model artifacts: ~50MB
- S3 storage: ~$0.001
- Model Registry: Free

**Total chi ph√≠ Task 4:**

| Component               | Duration | Cost        |
| ----------------------- | -------- | ----------- |
| Training (ml.m5.xlarge) | 15 mins  | $0.07       |
| Studio Notebooks        | 2 hours  | $0.12       |
| S3 Storage              | Monthly  | $0.001      |
| **Total**               |          | **‚âà $0.19** |

**So s√°nh v·ªõi c√°c options:**

| Approach                   | Instance     | Duration | Cost       | Performance       |
| -------------------------- | ------------ | -------- | ---------- | ----------------- |
| **Current (ml.m5.xlarge)** | 4 vCPU, 16GB | 15 min   | $0.07      | ‚úÖ Balanced       |
| Smaller (ml.m5.large)      | 2 vCPU, 8GB  | 25 min   | $0.06      | Slower            |
| Larger (ml.m5.2xlarge)     | 8 vCPU, 32GB | 8 min    | $0.07      | Faster, same cost |
| Spot instance              | Same specs   | 15 min   | $0.02-0.05 | 60-70% savings    |

{{% notice info %}}
**üí∞ Cost Optimization Tips:**

- **Spot instances:** 60-70% cheaper cho non-critical training
- **Smaller instances:** OK cho datasets < 1GB
- **Studio auto-shutdown:** T·ª± ƒë·ªông t·∫Øt notebooks sau 1h idle
- **Batch jobs:** Thay v√¨ real-time endpoints cho inference
  {{% /notice %}}

---

{{% notice info %}}
**üìä SageMaker Unified Studio Benefits:**

- **Integrated Workspace**: Project-based collaboration v·ªõi shared resources
- **Managed Infrastructure**: Auto-provisioned compute cho notebooks v√† training
- **Cross-Region Support**: Built-in handling c·ªßa S3 cross-region access
- **Asset Catalog**: Automatic registration c·ªßa models v√† datasets
- **Team Collaboration**: Shared notebooks, workflows, v√† approval processes
- **Cost Optimization**: Managed compute v·ªõi automatic scaling
- **Unified Interface**: Single pane for data, ML, v√† generative AI workflows
  {{% /notice %}}

## üìπ Video th·ª±c hi·ªán Task 4

<div style="position: relative; width: 100%; max-width: 2000px; margin: 0 auto; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe 
    style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" 
    src="https://www.youtube.com/embed/watch?v=pV-5ErGHAI0&list=PL53MEKrSAUpu0i5F-ttcVdKkSv0jb48Mc&index=3" 
    title="YouTube video player" 
    frameborder="0" 
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
    referrerpolicy="strict-origin-when-cross-origin" 
    allowfullscreen>
  </iframe>
</div>

---

**Next Step**: [Task 05: Production VPC](../5-vpc/)
