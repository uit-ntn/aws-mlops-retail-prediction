---
title: "End-to-End Model Training Pipeline"
weight: 4
chapter: false
pre: "<b>4. </b>"
---

## Má»¥c tiÃªu Task 4

Huáº¥n luyá»‡n mÃ´ hÃ¬nh dá»± bÃ¡o **BASKET_PRICE_SENSITIVITY** (Low/Medium/High) báº±ng Amazon SageMaker vá»›i pipeline tá»± Ä‘á»™ng ETL â†’ Training â†’ Model Registry.

â†’ **TrÃ¡i tim cá»§a MLOps pipeline** - tá»« dá»¯ liá»‡u thÃ´ Ä‘áº¿n model production-ready.

**Input**
<<<<<<< HEAD
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
- AWS Account vá»›i quyá»n SageMaker/S3/CloudWatch
- S3 bucket vá»›i dá»¯ liá»‡u (tá»« Task 3)
- IAM Role SageMaker (tá»« Task 2)

**Káº¿t quáº£**
<<<<<<< HEAD
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
- Model Ä‘áº¡t accuracy â‰¥ 80%, F1 â‰¥ 0.7
- Model Ä‘Æ°á»£c Ä‘Äƒng kÃ½ trong Model Registry
- Artifacts lÆ°u trá»¯ trong S3

**Chi phÃ­**: ~**$0.3-0.5/job** (ml.m5.large, 10-15 phÃºt)

{{% notice info %}}
**ğŸ’¡ Task 4 - MLOps Core Pipeline:**
<<<<<<< HEAD
- **ETL tá»± Ä‘á»™ng** - Raw data â†’ Features
- **Model training** - Random Forest classifier  
- **Model evaluation** - Accuracy, F1, Confusion Matrix
- **Model Registry** - Version control vÃ  approval
{{% /notice %}}
=======

- **ETL tá»± Ä‘á»™ng** - Raw data â†’ Features
- **Model training** - Random Forest classifier
- **Model evaluation** - Accuracy, F1, Confusion Matrix
- **Model Registry** - Version control vÃ  approval
  {{% /notice %}}
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48

## 1. Chuáº©n bá»‹ mÃ´i trÆ°á»ng vÃ  kiá»ƒm tra prerequisites

### 1.1. Kiá»ƒm tra S3 bucket (tá»« Task 3)

**AWS Console â†’ S3:**
<<<<<<< HEAD
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
1. TÃ¬m bucket: `mlops-retail-prediction-dev-[account-id]`
2. Kiá»ƒm tra cáº¥u trÃºc thá»±c táº¿:

   ```
   raw/           # transactions.csv + _select/ folder
<<<<<<< HEAD
   silver/        # shop_week partitions (200607-200619) 
   gold/          # features Ä‘Ã£ xá»­ lÃ½ (sáº½ táº¡o tá»« silver/)
   artifacts/     # model outputs (sáº½ táº¡o)
    ```

![S3 bucket placeholder](/images4-sagemake-training/01-s3-bucket.png)
=======
   silver/        # shop_week partitions (200607-200619)
   gold/          # features Ä‘Ã£ xá»­ lÃ½ (sáº½ táº¡o tá»« silver/)
   artifacts/     # model outputs (sáº½ táº¡o)
   ```

![S3 bucket placeholder](/images/4-sagemake-training/01-s3-bucket.png)
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48

### 1.2. XÃ¡c minh IAM Role (tá»« Task 2)

**AWS Console â†’ IAM â†’ Roles:**
<<<<<<< HEAD
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
1. TÃ¬m role: `mlops-retail-prediction-dev-sagemaker-execution`
2. Kiá»ƒm tra permissions:
   - âœ… `AmazonSageMakerFullAccess`
   - âœ… `AmazonS3FullAccess`
   - âœ… `CloudWatchLogsFullAccess`
<<<<<<< HEAD
 
![IAM role placeholder](/images4-sagemake-training/02-iam-role.png)
=======

![IAM role placeholder](/images/4-sagemake-training/02-iam-role.png)
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48

{{% notice warning %}}
Warning: Náº¿u bucket cá»§a báº¡n dÃ¹ng SSE-KMS, role cáº§n cÃ³ quyá»n decrypt/encrypt vá»›i KMS key; náº¿u dÃ¹ng cross-account S3, kiá»ƒm tra thÃªm trust policy.
{{% /notice %}}

## 2. Táº¡o SageMaker Domain vÃ  Project

### 2.1. Truy cáº­p SageMaker Unified Studio

**AWS Console â†’ SageMaker:**
<<<<<<< HEAD
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
1. Truy cáº­p URL: `https://[domain-id].studio.sagemaker.[region].amazonaws.com`
2. Hoáº·c tá»« SageMaker Console â†’ **Studio** â†’ **Open Studio**
3. Chá»n authentication method:
   - **Sign in with SSO** (náº¿u cÃ³ setup SSO)
   - **Sign in with AWS IAM** (dÃ¹ng IAM user/role)

<<<<<<< HEAD
![SageMaker Unified Studio Login](/images4-sagemake-training/04.1-domain.png)
=======
![SageMaker Unified Studio Login](/images/4-sagemake-training/04.1-domain.png)
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48

4. Sau khi Ä‘Äƒng nháº­p, báº¡n sáº½ tháº¥y giao diá»‡n **SageMaker Unified Studio**
5. Dashboard hiá»ƒn thá»‹:
   - **"Good morning"** greeting
   - **Search bar**: "Search for data products, assets, and projects"
   - **Discover section**: Catalog, Generative AI playground, Shared generative AI assets
   - **Build section**: ML and generative AI model development, Generative AI app development
   - **Browse all projects** vÃ  **Create project** buttons

<<<<<<< HEAD
![SageMaker Unified Studio Dashboard](/images4-sagemake-training/04.2-domain.png)


{{% notice info %}}
**ğŸ’¡ SageMaker Unified Studio:**
=======
![SageMaker Unified Studio Dashboard](/images/4-sagemake-training/04.2-domain.png)

{{% notice info %}}
**ğŸ’¡ SageMaker Unified Studio:**

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
- **Unified interface** cho data analytics, ML, vÃ  generative AI
- **Project-based workspace** vá»›i shared resources
- **Built-in collaboration** vá»›i team members vÃ  approval workflows
- **Integrated tools**: Notebooks, Visual ETL, Workflows, Chat agents
<<<<<<< HEAD
{{% /notice %}}
=======
  {{% /notice %}}
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48

### 2.2. Táº¡o Project trong Unified Studio

**Trong SageMaker Unified Studio dashboard:**

**BÆ°á»›c 1: Truy cáº­p Create Project**
<<<<<<< HEAD
1. Trong **Build** section, click **"Create project"** (nÃºt xanh)
2. Hoáº·c click **"Browse all projects"** â†’ **"Create project"**

![Project Name and Description](/images4-sagemake-training/05.2.png)

**BÆ°á»›c 2: Äiá»n thÃ´ng tin Project (Step 1)**
=======

1. Trong **Build** section, click **"Create project"** (nÃºt xanh)
2. Hoáº·c click **"Browse all projects"** â†’ **"Create project"**

![Project Name and Description](/images/4-sagemake-training/05.2.png)

**BÆ°á»›c 2: Äiá»n thÃ´ng tin Project (Step 1)**

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
1. **Project name**: `retail-ml-training`
2. **Description**: `Retail price sensitivity model training`
3. Click **Next** Ä‘á»ƒ chuyá»ƒn tá»›i Step 2

<<<<<<< HEAD
![Project Name and Description](/images4-sagemake-training/05.3.png)

**BÆ°á»›c 2.5: Chá»n Project Profile (Step 2)**
=======
![Project Name and Description](/images/4-sagemake-training/05.3.png)

**BÆ°á»›c 2.5: Chá»n Project Profile (Step 2)**

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
1. **Project profile**: Chá»n **"All capabilities"** (highlighted in blue)
   - **Description**: "Analyze data and build machine learning and generative AI models and applications powered by Amazon Bedrock, Amazon EMR, AWS Glue, Amazon Athena, Amazon SageMaker AI and Amazon SageMaker Lakehouse"
   - **Tooling**: LakeHouse Database, Workflows
   - **+ 12 more** capabilities
2. CÃ¡c options khÃ¡c: **Generative AI application development**, **SQL analytics**

<<<<<<< HEAD
![Project Profile Selection](/images4-sagemake-training/05.4.png)

**BÆ°á»›c 3: Blueprint Parameters**
=======
![Project Profile Selection](/images/4-sagemake-training/05.4.png)

**BÆ°á»›c 3: Blueprint Parameters**

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
- **S3 location**: `s3://amazon-sagemaker-[account-id]-ap-southeast-1-[random-id]`
- **Retention**: 731 days
- **Enable Project Repository Auto Sync**: false
- **Lakehouse Database**: `glue_db`

<<<<<<< HEAD
![Blueprint Parameters](/images4-sagemake-training/05.5.png)

**BÆ°á»›c 4: Create Project**
- Review cÃ¡c settings vÃ  click **"Create project"**

![Project Creation Final](/images4-sagemake-training/05.5.png)
=======
![Blueprint Parameters](/images/4-sagemake-training/05.5.png)

**BÆ°á»›c 4: Create Project**

- Review cÃ¡c settings vÃ  click **"Create project"**

![Project Creation Final](/images/4-sagemake-training/05.5.png)
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48

- Äá»£i 2-3 phÃºt Ä‘á»ƒ Project Ä‘Æ°á»£c provisioned

### 2.3. Truy cáº­p Project Workspace

**Sau khi Project `retail-ml-training` táº¡o thÃ nh cÃ´ng:**

**BÆ°á»›c 1: VÃ o Project Overview**
<<<<<<< HEAD
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
1. Project sáº½ hiá»ƒn thá»‹ trong danh sÃ¡ch vá»›i status **"Created"**
2. Click vÃ o project name `retail-ml-training` Ä‘á»ƒ vÃ o **Project overview**
3. Project overview hiá»ƒn thá»‹:
   - **Project title**: `retail-ml-training`
   - **Description**: "Retail price sensitivity model training"
   - **Project files (6)**: `.ipynb_checkpoints`, `workflows`, `.libs.json`, `.temp_sagemaker_unified_studio_debugging_info`, `README.md`, `getting_started.ipynb`
   - **S3 path**: `/dzd-5kultpj28sm31d/cu2gr2js1w1wv` (project workspace path)
   - **Actions** vÃ  **New** dropdown buttons
     - **Project overview** (active)
     - **Data** - data assets vÃ  connections
<<<<<<< HEAD
     - **Compute** - compute resources vÃ  environments  
=======
     - **Compute** - compute resources vÃ  environments
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
     - **Members** - team collaboration
     - **Project catalog** (expandable)
     - **Assets**, **Subscription requests**, **Data sources**, **Metadata entities**

<<<<<<< HEAD

**BÆ°á»›c 2: Táº¡o Notebook**
1. Click **"New"** dropdown (nÃºt xanh) â†’ chá»n **"Notebook"**
2. **New** dropdown hiá»ƒn thá»‹ cÃ¡c options (theo thá»© tá»± trong hÃ¬nh):

![Project Overview](/images4-sagemake-training/05.6.png)

![New Notebook Creation](/images4-sagemake-training/06.1.png)

   - **Notebook** (chá»n option nÃ y)

=======
**BÆ°á»›c 2: Táº¡o Notebook**

1. Click **"New"** dropdown (nÃºt xanh) â†’ chá»n **"Notebook"**
2. **New** dropdown hiá»ƒn thá»‹ cÃ¡c options (theo thá»© tá»± trong hÃ¬nh):

![Project Overview](/images/4-sagemake-training/05.6.png)

![New Notebook Creation](/images/4-sagemake-training/06.1.png)

- **Notebook** (chá»n option nÃ y)
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48

**BÆ°á»›c 3: Project Welcome**
Trong project overview, báº¡n cÅ©ng tháº¥y **Readme** section hiá»ƒn thá»‹ **"Welcome"** vá»›i hÆ°á»›ng dáº«n báº¯t Ä‘áº§u sá»­ dá»¥ng project.

### 2.4. XÃ¡c minh EC2 Permissions (Ä‘Ã£ cÃ³ tá»« Task 2)

**EC2 permissions Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘áº§y Ä‘á»§ trong Task 2**, bao gá»“m inline policy `SageMakerEC2Access` trong role `mlops-retail-prediction-dev-sagemaker-execution`.

**XÃ¡c minh EC2 permissions Ä‘Ã£ cÃ³:**

```powershell
# Kiá»ƒm tra inline policy Ä‘Ã£ Ä‘Æ°á»£c thÃªm tá»« Task 2
aws iam get-role-policy --role-name mlops-retail-prediction-dev-sagemaker-execution --policy-name SageMakerEC2Access

# Test quyá»n EC2 describe
aws ec2 describe-vpcs --region us-east-1
```

**Role Ä‘Ã£ cÃ³ Ä‘á»§ 4 policies tá»« Task 2:**
<<<<<<< HEAD
- âœ… `AmazonSageMakerFullAccess` (AWS managed)
- âœ… `AmazonS3FullAccess` (AWS managed)  
=======

- âœ… `AmazonSageMakerFullAccess` (AWS managed)
- âœ… `AmazonS3FullAccess` (AWS managed)
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
- âœ… `CloudWatchLogsFullAccess` (AWS managed)
- âœ… `SageMakerEC2Access` (inline policy cho Project creation)

{{% notice success %}}
**Project creation ready:** Role Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘áº§y Ä‘á»§ tá»« Task 2, cÃ³ thá»ƒ táº¡o Project ngay láº­p tá»©c.
{{% /notice %}}

### 2.5. Khuyáº¿n nghá»‹ Region cho Task 4

**TÃ³m táº¯t:** Náº¿u dá»¯ liá»‡u `gold/` vÃ  `artifacts/` hiá»‡n Ä‘ang náº±m trong bucket `mlops-retail-prediction-dev-842676018087` (region `us-east-1`), khuyáº¿n nghá»‹ lÃ  **táº¡o SageMaker Domain / Project á»Ÿ cÃ¹ng `us-east-1`** Ä‘á»ƒ trÃ¡nh lá»—i cross-region (S3 301), phá»©c táº¡p vá»›i KMS vÃ  endpoint.

- **Lá»£i Ã­ch khi táº¡o Project á»Ÿ `us-east-1`:**
<<<<<<< HEAD
    - Loáº¡i bá» lá»—i 'bucket must be addressed using the specified endpoint' khi SageMaker táº£i dá»¯ liá»‡u tá»« S3.
    - KhÃ´ng cáº§n duy trÃ¬ KMS keys hoáº·c IAM policies á»Ÿ nhiá»u region.
    - Ãt rá»§i ro khi cháº¡y training jobs tá»« Studio/Project.

- **Khi cáº§n táº¡o Project á»Ÿ `ap-southeast-1` (hoáº·c region khÃ¡c):**
    - Pháº£i **chuyá»ƒn** hoáº·c **sao chÃ©p** dá»¯ liá»‡u `gold/` vÃ  `artifacts/` sang bucket á»Ÿ region Ä‘Ã³ hoáº·c cáº¥u hÃ¬nh Cross-Region Replication (CRR).
    - Táº¡o KMS keys tÆ°Æ¡ng á»©ng vÃ  cáº­p nháº­t policies/roles cho bucket má»›i.
=======

  - Loáº¡i bá» lá»—i 'bucket must be addressed using the specified endpoint' khi SageMaker táº£i dá»¯ liá»‡u tá»« S3.
  - KhÃ´ng cáº§n duy trÃ¬ KMS keys hoáº·c IAM policies á»Ÿ nhiá»u region.
  - Ãt rá»§i ro khi cháº¡y training jobs tá»« Studio/Project.

- **Khi cáº§n táº¡o Project á»Ÿ `ap-southeast-1` (hoáº·c region khÃ¡c):**
  - Pháº£i **chuyá»ƒn** hoáº·c **sao chÃ©p** dá»¯ liá»‡u `gold/` vÃ  `artifacts/` sang bucket á»Ÿ region Ä‘Ã³ hoáº·c cáº¥u hÃ¬nh Cross-Region Replication (CRR).
  - Táº¡o KMS keys tÆ°Æ¡ng á»©ng vÃ  cáº­p nháº­t policies/roles cho bucket má»›i.
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48

---

Náº¿u báº¡n muá»‘n giá»¯ Project trong `ap-southeast-1`, Ä‘Ã¢y lÃ  vÃ­ dá»¥ lá»‡nh Ä‘á»ƒ táº¡o bucket vÃ  sao chÃ©p dá»¯ liá»‡u (PowerShell / CloudShell):

```powershell
# Táº¡o bucket má»›i á»Ÿ ap-southeast-1
aws s3 mb s3://mlops-retail-prediction-dev-842676018087-apse1 --region ap-southeast-1

# Äá»“ng bá»™ gold/ vÃ  artifacts/ sang bucket má»›i
aws s3 sync s3://mlops-retail-prediction-dev-842676018087/gold/ s3://mlops-retail-prediction-dev-842676018087-apse1/gold/ --acl bucket-owner-full-control --exact-timestamps
aws s3 sync s3://mlops-retail-prediction-dev-842676018087/artifacts/ s3://mlops-retail-prediction-dev-842676018087-apse1/artifacts/ --acl bucket-owner-full-control --exact-timestamps

# (Optional) Táº¡o KMS key á»Ÿ ap-southeast-1 vÃ  cáº­p nháº­t bucket policy / IAM role
# aws kms create-key --region ap-southeast-1 --description "KMS for mlops retail ap-southeast-1"
```

Ghi chÃº:
<<<<<<< HEAD
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
- Náº¿u bucket nguá»“n dÃ¹ng SSE-KMS, Ä‘áº£m báº£o báº¡n táº¡o tÆ°Æ¡ng á»©ng KMS key á»Ÿ region Ä‘Ã­ch vÃ  cáº­p nháº­t cáº£ bucket policy vÃ  role `mlops-retail-prediction-dev-sagemaker-execution`.
- Náº¿u muá»‘n resolution nhanh vÃ  Ã­t thay Ä‘á»•i IAM, chá»n táº¡o Project/Domain á»Ÿ `us-east-1` (nÆ¡i bucket hiá»‡n cÃ³) â€” Ä‘Ã¢y lÃ  phÆ°Æ¡ng Ã¡n khuyáº¿n nghá»‹ cho lab vÃ  cháº¡y training nhanh.

### 3. ETL - Chuáº©n bá»‹ dá»¯ liá»‡u trong SageMaker Studio

**ğŸ¯ Má»¥c tiÃªu:** Äá»c Táº¤T Cáº¢ shop_week partitions tá»« S3 silver/ vÃ  táº¡o train/test/validation splits

**Input:** `silver/shop_week=200607/` Ä‘áº¿n `silver/shop_week=200619/` (14 partitions)  
**Output:** `gold/train.parquet`, `gold/test.parquet`, `gold/validation.parquet`

#### **BÆ°á»›c 1: Táº¡o ETL Notebook trong Project**

**Tá»« Project overview:**
<<<<<<< HEAD
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
1. Click **"New"** dropdown â†’ chá»n **"Notebook"**
2. Notebook sáº½ má»Ÿ trong browser tab má»›i
3. Chá»n kernel: **Python 3 (Data Science 3.0)**
4. Äáº·t tÃªn notebook: **File** â†’ **Rename** â†’ `retail-etl-pipeline.ipynb`
5. Notebook sáº½ tá»± Ä‘á»™ng lÆ°u vÃ o S3 project path

{{% notice info %}}
<<<<<<< HEAD
**LÆ°u Ã½:** 
- Notebook cháº¡y trÃªn managed compute instance cá»§a SageMaker
- Files tá»± Ä‘á»™ng sync vá»›i S3 project storage
- CÃ³ thá»ƒ chia sáº» vá»›i team members trong project
{{% /notice %}}
=======
**LÆ°u Ã½:**

- Notebook cháº¡y trÃªn managed compute instance cá»§a SageMaker
- Files tá»± Ä‘á»™ng sync vá»›i S3 project storage
- CÃ³ thá»ƒ chia sáº» vá»›i team members trong project
  {{% /notice %}}
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48

#### **BÆ°á»›c 2: Thá»±c hiá»‡n ETL Pipeline**

Táº¡o vÃ  cháº¡y cÃ¡c cells sau theo thá»© tá»±:

**Cell 1: CÃ i Ä‘áº·t dependencies**
<<<<<<< HEAD
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
```bash
# Install all required packages
pip install pandas pyarrow s3fs scikit-learn xgboost sagemaker boto3 joblib
```

**Cell 2: Thiáº¿t láº­p cáº¥u hÃ¬nh**
<<<<<<< HEAD
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
```python
import boto3
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import json
from datetime import datetime

# Configuration - update bucket name theo project cá»§a báº¡n
# Náº¿u dÃ¹ng project S3: amazon-sagemaker-[account-id]-ap-southeast-1-[random-id]
# Hoáº·c bucket tá»« Task 3: mlops-retail-prediction-dev-[account-id]
bucket_name = 'amazon-sagemaker-842676018087-ap-southeast-1-f6cd5056a835'  # <-- Sá»¬A theo project cá»§a báº¡n
raw_prefix = 'silver/'
gold_prefix = 'gold/'

# Initialize AWS clients
s3 = boto3.client('s3')
print(f'âœ… AWS clients initialized. Bucket: {bucket_name}')
```

**Cell 3: Load dá»¯ liá»‡u tá»« táº¥t cáº£ partitions**
<<<<<<< HEAD
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
```python
print(f'ğŸ“Š Loading all partitioned data from s3://{bucket_name}/{raw_prefix}...')

# Discover all parquet files in silver/
response = s3.list_objects_v2(Bucket=bucket_name, Prefix=raw_prefix)
<<<<<<< HEAD
parquet_files = [obj['Key'] for obj in response.get('Contents', []) 
=======
parquet_files = [obj['Key'] for obj in response.get('Contents', [])
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
                if obj['Key'].endswith('.parquet') and obj['Size'] > 0]

print(f'ğŸ“ Found {len(parquet_files)} parquet files')

# Load all files into one DataFrame
all_dataframes = []
total_rows = 0

for i, key in enumerate(parquet_files[:10]):  # Limit to first 10 files for demo
    s3_path = f's3://{bucket_name}/{key}'
<<<<<<< HEAD
    
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
    try:
        df = pd.read_parquet(s3_path)
        all_dataframes.append(df)
        total_rows += len(df)
        print(f'  âœ… File {i+1}: {len(df):,} rows from {key.split("/")[-1]}')
    except Exception as e:
        print(f'  âŒ Failed to load {key}: {e}')

# Combine all data
combined_data = pd.concat(all_dataframes, ignore_index=True)
print(f'\nğŸ¯ Combined dataset: {combined_data.shape}')
print(f'ğŸ“‹ Columns: {list(combined_data.columns)}')
```

**Cell 4: Táº¡o features vÃ  target variable**
<<<<<<< HEAD
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
```python
print("ğŸ“Œ STEP 1 â€” Columns in combined_data:")
print(list(combined_data.columns))

# Force lowercase column names for safety
combined_data.columns = [c.lower() for c in combined_data.columns]
print("\nğŸ“Œ STEP 2 â€” Columns after lowercase normalization:")
print(list(combined_data.columns))

print("\nğŸ“Œ STEP 3 â€” Checking required columns...")

if 'basket_id' in combined_data.columns and 'spend' in combined_data.columns:
    print("âœ… Found basket_id and spend â€” proceeding with basket-level feature engineering.")

    print("\nğŸ“Œ STEP 4 â€” Converting numeric columns...")
    for col in ['spend', 'quantity']:
        if col in combined_data.columns:
            combined_data[col] = pd.to_numeric(combined_data[col], errors='coerce')
            print(f"  - Converted column '{col}' to numeric.")

    print("\nğŸ“Œ STEP 5 â€” Starting groupby aggregation...")
    print("âš™ï¸ Aggregating, this may take a moment...")

    features = combined_data.groupby('basket_id').agg({
        'spend': ['sum', 'mean', 'count'],
        'quantity': ['sum', 'mean'] if 'quantity' in combined_data.columns else []
    }).reset_index()

    print("âœ… Aggregation complete.")
    print(f"ğŸ“Š Features raw shape: {features.shape}")

    # Flatten column names
    print("\nğŸ“Œ STEP 6 â€” Flattening column names...")
    features.columns = (
        ['basket_id', 'spend_sum', 'spend_mean', 'spend_count'] +
        (['quantity_sum', 'quantity_mean'] if 'quantity' in combined_data.columns else [])
    )
    print("ğŸ“‹ New feature columns:", list(features.columns))

    print("\nğŸ“Œ STEP 7 â€” Creating price_sensitivity target variable...")
    median_spend = features['spend_sum'].median()
    print(f"Median spend = {median_spend}")

    features['price_sensitivity'] = (features['spend_sum'] > median_spend).astype(int)

else:
    print("âŒ Could NOT find required columns for basket-level engineering.")
    print("Available columns:", list(combined_data.columns))
<<<<<<< HEAD
    
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
    print("\nğŸ“Œ Fallback: Using transaction-level feature engineering...")
    features = combined_data.copy()

    if 'spend' in features.columns:
        features['price_sensitivity'] = (
            features['spend'] > features['spend'].median()
        ).astype(int)
        print("âœ… Created price_sensitivity for transaction-level data.")
    else:
        raise RuntimeError("âŒ spend column not found â€” cannot create price_sensitivity.")

print("\nğŸ“Œ STEP 8 â€” Final feature table shape:")
print(features.shape)

print("\nğŸ“Œ STEP 9 â€” Target distribution:")
print(features['price_sensitivity'].value_counts(dropna=False))

print("\nğŸ“Œ STEP 10 â€” Sample output:")
print(features.head())
```

**Cell 5: Táº¡o train/test/validation splits vÃ  lÆ°u vÃ o S3**
<<<<<<< HEAD
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
```python
print('ğŸ“‹ Creating train/test/validation splits...')

# Create stratified splits
train_data, temp_data = train_test_split(
    features, test_size=0.3, random_state=42,
    stratify=features['price_sensitivity']
)

test_data, val_data = train_test_split(
    temp_data, test_size=0.33, random_state=42,
    stratify=temp_data['price_sensitivity']
)

splits = {
    'train': train_data,
    'test': test_data,
    'validation': val_data
}

# Save to S3
print(f'ğŸ’¾ Saving splits to s3://{bucket_name}/{gold_prefix}...')

for split_name, split_data in splits.items():
    s3_path = f's3://{bucket_name}/{gold_prefix}{split_name}.parquet'
    split_data.to_parquet(s3_path, index=False)
    print(f'  âœ… {split_name}: {len(split_data):,} rows saved to {split_name}.parquet')

print('\nğŸ‰ ETL Complete! Data ready for training.')

# Verify files created
response = s3.list_objects_v2(Bucket=bucket_name, Prefix=gold_prefix)
if 'Contents' in response:
    print(f'\nğŸ“ Files in gold/:')
    for obj in response['Contents']:
        size_mb = obj['Size'] / (1024*1024)
        print(f'  ğŸ“„ {obj["Key"]}: {size_mb:.2f} MB')
```

## 4. Training - Huáº¥n luyá»‡n Model

**ğŸ¯ Má»¥c tiÃªu:** Huáº¥n luyá»‡n mÃ´ hÃ¬nh Random Forest Ä‘á»ƒ phÃ¢n loáº¡i Ä‘á»™ nháº¡y giÃ¡ khÃ¡ch hÃ ng

**Input:** S3 `gold/train.parquet`, `gold/test.parquet`, `gold/validation.parquet`  
**Output:** Trained Random Forest model trong S3 artifacts/ vá»›i performance metrics

#### **BÆ°á»›c 1: Táº¡o Training Notebook trong Project**

1. Trong Studio interface, click **File** â†’ **New** â†’ **Notebook**
2. Chá»n **conda_python3** kernel (hoáº·c **Python 3 (Data Science)**)
<<<<<<< HEAD
3. Äáº·t tÃªn notebook: `notebooks/retail-model-training.ipynb`  
4. Click **Create**

![Create notebook](/images4-sagemake-training/05.7.png)

=======
3. Äáº·t tÃªn notebook: `notebooks/retail-model-training.ipynb`
4. Click **Create**

![Create notebook](/images/4-sagemake-training/05.7.png)
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48

**ğŸ’¡ LÆ°u Ã½:** Notebook sáº½ Ä‘Æ°á»£c lÆ°u trong Project repository vÃ  cÃ³ thá»ƒ commit vÃ o CodeCommit.

#### **BÆ°á»›c 2: Thá»±c hiá»‡n Model Training**

Táº¡o vÃ  cháº¡y cÃ¡c cells sau theo thá»© tá»±:

**Cell 1: Setup SageMaker Configuration**
<<<<<<< HEAD
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
```python
import sagemaker
import boto3
import os
from sagemaker.sklearn.estimator import SKLearn

# Initialize SageMaker session and get execution role
sagemaker_session = sagemaker.Session()
role = sagemaker.get_execution_role()

# Configuration - update bucket name if different
bucket_name = 'mlops-retail-prediction-dev-842676018087'
gold_prefix = 'gold/'
artifacts_prefix = 'artifacts/'

print(f'âœ… SageMaker Role: {role}')
print(f'ğŸ“Š Training Data: s3://{bucket_name}/{gold_prefix}')
print(f'ğŸ“¦ Model Artifacts: s3://{bucket_name}/{artifacts_prefix}')
```

**Cell 2: Táº¡o Training Script**

```python
%%writefile train_retail_model.py
import pandas as pd
import joblib
import os
import json
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report

def main():
    # ÄÆ°á»ng dáº«n chuáº©n cá»§a SageMaker
    train_dir = os.environ.get("SM_CHANNEL_TRAIN", "/opt/ml/input/data/train")
    model_dir = os.environ.get("SM_MODEL_DIR", "/opt/ml/model")
<<<<<<< HEAD
    
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
    # 1. Load data
    train_path = os.path.join(train_dir, "train.parquet")
    print(f"ğŸ“– Loading training data from: {train_path}")
    df = pd.read_parquet(train_path)

    print(f"ğŸ“Š Dataset shape: {df.shape}")
    print(f"ğŸ“‹ Columns: {list(df.columns)}")
<<<<<<< HEAD
    
    # 2. Chuáº©n bá»‹ features & target
    target_col = "price_sensitivity" if "price_sensitivity" in df.columns else df.columns[-1]
    feature_cols = [c for c in df.columns if c not in [target_col, "basket_id"]]
    
    X = df[feature_cols]
    y = df[target_col]
    
    print(f"ğŸ”¢ Features: {len(feature_cols)} columns")
    print(f"ğŸ¯ Target: {target_col}")
    print(f"ğŸ“ˆ Target distribution: {dict(y.value_counts())}")
    
=======

    # 2. Chuáº©n bá»‹ features & target
    target_col = "price_sensitivity" if "price_sensitivity" in df.columns else df.columns[-1]
    feature_cols = [c for c in df.columns if c not in [target_col, "basket_id"]]

    X = df[feature_cols]
    y = df[target_col]

    print(f"ğŸ”¢ Features: {len(feature_cols)} columns")
    print(f"ğŸ¯ Target: {target_col}")
    print(f"ğŸ“ˆ Target distribution: {dict(y.value_counts())}")

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
    # 3. Train/validation split (stratified)
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
<<<<<<< HEAD
    
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
    # 4. Train Random Forest model
    print("\nğŸŒ² Training Random Forest model...")
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42
    )
<<<<<<< HEAD
    
    model.fit(X_train, y_train)
    
    # 5. Evaluate model
    y_pred = model.predict(X_val)
    
=======

    model.fit(X_train, y_train)

    # 5. Evaluate model
    y_pred = model.predict(X_val)

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
    accuracy = accuracy_score(y_val, y_pred)
    f1 = f1_score(y_val, y_pred, average="macro")
    precision = precision_score(y_val, y_pred, average="macro")
    recall = recall_score(y_val, y_pred, average="macro")
<<<<<<< HEAD
    
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
    print(f"\nğŸ“Š Model Performance:")
    print(f"  âœ… Accuracy:  {accuracy:.4f}")
    print(f"  âœ… Precision: {precision:.4f}")
    print(f"  âœ… Recall:    {recall:.4f}")
    print(f"  âœ… F1-Score:  {f1:.4f}")
<<<<<<< HEAD
    
    # 6. Save model and results
    os.makedirs(model_dir, exist_ok=True)
    
=======

    # 6. Save model and results
    os.makedirs(model_dir, exist_ok=True)

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
    # Save Random Forest model
    model_path = os.path.join(model_dir, 'model.joblib')
    joblib.dump(model, model_path)
    print(f'ğŸŒ² Random Forest model saved: {model_path}')
<<<<<<< HEAD
    
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
    # Save training results
    results_path = os.path.join(model_dir, 'training_results.json')
    training_summary = {
        'model_name': 'RandomForest',
        'accuracy': float(accuracy),
        'precision': float(precision),
        'recall': float(recall),
        'f1_score': float(f1),
        'feature_count': len(feature_cols),
        'training_samples': len(X_train),
        'validation_samples': len(X_val),
        'feature_names': feature_cols,
        'classification_report': classification_report(y_val, y_pred, output_dict=True)
<<<<<<< HEAD
    
    with open(results_path, 'w') as f:
        json.dump(training_summary, f, indent=2)
    
    print(f'ğŸ“‹ Results saved: {results_path}')
    print(f'ğŸ“¦ Model artifacts: 1 model + 1 results file')
    
    # Validation checks
    target_accuracy = 0.80
    target_f1 = 0.70
    
=======

    with open(results_path, 'w') as f:
        json.dump(training_summary, f, indent=2)

    print(f'ğŸ“‹ Results saved: {results_path}')
    print(f'ğŸ“¦ Model artifacts: 1 model + 1 results file')

    # Validation checks
    target_accuracy = 0.80
    target_f1 = 0.70

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
    print(f'\nğŸ¯ Performance Validation:')
    print(f'  ğŸ“Š Accuracy â‰¥ {target_accuracy}: {"âœ…" if accuracy >= target_accuracy else "âŒ"} ({accuracy:.3f})')
    print(f'  ğŸ“Š F1-Score â‰¥ {target_f1}: {"âœ…" if f1 >= target_f1 else "âŒ"} ({f1:.3f})')

if __name__ == '__main__':
    main()
'''

# Write training script to file
with open('train_retail_model.py', 'w') as f:
    f.write(train_script)

```

**Cell 3: Submit SageMaker Training Job**
<<<<<<< HEAD
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
```python
print("ğŸš€ Submitting SageMaker Training Job...")

# Pre-flight: kiá»ƒm tra region + data trong gold/
s3_client = boto3.client("s3")

bucket_location = s3_client.get_bucket_location(Bucket=bucket_name)
bucket_region = bucket_location["LocationConstraint"] or "us-east-1"
current_region = sagemaker_session.boto_region_name

print(f"ğŸ“ Bucket region:    {bucket_region}")
print(f"ğŸ“ SageMaker region: {current_region}")

# Kiá»ƒm tra cross-region issue
if bucket_region != current_region:
    print(f"âš ï¸ Region mismatch detected!")
    print(f"   Bucket: {bucket_name} in {bucket_region}")
    print(f"   SageMaker: {current_region}")
    print(f"   This may cause S3 301 redirect errors during training")
    print(f"   Consider using project bucket in same region or configure cross-region access")
<<<<<<< HEAD
    
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
    # Option 1: Use project bucket in same region
    print(f"\nğŸ’¡ Option 1: Use project bucket (same region):")
    print(f"   bucket_name = '{project_bucket}'")
    print(f"   (But need to copy gold/ data to this bucket first)")
<<<<<<< HEAD
    
    # Option 2: Continue with cross-region
    print(f"\nğŸ’¡ Option 2: Continue with cross-region (may need additional config)")
    
=======

    # Option 2: Continue with cross-region
    print(f"\nğŸ’¡ Option 2: Continue with cross-region (may need additional config)")

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
    # For demo, we'll continue but warn user
    import warnings
    warnings.warn(f"Cross-region S3 access: {bucket_region} -> {current_region}")
else:
    print(f"âœ… Same region - no cross-region issues")

train_s3_uri = f"s3://{bucket_name}/{gold_prefix}"

resp = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=gold_prefix)
data_files = [o["Key"] for o in resp.get("Contents", []) if o["Key"].endswith(".parquet")]

if not data_files:
    raise ValueError(f"âŒ No .parquet files found in {train_s3_uri}. Run ETL trÆ°á»›c Ä‘Ã£.")

print(f"âœ… Found {len(data_files)} training files in {train_s3_uri}")

# Cáº¥u hÃ¬nh estimator
estimator = SKLearn(
    entry_point="train_retail_model.py",
    role=role,
    instance_type="ml.m5.xlarge",
    instance_count=1,
    framework_version="1.0-1",
    py_version="py3",
    base_job_name="retail-prediction-training",
    sagemaker_session=sagemaker_session,
)

print(f"ğŸ“Š Training data location: {train_s3_uri}")

# Cháº¡y training job
estimator.fit({"train": train_s3_uri}, wait=True)

job_name = estimator.latest_training_job.name
model_artifacts = estimator.model_data

print("\nğŸ‰ Training job completed!")
print("ğŸ“ Job name:       ", job_name)
print("ğŸ’¾ Model artifacts:", model_artifacts)
<<<<<<< HEAD
        
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
except Exception as e:
    print(f'âŒ Pre-flight check failed: {e}')
    print('ğŸ’¡ Solutions:')
    print('  1. Run ETL notebook first to create gold/ data')
    print('  2. Make sure you ran debug cell (BÆ°á»›c 4) to fix regions')
    print('  3. Check IAM role has S3 access')
    raise

# Configure estimator for Unified Studio project
estimator = SKLearn(
    entry_point='train_retail_model.py',
    role=role,
    instance_type='ml.m5.xlarge',  # Larger instance for better performance
    instance_count=1,
    framework_version='1.0-1',
    py_version='py3',
    base_job_name='retail-prediction-training',
    sagemaker_session=sagemaker_session,
    # Output path sáº½ Ä‘Æ°á»£c set tá»± Ä‘á»™ng vÃ o project S3 location
    output_path=f's3://{bucket_name}/{artifacts_prefix}'
)

print(f'ğŸ“Š Training data location: {train_s3_uri}')

# Start training job with error handling
try:
    print('â³ Starting training job (this will take 5-10 minutes)...')
    estimator.fit({'train': train_s3_uri}, wait=True)
<<<<<<< HEAD
    
    # Get job results
    job_name = estimator.latest_training_job.name
    model_artifacts = estimator.model_data
    
    print(f'\\nğŸ‰ Training job completed!')
    print(f'ğŸ“ Job name: {job_name}')
    print(f'ğŸ’¾ Model artifacts: {model_artifacts}')
    
=======

    # Get job results
    job_name = estimator.latest_training_job.name
    model_artifacts = estimator.model_data

    print(f'\\nğŸ‰ Training job completed!')
    print(f'ğŸ“ Job name: {job_name}')
    print(f'ğŸ’¾ Model artifacts: {model_artifacts}')

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
except Exception as e:
    print(f'âŒ Training job failed: {e}')
    print('ğŸ’¡ Troubleshooting:')
    print('  - Check CloudWatch logs for detailed error')
    print('  - Verify IAM role permissions')
    print('  - Ensure training data format is correct')
    raise
```

**Cell 4: Download & Äá»c Training Results**

```python
import os
import tarfile
import json
import boto3

print("ğŸ“Š Analyzing training results...")

# model_artifacts láº¥y tá»« Cell 3 (estimator.model_data)
print("ğŸ“¦ Artifact path:", model_artifacts)

# TÃ¡ch bucket + key
art_parts = model_artifacts.replace("s3://", "").split("/", 1)
art_bucket = art_parts[0]
art_key = art_parts[1]

s3 = boto3.client("s3")

# Táº£i file model.tar.gz vá» local
local_tar = "model.tar.gz"
s3.download_file(art_bucket, art_key, local_tar)
print("ğŸ“¥ Downloaded", local_tar)

# Giáº£i nÃ©n vÃ o thÆ° má»¥c model_artifacts/
extract_dir = "model_artifacts"
os.makedirs(extract_dir, exist_ok=True)

with tarfile.open(local_tar, "r:gz") as tar:
    tar.extractall(extract_dir)

print("\nğŸ“‚ Files inside model:")
print(os.listdir(extract_dir))

# Äá»c training_results.json
results_path = os.path.join(extract_dir, "training_results.json")
with open(results_path, "r") as f:
    results = json.load(f)

print("\nğŸŒ² RANDOM FOREST TRAINING RESULTS:")
print(f"  ğŸ¤– Model:      {results['model_name']}")
print(f"  ğŸ“Š Accuracy:   {results['accuracy']:.4f}")
print(f"  ğŸ“Š Precision:  {results['precision']:.4f}")
print(f"  ğŸ“Š Recall:     {results['recall']:.4f}")
print(f"  ğŸ“Š F1-Score:   {results['f1_score']:.4f}")
print(f"  ğŸ”¢ Features:   {results['feature_count']}")
print(f"  ğŸ“š Train rows: {results['training_samples']:,}")
print(f"  ğŸ§ª Val rows:   {results['validation_samples']:,}")

print("\nğŸ“‹ Per-class Performance:")
class_report = results['classification_report']
for class_name, metrics in class_report.items():
    if isinstance(metrics, dict) and 'f1-score' in metrics:
        print(f"  {class_name:>12}: Precision={metrics['precision']:.3f}, Recall={metrics['recall']:.3f}, F1={metrics['f1-score']:.3f}")

# Validate target
acc_target = 0.80
f1_target = 0.70

print("\nğŸ¯ Performance validation:")
print(f"  ğŸ“Š Accuracy â‰¥ {acc_target}: {'âœ…' if results['accuracy'] >= acc_target else 'âŒ'} ({results['accuracy']:.3f})")
print(f"  ğŸ“Š F1-score â‰¥ {f1_target}: {'âœ…' if results['f1_score'] >= f1_target else 'âŒ'} ({results['f1_score']:.3f})")
```

**Káº¿t quáº£**
<<<<<<< HEAD
![Káº¿t quáº£ training](/images4-sagemake-training/00.png)

=======
![Káº¿t quáº£ training](/images/4-sagemake-training/00.png)
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48

{{% notice success %}}
âœ… **Training HoÃ n thÃ nh!** Model Ä‘áº¡t target performance vÃ  sáºµn sÃ ng cho Model Registry.
{{% /notice %}}

## 5. Monitoring & Results

### 5.1. Theo dÃµi Training Job trong Studio

Trong **SageMaker Studio (Unified Studio)**:

1. Má»Ÿ má»¥c **Build** á»Ÿ thanh bÃªn trÃ¡i

2. Chá»n **Training jobs**

<<<<<<< HEAD
![Training logs example](/images4-sagemake-training/08.1.png)
=======
![Training logs example](/images/4-sagemake-training/08.1.png)
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48

3. TÃ¬m job cÃ³ tÃªn báº¯t Ä‘áº§u báº±ng: retail-prediction-training-

4. Nháº¥p vÃ o **tÃªn Training Job** Ä‘á»ƒ má»Ÿ chi tiáº¿t

<<<<<<< HEAD
![Training logs example](/images4-sagemake-training/08.2.png)

5. Chá»n tab **Logs** Ä‘á»ƒ xem log real-time  

6. (TÃ¹y chá»n) Báº¥m **â€œOpen in CloudWatchâ€** Ä‘á»ƒ xem log Ä‘áº§y Ä‘á»§

![Training logs example](/images4-sagemake-training/08.3.png)

![Training logs example](/images4-sagemake-training/08.4.png)


=======
![Training logs example](/images/4-sagemake-training/08.2.png)

5. Chá»n tab **Logs** Ä‘á»ƒ xem log real-time

6. (TÃ¹y chá»n) Báº¥m **â€œOpen in CloudWatchâ€** Ä‘á»ƒ xem log Ä‘áº§y Ä‘á»§

![Training logs example](/images/4-sagemake-training/08.3.png)

![Training logs example](/images/4-sagemake-training/08.4.png)
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48

{{% notice info %}}
**Info:**  
CloudWatch logs cho Training Job Ä‘Æ°á»£c lÆ°u dÆ°á»›i dáº¡ng: /aws/sagemaker/TrainingJobs/<job-name> Náº¿u job bá»‹ lá»—i, lá»—i Python sáº½ náº±m á»Ÿ cuá»‘i log.
{{% /notice %}}

## 6. Model Registry (Giao diá»‡n má»›i trong Project)

Sau khi training job hoÃ n thÃ nh vÃ  táº¡o ra `model.tar.gz`, bÆ°á»›c tiáº¿p theo lÃ  Ä‘Äƒng kÃ½ mÃ´ hÃ¬nh trong **Model Registry**.  
á» giao diá»‡n SageMaker Unified Studio má»›i, Model Registry náº±m **bÃªn trong tá»«ng Project**, khÃ´ng cÃ²n tÃ¡ch riÃªng nhÆ° trÆ°á»›c.

---

### 6.1. Má»Ÿ Model Registry trong Project

**SageMaker Studio â†’ Projects â†’ mlops-retail-prediction â†’ Models â†’ Registered models**

<<<<<<< HEAD
![Model registry](/images4-sagemake-training/09.1.png)

![Model registry](/images4-sagemake-training/09.2.png)

=======
![Model registry](/images/4-sagemake-training/09.1.png)

![Model registry](/images/4-sagemake-training/09.2.png)
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48

### 6.2. Táº¡o Model Group

1. Báº¥m **Register model group**
2. Äiá»n:
   - **Name**: `retail-price-sensitivity-models`
   - **Description**: `Model group for retail price sensitivity prediction`
3. Nháº¥n **Register model group**

<<<<<<< HEAD
![Model registry](/images4-sagemake-training/09.3.png)


Model Group sáº½ xuáº¥t hiá»‡n trong danh sÃ¡ch.

![Model registry](/images4-sagemake-training/09.4.png)
=======
![Model registry](/images/4-sagemake-training/09.3.png)

Model Group sáº½ xuáº¥t hiá»‡n trong danh sÃ¡ch.

![Model registry](/images/4-sagemake-training/09.4.png)
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48

---

### 6.3. ÄÄƒng kÃ½ Model Version sau khi Training

<<<<<<< HEAD
![Model registry](/images4-sagemake-training/09.5.png)

![Model registry](/images4-sagemake-training/09.6.png)


1. VÃ o **Models â†’ Registered models versions â†’ Model groups**

![Model registry](/images4-sagemake-training/09.7.png)
=======
![Model registry](/images/4-sagemake-training/09.5.png)

![Model registry](/images/4-sagemake-training/09.6.png)

1. VÃ o **Models â†’ Registered models versions â†’ Model groups**

![Model registry](/images/4-sagemake-training/09.7.png)
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48

2. Chá»n nhÃ³m: **retail-price-sensitivity-models**
3. Nháº¥n **Register model**
4. Nháº­p thÃ´ng tin:
   - **Model artifact location (S3)**  
     VÃ­ dá»¥:
     ```
     s3://amazon-sagemaker-842676018087-us-east-1-xxxx/output/model.tar.gz
     ```
   - **Version description**: `Retail prediction model v1.0`
   - **Approval status**: `Pending manual approval`
5. Nháº¥n **Register**

<<<<<<< HEAD
![Model registry](/images4-sagemake-training/09.8.png)

Má»™t *Model Version* má»›i sáº½ Ä‘Æ°á»£c táº¡o.

![Model registry](/images4-sagemake-training/09.9.png)

![Model registry](/images4-sagemake-training/09.10.png)
=======
![Model registry](/images/4-sagemake-training/09.8.png)

Má»™t _Model Version_ má»›i sáº½ Ä‘Æ°á»£c táº¡o.

![Model registry](/images/4-sagemake-training/09.9.png)

![Model registry](/images/4-sagemake-training/09.10.png)
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48

---

### 6.4. Approve Model Version

1. Má»Ÿ **Model group â†’ retail-price-sensitivity-models**
2. Chá»n **Version 1**
3. Nháº¥n **Update status**
4. Äáº·t:
   - **Approved**
5. Save

<<<<<<< HEAD
![Model registry](/images4-sagemake-training/09.11.png)
=======
![Model registry](/images/4-sagemake-training/09.11.png)
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48

### HoÃ n thÃ nh Task 4

**ğŸ“ Notebook thá»±c thi:** `notebooks/sagemaker-retail-etl-training.ipynb`

**ÄÃ£ thÃ nh cÃ´ng:**
<<<<<<< HEAD
- âœ… **Táº¡o SageMaker Domain** vÃ  cáº¥u hÃ¬nh
- âœ… **Táº¡o Project** vÃ  má»Ÿ Studio workspace  
- âœ… **ETL toÃ n bá»™ dataset** - All shop_week partitions â†’ Gold Parquet
- âœ… **Auto-detect partitions** - Scan táº¥t cáº£ shop_week cÃ³ sáºµn
- âœ… **Train Random Forest** vá»›i optimal hyperparameters
- âœ… **Chá»n single model** focus Ä‘á»ƒ tá»‘i Æ°u performance  
=======

- âœ… **Táº¡o SageMaker Domain** vÃ  cáº¥u hÃ¬nh
- âœ… **Táº¡o Project** vÃ  má»Ÿ Studio workspace
- âœ… **ETL toÃ n bá»™ dataset** - All shop_week partitions â†’ Gold Parquet
- âœ… **Auto-detect partitions** - Scan táº¥t cáº£ shop_week cÃ³ sáºµn
- âœ… **Train Random Forest** vá»›i optimal hyperparameters
- âœ… **Chá»n single model** focus Ä‘á»ƒ tá»‘i Æ°u performance
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
- âœ… **Spot instances** - Cost optimization vá»›i auto-scaling
- âœ… **Complete notebook** - 4 cells vá»›i detailed logging

## 7. Clean Up Resources (AWS CLI)

### 7.1. XÃ³a SageMaker Training Jobs

```bash
# Liá»‡t kÃª training jobs
aws sagemaker list-training-jobs --name-contains "retail-prediction-training" --query 'TrainingJobSummaries[*].[TrainingJobName,TrainingJobStatus]' --output table

# Dá»«ng training job Ä‘ang cháº¡y (náº¿u cÃ³)
aws sagemaker stop-training-job --training-job-name <job-name>

# Training jobs tá»± Ä‘á»™ng cleanup sau khi hoÃ n thÃ nh (khÃ´ng cáº§n xÃ³a manual)
```

### 7.2. XÃ³a Model Registry

```bash
# Liá»‡t kÃª model packages
aws sagemaker list-model-packages --model-package-group-name retail-price-sensitivity-models --query 'ModelPackageSummaryList[*].[ModelPackageArn,ModelPackageStatus]' --output table

# XÃ³a tá»«ng model package version
aws sagemaker delete-model-package --model-package-name <model-package-arn>

# XÃ³a model package group
aws sagemaker delete-model-package-group --model-package-group-name retail-price-sensitivity-models
```

### 7.3. XÃ³a SageMaker Domain vÃ  Project

```bash
# Liá»‡t kÃª domains
aws sagemaker list-domains --query 'Domains[*].[DomainId,DomainName,Status]' --output table

# XÃ³a user profiles trÆ°á»›c
aws sagemaker list-user-profiles --domain-id <domain-id> --query 'UserProfiles[*].UserProfileName' --output text

# XÃ³a tá»«ng user profile
aws sagemaker delete-user-profile --domain-id <domain-id> --user-profile-name <user-profile-name>

# XÃ³a domain (sau khi xÃ³a háº¿t user profiles)
aws sagemaker delete-domain --domain-id <domain-id>
```

### 7.4. Clean Up S3 Artifacts

```bash
# XÃ³a model artifacts
aws s3 rm s3://amazon-sagemaker-<account-id>-<region>-<random-id>/artifacts/ --recursive

# XÃ³a gold datasets
aws s3 rm s3://amazon-sagemaker-<account-id>-<region>-<random-id>/gold/ --recursive

# Kiá»ƒm tra project bucket cÃ²n gÃ¬
aws s3 ls s3://amazon-sagemaker-<account-id>-<region>-<random-id>/ --recursive
```

### 7.5. XÃ³a CloudWatch Logs

```bash
# Liá»‡t kÃª log groups cá»§a SageMaker
aws logs describe-log-groups --log-group-name-prefix "/aws/sagemaker/TrainingJobs" --query 'logGroups[*].logGroupName'

# XÃ³a training job logs
aws logs delete-log-group --log-group-name "/aws/sagemaker/TrainingJobs/retail-prediction-training-<timestamp>"
```

---

## 8. Báº£ng giÃ¡ SageMaker

### 8.1. Chi phÃ­ Training Instances

<<<<<<< HEAD
| Instance Type | vCPU | RAM | GiÃ¡ (USD/hour) | PhÃ¹ há»£p cho |
|---------------|------|-----|----------------|-------------|
| **ml.m5.large** | 2 | 8 GB | $0.138 | Small datasets, prototyping |
| **ml.m5.xlarge** | 4 | 16 GB | $0.276 | Medium datasets (Ä‘Ã£ dÃ¹ng) |
| **ml.m5.2xlarge** | 8 | 32 GB | $0.552 | Large datasets |
| **ml.c5.xlarge** | 4 | 8 GB | $0.238 | CPU-intensive training |
| **ml.p3.2xlarge** | 8 | 61 GB | $4.284 | GPU deep learning |

### 8.2. Chi phÃ­ SageMaker Studio

| Component | GiÃ¡ (USD) | Ghi chÃº |
|-----------|-----------|---------|
| **Studio Notebooks** | $0.0582/hour | ml.t3.medium default |
| **Domain Setup** | Free | One-time setup |
| **Data Wrangler** | $0.42/hour | Visual data prep |
| **Processing Jobs** | Instance pricing | Same as training |

### 8.3. Chi phÃ­ Model Registry & Endpoints

| Service | GiÃ¡ (USD) | Ghi chÃº |
|---------|-----------|---------|
| **Model Registry** | Free | Model versioning |
| **Real-time Endpoint** | $0.076/hour | ml.t2.medium |
| **Batch Transform** | Instance pricing | Pay per job |
=======
| Instance Type     | vCPU | RAM   | GiÃ¡ (USD/hour) | PhÃ¹ há»£p cho                 |
| ----------------- | ---- | ----- | -------------- | --------------------------- |
| **ml.m5.large**   | 2    | 8 GB  | $0.138         | Small datasets, prototyping |
| **ml.m5.xlarge**  | 4    | 16 GB | $0.276         | Medium datasets (Ä‘Ã£ dÃ¹ng)   |
| **ml.m5.2xlarge** | 8    | 32 GB | $0.552         | Large datasets              |
| **ml.c5.xlarge**  | 4    | 8 GB  | $0.238         | CPU-intensive training      |
| **ml.p3.2xlarge** | 8    | 61 GB | $4.284         | GPU deep learning           |

### 8.2. Chi phÃ­ SageMaker Studio

| Component            | GiÃ¡ (USD)        | Ghi chÃº              |
| -------------------- | ---------------- | -------------------- |
| **Studio Notebooks** | $0.0582/hour     | ml.t3.medium default |
| **Domain Setup**     | Free             | One-time setup       |
| **Data Wrangler**    | $0.42/hour       | Visual data prep     |
| **Processing Jobs**  | Instance pricing | Same as training     |

### 8.3. Chi phÃ­ Model Registry & Endpoints

| Service                  | GiÃ¡ (USD)             | Ghi chÃº           |
| ------------------------ | --------------------- | ----------------- |
| **Model Registry**       | Free                  | Model versioning  |
| **Real-time Endpoint**   | $0.076/hour           | ml.t2.medium      |
| **Batch Transform**      | Instance pricing      | Pay per job       |
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
| **Multi-model Endpoint** | $0.076/hour + storage | Cost optimization |

### 8.4. Æ¯á»›c tÃ­nh chi phÃ­ cho Task 4

**Training Job thá»±c táº¿:**
<<<<<<< HEAD
- Instance: ml.m5.xlarge
- Duration: ~10-15 minutes  
- **Training cost:** $0.276 Ã— 0.25h = **$0.07**

**SageMaker Studio:**
=======

- Instance: ml.m5.xlarge
- Duration: ~10-15 minutes
- **Training cost:** $0.276 Ã— 0.25h = **$0.07**

**SageMaker Studio:**

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
- Notebook development: ~2 hours
- Instance: ml.t3.medium
- **Studio cost:** $0.0582 Ã— 2h = **$0.12**

**Storage & Model Registry:**
<<<<<<< HEAD
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
- Model artifacts: ~50MB
- S3 storage: ~$0.001
- Model Registry: Free

**Total chi phÃ­ Task 4:**

<<<<<<< HEAD
| Component | Duration | Cost |
|-----------|----------|------|
| Training (ml.m5.xlarge) | 15 mins | $0.07 |
| Studio Notebooks | 2 hours | $0.12 |
| S3 Storage | Monthly | $0.001 |
| **Total** | | **â‰ˆ $0.19** |

**So sÃ¡nh vá»›i cÃ¡c options:**

| Approach | Instance | Duration | Cost | Performance |
|----------|----------|----------|------|-------------|
| **Current (ml.m5.xlarge)** | 4 vCPU, 16GB | 15 min | $0.07 | âœ… Balanced |
| Smaller (ml.m5.large) | 2 vCPU, 8GB | 25 min | $0.06 | Slower |
| Larger (ml.m5.2xlarge) | 8 vCPU, 32GB | 8 min | $0.07 | Faster, same cost |
| Spot instance | Same specs | 15 min | $0.02-0.05 | 60-70% savings |

{{% notice info %}}
**ğŸ’° Cost Optimization Tips:**
- **Spot instances:** 60-70% cheaper cho non-critical training
- **Smaller instances:** OK cho datasets < 1GB  
- **Studio auto-shutdown:** Tá»± Ä‘á»™ng táº¯t notebooks sau 1h idle
- **Batch jobs:** Thay vÃ¬ real-time endpoints cho inference
{{% /notice %}}
=======
| Component               | Duration | Cost        |
| ----------------------- | -------- | ----------- |
| Training (ml.m5.xlarge) | 15 mins  | $0.07       |
| Studio Notebooks        | 2 hours  | $0.12       |
| S3 Storage              | Monthly  | $0.001      |
| **Total**               |          | **â‰ˆ $0.19** |

**So sÃ¡nh vá»›i cÃ¡c options:**

| Approach                   | Instance     | Duration | Cost       | Performance       |
| -------------------------- | ------------ | -------- | ---------- | ----------------- |
| **Current (ml.m5.xlarge)** | 4 vCPU, 16GB | 15 min   | $0.07      | âœ… Balanced       |
| Smaller (ml.m5.large)      | 2 vCPU, 8GB  | 25 min   | $0.06      | Slower            |
| Larger (ml.m5.2xlarge)     | 8 vCPU, 32GB | 8 min    | $0.07      | Faster, same cost |
| Spot instance              | Same specs   | 15 min   | $0.02-0.05 | 60-70% savings    |

{{% notice info %}}
**ğŸ’° Cost Optimization Tips:**

- **Spot instances:** 60-70% cheaper cho non-critical training
- **Smaller instances:** OK cho datasets < 1GB
- **Studio auto-shutdown:** Tá»± Ä‘á»™ng táº¯t notebooks sau 1h idle
- **Batch jobs:** Thay vÃ¬ real-time endpoints cho inference
  {{% /notice %}}
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48

---

{{% notice info %}}
**ğŸ“Š SageMaker Unified Studio Benefits:**
<<<<<<< HEAD
=======

>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48
- **Integrated Workspace**: Project-based collaboration vá»›i shared resources
- **Managed Infrastructure**: Auto-provisioned compute cho notebooks vÃ  training
- **Cross-Region Support**: Built-in handling cá»§a S3 cross-region access
- **Asset Catalog**: Automatic registration cá»§a models vÃ  datasets
- **Team Collaboration**: Shared notebooks, workflows, vÃ  approval processes
- **Cost Optimization**: Managed compute vá»›i automatic scaling
- **Unified Interface**: Single pane for data, ML, vÃ  generative AI workflows
<<<<<<< HEAD
{{% /notice %}}
=======
  {{% /notice %}}
>>>>>>> e2332b6d9a96695941b1fb2baeb1eb38bfa46e48

## ğŸ“¹ Video thá»±c hiá»‡n Task 4

<div style="position: relative; width: 100%; max-width: 2000px; margin: 0 auto; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe 
    style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" 
    src="https://www.youtube.com/embed/watch?v=pV-5ErGHAI0&list=PL53MEKrSAUpu0i5F-ttcVdKkSv0jb48Mc&index=3" 
    title="YouTube video player" 
    frameborder="0" 
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
    referrerpolicy="strict-origin-when-cross-origin" 
    allowfullscreen>
  </iframe>
</div>

---

**Next Step**: [Task 05: Production VPC](../5-vpc/)
